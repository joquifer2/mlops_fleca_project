{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d7e513",
   "metadata": {},
   "source": [
    "# Feature Pipeline - Datos Históricos Completos (Semanas Completas)\n",
    "\n",
    "> **OBJETIVO**: Este notebook replica la lógica del pipeline de datos pero utiliza la función `load_raw_data_historico` para obtener un conjunto completo de datos, asegurando además que todas las semanas son completas (de lunes a domingo).\n",
    "\n",
    "## El enfoque:\n",
    "1. Usar `load_raw_data_historico` que internamente utiliza `descargar_datos_bigquery_histórico`\n",
    "2. Obtener datos completos desde ambas tablas de BigQuery\n",
    "3. **Asegurar que los datos terminan en un domingo** (semana completa) usando el nuevo parámetro `usar_semana_completa=True`\n",
    "4. Procesar y transformar los datos para el Feature Store, incluyendo solo semanas que tengan los 7 días\n",
    "5. Preparar las series temporales para el entrenamiento de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fd8c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Añadir directorio raíz al path para importar correctamente los módulos\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from datetime import datetime, timedelta, date\n",
    "from pathlib import Path\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src import config\n",
    "import hopsworks\n",
    "\n",
    "# Importamos las funciones que necesitaremos\n",
    "from src.data_utils import descargar_datos_bigquery_histórico, transformar_a_series_temporales, load_raw_data_historico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9db2eb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando carga de datos históricos completos...\n",
      "Descargando datos históricos completos desde BigQuery\n",
      "Iniciando conexión con BigQuery...\n",
      "Conexión establecida.\n",
      "Descargando datos de fleca-del-port.varios.raw_data_bq_forecasting_20250630 ...\n",
      "Conexión establecida.\n",
      "Descargando datos de fleca-del-port.varios.raw_data_bq_forecasting_20250630 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Workspace\\mlops_fleca_project\\.venv\\lib\\site-packages\\google\\cloud\\bigquery\\table.py:1957: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas descargadas de la primera tabla: 337353\n",
      "Descargando datos de fleca-del-port.fleca_ventas_dia.t_facturas_dia_extendida_2023 ...\n",
      "Filas descargadas de la segunda tabla: 21276\n",
      "Total de filas tras concatenar: 358629\n",
      "Guardando archivo en C:\\Workspace\\mlops_fleca_project\\data\\raw\\raw_data_bq_forecasting_20250817.parquet ...\n",
      "Archivo guardado correctamente.\n",
      "Filas descargadas de la segunda tabla: 21276\n",
      "Total de filas tras concatenar: 358629\n",
      "Guardando archivo en C:\\Workspace\\mlops_fleca_project\\data\\raw\\raw_data_bq_forecasting_20250817.parquet ...\n",
      "Archivo guardado correctamente.\n",
      "Ajustando fecha final a último domingo completo: 2025-08-10\n",
      "Filtrando datos hasta: 2025-08-10 00:00:00\n",
      "Ajustando fecha final a último domingo completo: 2025-08-10\n",
      "Filtrando datos hasta: 2025-08-10 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Descargar datos históricos completos usando la función load_raw_data_historico\n",
    "print(\"Iniciando carga de datos históricos completos...\")\n",
    "\n",
    "# Usar el nuevo parámetro usar_semana_completa para asegurar que las fechas finales terminan en domingo\n",
    "df_raw = load_raw_data_historico(\n",
    "    descargar_bq=True, \n",
    "    usar_semana_completa=True,\n",
    "    fecha_inicio=None,  # Fecha inicial en formato YYYY-MM-DD\n",
    "    fecha_fin=None      # Fecha final en formato YYYY-MM-DD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68b7c995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series temporales generadas: (132, 8)\n",
      "   year  week   familia  base_imponible  is_summer_peak  is_easter  \\\n",
      "0  2023     1  BOLLERIA          825.11               0          0   \n",
      "1  2023     2  BOLLERIA          658.40               0          0   \n",
      "2  2023     3  BOLLERIA          741.40               0          0   \n",
      "3  2023     4  BOLLERIA          653.64               0          0   \n",
      "4  2023     5  BOLLERIA          680.46               0          0   \n",
      "\n",
      "   dias_semana week_start  \n",
      "0            7 2023-01-02  \n",
      "1            7 2023-01-09  \n",
      "2            7 2023-01-16  \n",
      "3            7 2023-01-23  \n",
      "4            7 2023-01-30  \n"
     ]
    }
   ],
   "source": [
    "# 4. Transformar a series temporales semanales solo para la familia BOLLERIA\n",
    "df_ts = transformar_a_series_temporales(df_raw, familia=\"BOLLERIA\")\n",
    "print('Series temporales generadas:', df_ts.shape)\n",
    "print(df_ts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2076a211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year                       int64\n",
      "week                       int64\n",
      "familia           string[python]\n",
      "base_imponible           float64\n",
      "is_summer_peak             int32\n",
      "is_easter                  int64\n",
      "dias_semana                int64\n",
      "week_start        datetime64[ns]\n",
      "dtype: object\n",
      "   year  week   familia  base_imponible  is_summer_peak  is_easter  \\\n",
      "0  2023     1  BOLLERIA          825.11               0          0   \n",
      "1  2023     2  BOLLERIA          658.40               0          0   \n",
      "2  2023     3  BOLLERIA          741.40               0          0   \n",
      "3  2023     4  BOLLERIA          653.64               0          0   \n",
      "4  2023     5  BOLLERIA          680.46               0          0   \n",
      "\n",
      "   dias_semana week_start  \n",
      "0            7 2023-01-02  \n",
      "1            7 2023-01-09  \n",
      "2            7 2023-01-16  \n",
      "3            7 2023-01-23  \n",
      "4            7 2023-01-30  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ajustar tipos para coincidir con el schema del Feature Group histórico\n",
    "df_ts['year'] = df_ts['year'].astype('int64')  # bigint\n",
    "df_ts['week'] = df_ts['week'].astype('int64')  # bigint\n",
    "df_ts['familia'] = df_ts['familia'].astype('string')  # string\n",
    "df_ts['base_imponible'] = df_ts['base_imponible'].astype('float64')  # double\n",
    "df_ts['is_summer_peak'] = df_ts['is_summer_peak'].astype('int32')  # int\n",
    "df_ts['is_easter'] = df_ts['is_easter'].astype('int64')  # bigint\n",
    "df_ts['week_start'] = pd.to_datetime(df_ts['week_start'])  # timestamp\n",
    "\n",
    "print(df_ts.dtypes)\n",
    "print(df_ts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e785f261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 19:56:46,524 INFO: Initializing external client\n",
      "2025-08-17 19:56:46,524 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-08-17 19:56:46,524 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-08-17 19:56:48,009 INFO: Python Engine initialized.\n",
      "2025-08-17 19:56:48,009 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1242272\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1242272\n"
     ]
    }
   ],
   "source": [
    "# 5. Conectar a hopsworks\n",
    "project = hopsworks.login(\n",
    "    api_key_value=config.HOPSWORKS_API_KEY, \n",
    "    project=config.HOPSWORKS_PROJECT_NAME)\n",
    "\n",
    "# Conectar al feature store\n",
    "feature_store = project.get_feature_store()\n",
    "\n",
    "# Conectar al Feature Group histórico\n",
    "try:\n",
    "    feature_group = feature_store.get_feature_group(\n",
    "        name=config.FEATURE_GROUP_NAME,\n",
    "        version=config.FEATURE_GROUP_VERSION,\n",
    "        \n",
    "    )\n",
    "    if feature_group is None:\n",
    "        raise Exception(\"El Feature Group histórico no existe o el nombre/version no coinciden exactamente. Verifica en Hopsworks.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al crear/conectar el Feature Group: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "927653b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subida a Hopsworks desactivada.\n",
      "Cambia 'subir_a_hopsworks = True' cuando quieras subir los datos.\n"
     ]
    }
   ],
   "source": [
    "# Solo para cuando estemos seguros que los datos están correctos\n",
    "subir_a_hopsworks = False  # Cambiar a True cuando quieras subir los datos\n",
    "\n",
    "if subir_a_hopsworks:\n",
    "    # Importar módulos necesarios para Hopsworks\n",
    "    import hopsworks\n",
    "    from src import config\n",
    "    \n",
    "    # Conectar a Hopsworks\n",
    "    project = hopsworks.login(\n",
    "        api_key_value=config.HOPSWORKS_API_KEY, \n",
    "        project=config.HOPSWORKS_PROJECT_NAME)\n",
    "\n",
    "    # Conectar al feature store\n",
    "    feature_store = project.get_feature_store()\n",
    "\n",
    "    # Conectar al Feature Group histórico\n",
    "    try:\n",
    "        feature_group = feature_store.get_feature_group(\n",
    "            name=config.FEATURE_GROUP_NAME,\n",
    "            version=config.FEATURE_GROUP_VERSION,\n",
    "        )\n",
    "        if feature_group is None:\n",
    "            raise Exception(\"El Feature Group histórico no existe o el nombre/version no coinciden exactamente. Verifica en Hopsworks.\")\n",
    "        \n",
    "        # Insertar los datos en el Feature Group\n",
    "        print(\"Subiendo datos a Hopsworks...\")\n",
    "        feature_group.insert(\n",
    "            df_ts,\n",
    "            write_options={'wait_for_job': True}\n",
    "        )\n",
    "        print(\"¡Datos subidos con éxito a Hopsworks!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al conectar o subir datos a Hopsworks: {e}\")\n",
    "else:\n",
    "    print(\"Subida a Hopsworks desactivada.\")\n",
    "    print(\"Cambia 'subir_a_hopsworks = True' cuando quieras subir los datos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bc088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insertar los datos en el Feature Group\n",
    "feature_group.insert(\n",
    "    df_ts,\n",
    "    write_options={'wait_for_job': True}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-project-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
