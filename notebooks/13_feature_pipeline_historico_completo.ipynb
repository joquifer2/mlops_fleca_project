{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d7e513",
   "metadata": {},
   "source": [
    "# Feature Pipeline - Datos Históricos Completos (Semanas Completas)\n",
    "\n",
    "> **OBJETIVO**: Este notebook replica la lógica del pipeline de datos pero utiliza la función `load_raw_data_historico` para obtener un conjunto completo de datos, asegurando además que todas las semanas son completas (de lunes a domingo).\n",
    "\n",
    "## El enfoque:\n",
    "1. Usar `load_raw_data_historico` que internamente utiliza `descargar_datos_bigquery_histórico`\n",
    "2. Obtener datos completos desde ambas tablas de BigQuery\n",
    "3. **Asegurar que los datos terminan en un domingo** (semana completa) usando el nuevo parámetro `usar_semana_completa=True`\n",
    "4. Procesar y transformar los datos para el Feature Store, incluyendo solo semanas que tengan los 7 días\n",
    "5. Preparar las series temporales para el entrenamiento de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fd8c6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG ENV HOPSWORKS_PROJECT_NAME: fleca_mlops\n",
      "DEBUG ENV PATH: C:\\Workspace\\mlops_fleca_project\\.env\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Añadir directorio raíz al path para importar correctamente los módulos\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from datetime import datetime, timedelta, date\n",
    "from pathlib import Path\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src import config\n",
    "import hopsworks\n",
    "\n",
    "# Importamos las funciones que necesitaremos\n",
    "from src.data_utils import descargar_datos_bigquery_histórico, transformar_a_series_temporales, load_raw_data_historico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db2eb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando carga de datos históricos completos...\n",
      "Descargando datos históricos completos desde BigQuery\n",
      "Iniciando conexión con BigQuery...\n",
      "Conexión establecida.\n",
      "Descargando datos de fleca-del-port.varios.raw_data_bq_forecasting_20250630 ...\n",
      "Conexión establecida.\n",
      "Descargando datos de fleca-del-port.varios.raw_data_bq_forecasting_20250630 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas descargadas de la primera tabla: 337353\n",
      "Descargando datos de fleca-del-port.fleca_ventas_dia.t_facturas_dia_extendida_2023 ...\n",
      "Filas descargadas de la segunda tabla: 28295\n",
      "Total de filas tras concatenar: 365648\n",
      "Guardando archivo en C:\\Workspace\\mlops_fleca_project\\data\\raw\\raw_data_bq_forecasting_20250902.parquet ...\n",
      "Archivo guardado correctamente.\n",
      "Filas descargadas de la segunda tabla: 28295\n",
      "Total de filas tras concatenar: 365648\n",
      "Guardando archivo en C:\\Workspace\\mlops_fleca_project\\data\\raw\\raw_data_bq_forecasting_20250902.parquet ...\n",
      "Archivo guardado correctamente.\n",
      "Ajustando fecha final a último domingo completo: 2025-08-24\n",
      "Filtrando datos hasta: 2025-08-24 00:00:00\n",
      "Ajustando fecha final a último domingo completo: 2025-08-24\n",
      "Filtrando datos hasta: 2025-08-24 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Descargar datos históricos completos usando la función load_raw_data_historico\n",
    "print(\"Iniciando carga de datos históricos completos...\")\n",
    "\n",
    "# Usar el nuevo parámetro usar_semana_completa para asegurar que las fechas finales terminan en domingo\n",
    "df_raw = load_raw_data_historico(\n",
    "    descargar_bq=True, \n",
    "    usar_semana_completa=True,\n",
    "    fecha_inicio=None,  # Fecha inicial en formato YYYY-MM-DD\n",
    "    fecha_fin=None      # Fecha final en formato YYYY-MM-DD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68b7c995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series temporales generadas: (134, 8)\n",
      "   year  week   familia  base_imponible  is_summer_peak  is_easter  \\\n",
      "0  2023     1  BOLLERIA          825.11               0          0   \n",
      "1  2023     2  BOLLERIA          658.40               0          0   \n",
      "2  2023     3  BOLLERIA          741.40               0          0   \n",
      "3  2023     4  BOLLERIA          653.64               0          0   \n",
      "4  2023     5  BOLLERIA          680.46               0          0   \n",
      "\n",
      "   dias_semana week_start  \n",
      "0            7 2023-01-02  \n",
      "1            7 2023-01-09  \n",
      "2            7 2023-01-16  \n",
      "3            7 2023-01-23  \n",
      "4            7 2023-01-30  \n"
     ]
    }
   ],
   "source": [
    "# 4. Transformar a series temporales semanales solo para la familia BOLLERIA\n",
    "df_ts = transformar_a_series_temporales(df_raw, familia=\"BOLLERIA\")\n",
    "print('Series temporales generadas:', df_ts.shape)\n",
    "print(df_ts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2076a211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year                       int64\n",
      "week                       int64\n",
      "familia           string[python]\n",
      "base_imponible           float64\n",
      "is_summer_peak             int32\n",
      "is_easter                  int64\n",
      "dias_semana                int64\n",
      "week_start        datetime64[ns]\n",
      "dtype: object\n",
      "   year  week   familia  base_imponible  is_summer_peak  is_easter  \\\n",
      "0  2023     1  BOLLERIA          825.11               0          0   \n",
      "1  2023     2  BOLLERIA          658.40               0          0   \n",
      "2  2023     3  BOLLERIA          741.40               0          0   \n",
      "3  2023     4  BOLLERIA          653.64               0          0   \n",
      "4  2023     5  BOLLERIA          680.46               0          0   \n",
      "\n",
      "   dias_semana week_start  \n",
      "0            7 2023-01-02  \n",
      "1            7 2023-01-09  \n",
      "2            7 2023-01-16  \n",
      "3            7 2023-01-23  \n",
      "4            7 2023-01-30  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ajustar tipos para coincidir con el schema del Feature Group histórico\n",
    "df_ts['year'] = df_ts['year'].astype('int64')  # bigint\n",
    "df_ts['week'] = df_ts['week'].astype('int64')  # bigint\n",
    "df_ts['familia'] = df_ts['familia'].astype('string')  # string\n",
    "df_ts['base_imponible'] = df_ts['base_imponible'].astype('float64')  # double\n",
    "df_ts['is_summer_peak'] = df_ts['is_summer_peak'].astype('int32')  # int\n",
    "df_ts['is_easter'] = df_ts['is_easter'].astype('int64')  # bigint\n",
    "df_ts['week_start'] = pd.to_datetime(df_ts['week_start'])  # timestamp\n",
    "\n",
    "print(df_ts.dtypes)\n",
    "print(df_ts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e785f261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-02 19:54:21,029 INFO: Initializing external client\n",
      "2025-09-02 19:54:21,029 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-09-02 19:54:21,029 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "UserWarning: The installed hopsworks client version 4.3.1 may not be compatible with the connected Hopsworks backend version 4.2.2. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-02 19:54:22,198 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1242272\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1242272\n"
     ]
    }
   ],
   "source": [
    "# 5. Conectar a hopsworks\n",
    "project = hopsworks.login(\n",
    "    api_key_value=config.HOPSWORKS_API_KEY, \n",
    "    project=config.HOPSWORKS_PROJECT_NAME)\n",
    "\n",
    "# Conectar al feature store\n",
    "feature_store = project.get_feature_store()\n",
    "\n",
    "# Conectar al Feature Group histórico\n",
    "try:\n",
    "    feature_group = feature_store.get_feature_group(\n",
    "        name=config.FEATURE_GROUP_NAME,\n",
    "        version=config.FEATURE_GROUP_VERSION,\n",
    "        \n",
    "    )\n",
    "    if feature_group is None:\n",
    "        raise Exception(\"El Feature Group histórico no existe o el nombre/version no coinciden exactamente. Verifica en Hopsworks.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al crear/conectar el Feature Group: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "927653b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-02 19:56:23,999 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS. Final status: SUCCEEDED\n",
      "2025-09-02 19:56:24,163 INFO: Waiting for log aggregation to finish.\n",
      "2025-09-02 19:56:24,163 INFO: Waiting for log aggregation to finish.\n",
      "2025-09-02 19:56:36,179 INFO: Execution finished successfully.\n",
      "¡Datos subidos con éxito a Hopsworks!\n",
      "2025-09-02 19:56:36,179 INFO: Execution finished successfully.\n",
      "¡Datos subidos con éxito a Hopsworks!\n"
     ]
    }
   ],
   "source": [
    "# Solo para cuando estemos seguros que los datos están correctos\n",
    "subir_a_hopsworks = True  # Cambiar a True cuando quieras subir los datos\n",
    "\n",
    "if subir_a_hopsworks:\n",
    "    # Importar módulos necesarios para Hopsworks\n",
    "    import hopsworks\n",
    "    from src import config\n",
    "    \n",
    "    # Conectar a Hopsworks\n",
    "    project = hopsworks.login(\n",
    "        api_key_value=config.HOPSWORKS_API_KEY, \n",
    "        project=config.HOPSWORKS_PROJECT_NAME)\n",
    "\n",
    "    # Conectar al feature store\n",
    "    feature_store = project.get_feature_store()\n",
    "\n",
    "    # Conectar al Feature Group histórico\n",
    "    try:\n",
    "        feature_group = feature_store.get_feature_group(\n",
    "            name=config.FEATURE_GROUP_NAME,\n",
    "            version=config.FEATURE_GROUP_VERSION,\n",
    "        )\n",
    "        if feature_group is None:\n",
    "            raise Exception(\"El Feature Group histórico no existe o el nombre/version no coinciden exactamente. Verifica en Hopsworks.\")\n",
    "        \n",
    "        # Insertar los datos en el Feature Group\n",
    "        print(\"Subiendo datos a Hopsworks...\")\n",
    "        feature_group.insert(\n",
    "            df_ts,\n",
    "            write_options={'wait_for_job': True}\n",
    "        )\n",
    "        print(\"¡Datos subidos con éxito a Hopsworks!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al conectar o subir datos a Hopsworks: {e}\")\n",
    "else:\n",
    "    print(\"Subida a Hopsworks desactivada.\")\n",
    "    print(\"Cambia 'subir_a_hopsworks = True' cuando quieras subir los datos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9bc088d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 134/134 | Elapsed Time: 00:00 | Remaining Time: 00:00\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: times_series_bolleria_feature_group_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1242272/jobs/named/times_series_bolleria_feature_group_1_offline_fg_materialization/executions\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1242272/jobs/named/times_series_bolleria_feature_group_1_offline_fg_materialization/executions\n",
      "2025-09-02 19:57:02,465 INFO: Waiting for execution to finish. Current state: INITIALIZING. Final status: UNDEFINED\n",
      "2025-09-02 19:57:02,465 INFO: Waiting for execution to finish. Current state: INITIALIZING. Final status: UNDEFINED\n",
      "2025-09-02 19:57:05,658 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2025-09-02 19:57:05,658 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2025-09-02 19:58:32,307 INFO: Waiting for execution to finish. Current state: SUCCEEDING. Final status: UNDEFINED\n",
      "2025-09-02 19:58:32,307 INFO: Waiting for execution to finish. Current state: SUCCEEDING. Final status: UNDEFINED\n",
      "2025-09-02 19:58:35,514 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS. Final status: SUCCEEDED\n",
      "2025-09-02 19:58:35,514 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS. Final status: SUCCEEDED\n",
      "2025-09-02 19:58:35,677 INFO: Waiting for log aggregation to finish.\n",
      "2025-09-02 19:58:35,677 INFO: Waiting for log aggregation to finish.\n",
      "2025-09-02 19:58:44,292 INFO: Execution finished successfully.\n",
      "2025-09-02 19:58:44,292 INFO: Execution finished successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('times_series_bolleria_feature_group_1_offline_fg_materialization', 'SPARK'),\n",
       " None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insertar los datos en el Feature Group\n",
    "feature_group.insert(\n",
    "    df_ts,\n",
    "    write_options={'wait_for_job': True}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-project-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
