{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a696f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Añade el directorio raíz del proyecto al sys.path\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cd32205",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hopsworks'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_raw_data, transformar_a_series_temporales\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhopsworks\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'hopsworks'"
     ]
    }
   ],
   "source": [
    "# 1. Imports\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from src.data_utils import load_raw_data, transformar_a_series_temporales\n",
    "from src import config\n",
    "import hopsworks\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2086e648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha actual: 2025-09-21 Sunday\n",
      "Última semana completa:\n",
      "  Lunes: 2025-09-08 Monday\n",
      "  Domingo: 2025-09-14 Sunday\n",
      "  Rango: 2025-09-08 a 2025-09-14\n"
     ]
    }
   ],
   "source": [
    "# 2. Calcular la última semana completa (lunes a domingo)\n",
    "\n",
    "hoy = datetime.utcnow()\n",
    "print(f\"Fecha actual: {hoy.strftime('%Y-%m-%d %A')}\")\n",
    "\n",
    "# Calcular cuántos días han pasado desde el lunes de esta semana\n",
    "dias_desde_lunes = hoy.weekday()  # 0=lunes, 1=martes, ..., 6=domingo\n",
    "\n",
    "# Ir al lunes de la semana pasada (última semana completa)\n",
    "ultimo_lunes = (hoy - timedelta(days=dias_desde_lunes + 7)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "ultimo_domingo = ultimo_lunes + timedelta(days=6)\n",
    "\n",
    "print(f\"Última semana completa:\")\n",
    "print(f\"  Lunes: {ultimo_lunes.strftime('%Y-%m-%d %A')}\")\n",
    "print(f\"  Domingo: {ultimo_domingo.strftime('%Y-%m-%d %A')}\")\n",
    "print(f\"  Rango: {ultimo_lunes.strftime('%Y-%m-%d')} a {ultimo_domingo.strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a81d706b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando datos desde BigQuery porque descargar_bq=True o no existe el archivo C:\\Workspace\\mlops_fleca_project\\data\\raw\\raw_data_bq_forecasting.parquet\n",
      "Iniciando conexión con BigQuery...\n",
      "Conexión establecida.\n",
      "Usando fechas en consulta SQL: fecha_inicio='2025-09-08' y fecha_fin='2025-09-14'\n",
      "Descargando datos de fleca-del-port.fleca_ventas_dia.t_facturas_dia_extendida_2023 ...\n",
      "Ejecutando consulta SQL:\n",
      "\n",
      "    SELECT \n",
      "        fecha,\n",
      "        n_factura,\n",
      "        zona_de_venta,\n",
      "        producto,\n",
      "        familia,\n",
      "        cantidad,\n",
      "        base_imponible,\n",
      "        tipo_IVA,\n",
      "        total\n",
      "    FROM `fleca-del-port.fleca_ventas_dia.t_facturas_dia_extendida_2023`\n",
      "    WHERE fecha >= '2025-09-08' AND fecha <= '2025-09-14'\n",
      "Conexión establecida.\n",
      "Usando fechas en consulta SQL: fecha_inicio='2025-09-08' y fecha_fin='2025-09-14'\n",
      "Descargando datos de fleca-del-port.fleca_ventas_dia.t_facturas_dia_extendida_2023 ...\n",
      "Ejecutando consulta SQL:\n",
      "\n",
      "    SELECT \n",
      "        fecha,\n",
      "        n_factura,\n",
      "        zona_de_venta,\n",
      "        producto,\n",
      "        familia,\n",
      "        cantidad,\n",
      "        base_imponible,\n",
      "        tipo_IVA,\n",
      "        total\n",
      "    FROM `fleca-del-port.fleca_ventas_dia.t_facturas_dia_extendida_2023`\n",
      "    WHERE fecha >= '2025-09-08' AND fecha <= '2025-09-14'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas descargadas de la segunda tabla: 2483\n",
      "Guardando archivo en C:\\Workspace\\mlops_fleca_project\\data\\raw\\raw_data_bq_forecasting_20250921.parquet ...\n",
      "Archivo guardado correctamente.\n",
      "Usando archivo recién generado: C:\\Workspace\\mlops_fleca_project\\data\\raw\\raw_data_bq_forecasting_20250921.parquet\n",
      "Cargando datos desde: C:\\Workspace\\mlops_fleca_project\\data\\raw\\raw_data_bq_forecasting_20250921.parquet\n",
      "Validando fechas entre 2025-09-08 y 2025-09-14 (7 días)\n",
      "Total de fechas faltantes: 0\n",
      "No faltan fechas en el rango especificado.\n",
      "Datos descargados: (2483, 11)\n"
     ]
    }
   ],
   "source": [
    "# 3. Descargar y cargar datos de BigQuery para esa semana\n",
    "df = load_raw_data(\n",
    "    fecha_inicio=ultimo_lunes.strftime('%Y-%m-%d'),\n",
    "    fecha_fin=ultimo_domingo.strftime('%Y-%m-%d'),\n",
    "    descargar_bq=True\n",
    ")\n",
    "print('Datos descargados:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "882b4ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series temporales generadas: (1, 8)\n",
      "   year  week   familia  base_imponible  is_summer_peak  is_easter  \\\n",
      "0  2025    37  BOLLERIA          969.84               0          0   \n",
      "\n",
      "   dias_semana week_start  \n",
      "0            7 2025-09-08  \n"
     ]
    }
   ],
   "source": [
    "# 4. Transformar a series temporales semanales solo para la familia BOLLERIA\n",
    "df_ts = transformar_a_series_temporales(df, familia=\"BOLLERIA\")\n",
    "print('Series temporales generadas:', df_ts.shape)\n",
    "print(df_ts.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c09cb7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year                       int64\n",
      "week                       int64\n",
      "familia           string[python]\n",
      "base_imponible           float64\n",
      "is_summer_peak             int32\n",
      "is_easter                  int64\n",
      "dias_semana                int64\n",
      "week_start        datetime64[ns]\n",
      "dtype: object\n",
      "   year  week   familia  base_imponible  is_summer_peak  is_easter  \\\n",
      "0  2025    37  BOLLERIA          969.84               0          0   \n",
      "\n",
      "   dias_semana week_start  \n",
      "0            7 2025-09-08  \n"
     ]
    }
   ],
   "source": [
    "# Eliminar columna 'fecha' si existe\n",
    "if 'fecha' in df_ts.columns:\n",
    "    df_ts = df_ts.drop(columns=['fecha'])\n",
    "\n",
    "# Ajustar tipos para coincidir con el schema del Feature Group histórico\n",
    "df_ts['year'] = df_ts['year'].astype('int64')  # bigint\n",
    "df_ts['week'] = df_ts['week'].astype('int64')  # bigint\n",
    "df_ts['familia'] = df_ts['familia'].astype('string')  # string\n",
    "df_ts['base_imponible'] = df_ts['base_imponible'].astype('float64')  # double\n",
    "df_ts['is_summer_peak'] = df_ts['is_summer_peak'].astype('int32')  # int\n",
    "df_ts['is_easter'] = df_ts['is_easter'].astype('int64')  # bigint\n",
    "df_ts['week_start'] = pd.to_datetime(df_ts['week_start'])  # timestamp\n",
    "\n",
    "print(df_ts.dtypes)\n",
    "print(df_ts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f5142035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-21 19:58:36,867 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-09-21 19:58:36,871 INFO: Initializing external client\n",
      "2025-09-21 19:58:36,871 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "Connection closed.\n",
      "2025-09-21 19:58:36,871 INFO: Initializing external client\n",
      "2025-09-21 19:58:36,871 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "UserWarning: The installed hopsworks client version 4.3.1 may not be compatible with the connected Hopsworks backend version 4.2.2. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-21 19:58:37,993 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1242272\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1242272\n"
     ]
    }
   ],
   "source": [
    "# 5. Conectar a hopsworks\n",
    "project = hopsworks.login(\n",
    "    api_key_value=config.HOPSWORKS_API_KEY, \n",
    "    project=config.HOPSWORKS_PROJECT_NAME)\n",
    "\n",
    "# Conectar al feature store\n",
    "feature_store = project.get_feature_store()\n",
    "\n",
    "# Conectar al Feature Group histórico\n",
    "try:\n",
    "    feature_group = feature_store.get_feature_group(\n",
    "        name=config.FEATURE_GROUP_NAME,\n",
    "        version=config.FEATURE_GROUP_VERSION,\n",
    "        \n",
    "    )\n",
    "    if feature_group is None:\n",
    "        raise Exception(\"El Feature Group histórico no existe o el nombre/version no coinciden exactamente. Verifica en Hopsworks.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al crear/conectar el Feature Group: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f320788",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Añadir solo la nueva semana al Feature Group histórico (evitando duplicados)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transformar_a_series_temporales\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Supón que df contiene solo la última semana descargada\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Workspace\\mlops_fleca_project\\src\\data_utils.py:6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpq\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Añadir solo la nueva semana al Feature Group histórico (evitando duplicados)\n",
    "from src.data_utils import transformar_a_series_temporales\n",
    "import pandas as pd\n",
    "\n",
    "# Supón que df contiene solo la última semana descargada\n",
    "df_ts = transformar_a_series_temporales(df, familia=\"BOLLERIA\")\n",
    "ultima_semana = df_ts['week_start'].max()\n",
    "print(f\"Semana a insertar: {ultima_semana}\")\n",
    "\n",
    "# --- FIX: Conversión explícita de tipos antes del insert ---\n",
    "# Eliminar columna 'fecha' si existe\n",
    "if 'fecha' in df_ts.columns:\n",
    "    df_ts = df_ts.drop(columns=['fecha'])\n",
    "# Ajustar tipos para coincidir con el schema del Feature Group histórico\n",
    "# (esto es CRÍTICO para evitar errores de tipo en Hopsworks)\n",
    "df_ts['year'] = df_ts['year'].astype('int64')  # bigint\n",
    "df_ts['week'] = df_ts['week'].astype('int64')  # bigint\n",
    "df_ts['familia'] = df_ts['familia'].astype('string')  # string\n",
    "df_ts['base_imponible'] = df_ts['base_imponible'].astype('float64')  # double\n",
    "df_ts['is_summer_peak'] = df_ts['is_summer_peak'].astype('int32')  # int (¡IMPORTANTE!)\n",
    "df_ts['is_easter'] = df_ts['is_easter'].astype('int64')  # bigint\n",
    "df_ts['week_start'] = pd.to_datetime(df_ts['week_start'])  # timestamp\n",
    "print(df_ts.dtypes)\n",
    "print(df_ts.head())\n",
    "# --- FIN FIX ---\n",
    "\n",
    "# Conectar a Hopsworks y al Feature Store\n",
    "from src.inference import conectar_hopsworks_feature_store\n",
    "import src.config as config\n",
    "proyecto, feature_store = conectar_hopsworks_feature_store()\n",
    "feature_group = feature_store.get_feature_group(\n",
    "    name=config.FEATURE_GROUP_NAME,\n",
    "    version=config.FEATURE_GROUP_VERSION,\n",
    ")\n",
    "\n",
    "# Leer semanas ya presentes en el Feature Group\n",
    "df_hopsworks = feature_group.read()\n",
    "semanas_existentes = set(df_hopsworks['week_start'].values)\n",
    "\n",
    "# Filtrar solo las filas de la nueva semana que no estén ya en Hopsworks\n",
    "nuevas_filas = df_ts[~df_ts['week_start'].isin(semanas_existentes)]\n",
    "if nuevas_filas.empty:\n",
    "    print(\"La semana ya existe en el Feature Group. No se inserta nada.\")\n",
    "else:\n",
    "    print(f\"Insertando {len(nuevas_filas)} filas nuevas en el Feature Group.\")\n",
    "    feature_group.insert(nuevas_filas, write_options={'wait_for_job': True})\n",
    "    print(\"Inserción incremental realizada correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f741ada1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-21 19:58:44,192 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-09-21 19:58:44,195 INFO: Initializing external client\n",
      "2025-09-21 19:58:44,196 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "Connection closed.\n",
      "2025-09-21 19:58:44,195 INFO: Initializing external client\n",
      "2025-09-21 19:58:44,196 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-21 19:58:45,246 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1242272\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1242272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JobWarning: All jobs associated to feature view `times_series_bolleria_feature_view`, version `1` will be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature View 'times_series_bolleria_feature_view' eliminado correctamente.\n",
      "Feature view created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1242272/fs/1224799/fv/times_series_bolleria_feature_view/version/1\n",
      "Feature View 'times_series_bolleria_feature_view' creado y actualizado correctamente.\n",
      "Feature View de histórico actualizado correctamente.\n",
      "Feature view created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1242272/fs/1224799/fv/times_series_bolleria_feature_view/version/1\n",
      "Feature View 'times_series_bolleria_feature_view' creado y actualizado correctamente.\n",
      "Feature View de histórico actualizado correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Actualizar el Feature View de histórico tras insertar los datos en el Feature Group\n",
    "from src.inference import conectar_hopsworks_feature_store\n",
    "from scripts.update_feature_view import update_feature_view\n",
    "import src.config as config\n",
    "\n",
    "# Conexión a Hopsworks\n",
    "proyecto, feature_store = conectar_hopsworks_feature_store()\n",
    "update_feature_view(\n",
    "    feature_store,\n",
    "    config.HISTORICAL_FEATURE_GROUP_METADATA,\n",
    "    config.HISTORICAL_FEATURE_VIEW_METADATA\n",
    " )\n",
    "print(\"Feature View de histórico actualizado correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb24ce5",
   "metadata": {},
   "source": [
    "FIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e68140a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.2-cp311-cp311-win_amd64.whl (11.3 MB)\n",
      "     --------------------------------------- 11.3/11.3 MB 54.7 MB/s eta 0:00:00\n",
      "Collecting numpy>=1.23.2\n",
      "  Downloading numpy-2.3.3-cp311-cp311-win_amd64.whl (13.1 MB)\n",
      "     --------------------------------------- 13.1/13.1 MB 54.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.3.3 pandas-2.3.2 pytz-2025.2 tzdata-2025.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "# Instalar pandas en el entorno del notebook gestionado por Poetry\n",
    "!poetry run pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7d288b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-win_amd64.whl (26.2 MB)\n",
      "     --------------------------------------- 26.2/26.2 MB 46.9 MB/s eta 0:00:00\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-21.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "# Instalar pyarrow en el entorno del notebook gestionado por Poetry\n",
    "!poetry run pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68fa2916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-bigquery\n",
      "  Downloading google_cloud_bigquery-3.38.0-py3-none-any.whl (259 kB)\n",
      "     -------------------------------------- 259.3/259.3 kB 5.3 MB/s eta 0:00:00\n",
      "Collecting google-api-core[grpc]<3.0.0,>=2.11.1\n",
      "  Downloading google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
      "     -------------------------------------- 160.8/160.8 kB 9.4 MB/s eta 0:00:00\n",
      "Collecting google-auth<3.0.0,>=2.14.1\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "     ------------------------------------- 216.1/216.1 kB 12.9 MB/s eta 0:00:00\n",
      "Collecting google-cloud-core<3.0.0,>=2.4.1\n",
      "  Downloading google_cloud_core-2.4.3-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-resumable-media<3.0.0,>=2.0.0\n",
      "  Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "     ---------------------------------------- 81.3/81.3 kB 4.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=24.2.0 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from google-cloud-bigquery) (25.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from google-cloud-bigquery) (2.9.0.post0)\n",
      "Collecting requests<3.0.0,>=2.21.0\n",
      "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "     ---------------------------------------- 64.7/64.7 kB ? eta 0:00:00\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "     ------------------------------------- 294.5/294.5 kB 17.8 MB/s eta 0:00:00\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5\n",
      "  Downloading protobuf-6.32.1-cp310-abi3-win_amd64.whl (435 kB)\n",
      "     ------------------------------------- 435.7/435.7 kB 26.6 MB/s eta 0:00:00\n",
      "Collecting proto-plus<2.0.0,>=1.22.3\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "     ---------------------------------------- 50.2/50.2 kB ? eta 0:00:00\n",
      "Collecting grpcio<2.0.0,>=1.33.2\n",
      "  Downloading grpcio-1.75.1-cp311-cp311-win_amd64.whl (4.6 MB)\n",
      "     ---------------------------------------- 4.6/4.6 MB 49.3 MB/s eta 0:00:00\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2\n",
      "  Downloading grpcio_status-1.75.1-py3-none-any.whl (14 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "     ---------------------------------------- 181.3/181.3 kB ? eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.7.1-cp311-cp311-win_amd64.whl (33 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery) (1.17.0)\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.3-cp311-cp311-win_amd64.whl (107 kB)\n",
      "     ---------------------------------------- 107.1/107.1 kB ? eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "     ---------------------------------------- 70.4/70.4 kB ? eta 0:00:00\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "     ---------------------------------------- 129.8/129.8 kB ? eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "     ---------------------------------------- 161.2/161.2 kB ? eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions~=4.12 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from grpcio<2.0.0,>=1.33.2->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (4.15.0)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "     ---------------------------------------- 83.1/83.1 kB 4.9 MB/s eta 0:00:00\n",
      "Installing collected packages: urllib3, pyasn1, protobuf, idna, grpcio, google-crc32c, charset_normalizer, certifi, cachetools, rsa, requests, pyasn1-modules, proto-plus, googleapis-common-protos, google-resumable-media, grpcio-status, google-auth, google-api-core, google-cloud-core, google-cloud-bigquery\n",
      "Successfully installed cachetools-5.5.2 certifi-2025.8.3 charset_normalizer-3.4.3 google-api-core-2.25.1 google-auth-2.40.3 google-cloud-bigquery-3.38.0 google-cloud-core-2.4.3 google-crc32c-1.7.1 google-resumable-media-2.7.2 googleapis-common-protos-1.70.0 grpcio-1.75.1 grpcio-status-1.75.1 idna-3.10 proto-plus-1.26.1 protobuf-6.32.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 requests-2.32.5 rsa-4.9.1 urllib3-2.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "# Instalar google-cloud-bigquery en el entorno del notebook gestionado por Poetry\n",
    "!poetry run pip install google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10ea258c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "# Instalar python-dotenv en el entorno del notebook gestionado por Poetry\n",
    "!poetry run pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d0002dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hopsworks\n",
      "  Downloading hopsworks-4.4.2-py3-none-any.whl (691 kB)\n",
      "     -------------------------------------- 691.6/691.6 kB 8.7 MB/s eta 0:00:00\n",
      "Collecting pyhumps==1.6.1\n",
      "  Downloading pyhumps-1.6.1-py3-none-any.whl (5.0 kB)\n",
      "Requirement already satisfied: requests in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from hopsworks) (2.32.5)\n",
      "Collecting furl\n",
      "  Downloading furl-2.1.4-py2.py3-none-any.whl (27 kB)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.40.40-py3-none-any.whl (139 kB)\n",
      "     ---------------------------------------- 139.3/139.3 kB ? eta 0:00:00\n",
      "Collecting pandas[mysql]<2.3.0\n",
      "  Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "     --------------------------------------- 11.6/11.6 MB 36.4 MB/s eta 0:00:00\n",
      "Collecting numpy<2\n",
      "  Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "     --------------------------------------- 15.8/15.8 MB 54.4 MB/s eta 0:00:00\n",
      "Collecting pyjks\n",
      "  Downloading pyjks-20.0.0-py2.py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 45.3/45.3 kB ? eta 0:00:00\n",
      "Collecting mock\n",
      "  Downloading mock-5.2.0-py3-none-any.whl (31 kB)\n",
      "Collecting avro==1.11.3\n",
      "  Downloading avro-1.11.3.tar.gz (90 kB)\n",
      "     ---------------------------------------- 90.6/90.6 kB ? eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting PyMySQL[rsa]\n",
      "  Downloading pymysql-1.1.2-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 45.3/45.3 kB ? eta 0:00:00\n",
      "Collecting tzlocal\n",
      "  Downloading tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "     ------------------------------------- 199.3/199.3 kB 11.8 MB/s eta 0:00:00\n",
      "Collecting retrying\n",
      "  Downloading retrying-1.4.2-py3-none-any.whl (10 kB)\n",
      "Collecting hopsworks_aiomysql[sa]==0.2.1\n",
      "  Downloading hopsworks_aiomysql-0.2.1-py3-none-any.whl (44 kB)\n",
      "     ---------------------------------------- 44.2/44.2 kB ? eta 0:00:00\n",
      "Collecting opensearch-py<=2.4.2,>=1.1.0\n",
      "  Downloading opensearch_py-2.4.2-py2.py3-none-any.whl (258 kB)\n",
      "     ------------------------------------- 258.6/258.6 kB 15.5 MB/s eta 0:00:00\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.49.1 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from hopsworks) (1.75.1)\n",
      "Collecting protobuf<5.0.0,>=4.25.4\n",
      "  Downloading protobuf-4.25.8-cp310-abi3-win_amd64.whl (413 kB)\n",
      "     ---------------------------------------- 413.7/413.7 kB ? eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from hopsworks) (25.0)\n",
      "Collecting sqlalchemy<=2.0.29,>=1.3\n",
      "  Downloading SQLAlchemy-2.0.29-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 16.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions~=4.12 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from grpcio<2.0.0,>=1.49.1->hopsworks) (4.15.0)\n",
      "Requirement already satisfied: urllib3>=1.26.18 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks) (2.5.0)\n",
      "Requirement already satisfied: six in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks) (2.9.0.post0)\n",
      "Requirement already satisfied: certifi>=2022.12.07 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks) (2025.8.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from pandas[mysql]<2.3.0->hopsworks) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from pandas[mysql]<2.3.0->hopsworks) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from requests->hopsworks) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from requests->hopsworks) (3.10)\n",
      "Collecting botocore<1.41.0,>=1.40.40\n",
      "  Downloading botocore-1.40.40-py3-none-any.whl (14.0 MB)\n",
      "     --------------------------------------- 14.0/14.0 MB 54.4 MB/s eta 0:00:00\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.15.0,>=0.14.0\n",
      "  Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
      "     ---------------------------------------- 85.7/85.7 kB 5.0 MB/s eta 0:00:00\n",
      "Collecting orderedmultidict>=1.0.1\n",
      "  Downloading orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting javaobj-py3\n",
      "  Downloading javaobj_py3-0.4.4-py2.py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.1/57.1 kB ? eta 0:00:00\n",
      "Requirement already satisfied: pyasn1>=0.3.5 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from pyjks->hopsworks) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from pyjks->hopsworks) (0.4.2)\n",
      "Collecting pycryptodomex\n",
      "  Downloading pycryptodomex-3.23.0-cp37-abi3-win_amd64.whl (1.8 MB)\n",
      "     ---------------------------------------- 1.8/1.8 MB 56.0 MB/s eta 0:00:00\n",
      "Collecting twofish\n",
      "  Downloading twofish-0.3.0.tar.gz (26 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting cryptography\n",
      "  Downloading cryptography-46.0.1-cp311-abi3-win_amd64.whl (3.5 MB)\n",
      "     ---------------------------------------- 3.5/3.5 MB 55.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from tqdm->hopsworks) (0.4.6)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-3.2.4-cp311-cp311-win_amd64.whl (299 kB)\n",
      "     ---------------------------------------- 299.1/299.1 kB ? eta 0:00:00\n",
      "Collecting cffi>=2.0.0\n",
      "  Downloading cffi-2.0.0-cp311-cp311-win_amd64.whl (182 kB)\n",
      "     ---------------------------------------- 182.8/182.8 kB ? eta 0:00:00\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "     ---------------------------------------- 118.1/118.1 kB ? eta 0:00:00\n",
      "Building wheels for collected packages: avro\n",
      "  Building wheel for avro (pyproject.toml): started\n",
      "  Building wheel for avro (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for avro: filename=avro-1.11.3-py2.py3-none-any.whl size=123978 sha256=df6d70450c87a5377d9b6139d5315e2f7d20d27dc283653cd21b067721ab1f8e\n",
      "  Stored in directory: c:\\users\\jordi\\appdata\\local\\pip\\cache\\wheels\\f2\\0e\\fb\\bea3232dc4aa9c683a98c6b4247925872992a0aa0d9f6bfac6\n",
      "Successfully built avro\n",
      "Installing collected packages: twofish, pyhumps, javaobj-py3, tzlocal, tqdm, retrying, PyMySQL, pycryptodomex, pycparser, protobuf, orderedmultidict, numpy, mock, jmespath, greenlet, fsspec, avro, sqlalchemy, pyjks, pandas, opensearch-py, hopsworks_aiomysql, furl, cffi, botocore, s3transfer, cryptography, boto3, hopsworks\n",
      "  Running setup.py install for twofish: started\n",
      "  Running setup.py install for twofish: finished with status 'done'\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 6.32.1\n",
      "    Uninstalling protobuf-6.32.1:\n",
      "      Successfully uninstalled protobuf-6.32.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: twofish is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Acceso denegado: 'C:\\\\Workspace\\\\mlops_fleca_project\\\\.venv\\\\lib\\\\site-packages\\\\google\\\\~upb\\\\_message.pyd'\n",
      "Check the permissions.\n",
      "\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "# Instalar hopsworks en el entorno del notebook gestionado por Poetry\n",
    "!poetry run pip install hopsworks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
