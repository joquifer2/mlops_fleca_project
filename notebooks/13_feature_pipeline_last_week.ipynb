{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a696f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Añade el directorio raíz del proyecto al sys.path\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cd32205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG ENV HOPSWORKS_PROJECT_NAME: fleca_mlops\n",
      "DEBUG ENV PATH: C:\\Workspace\\mlops_fleca_project\\.env\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Workspace\\mlops_fleca_project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from src.data_utils import load_raw_data, transformar_a_series_temporales\n",
    "from src import config\n",
    "import hopsworks\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2086e648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha actual: 2025-09-28 Sunday\n",
      "Última semana completa:\n",
      "  Lunes: 2025-09-15 Monday\n",
      "  Domingo: 2025-09-21 Sunday\n",
      "  Rango: 2025-09-15 a 2025-09-21\n"
     ]
    }
   ],
   "source": [
    "# 2. Calcular la última semana completa (lunes a domingo)\n",
    "\n",
    "hoy = datetime.utcnow()\n",
    "print(f\"Fecha actual: {hoy.strftime('%Y-%m-%d %A')}\")\n",
    "\n",
    "# Calcular cuántos días han pasado desde el lunes de esta semana\n",
    "dias_desde_lunes = hoy.weekday()  # 0=lunes, 1=martes, ..., 6=domingo\n",
    "\n",
    "# Ir al lunes de la semana pasada (última semana completa)\n",
    "ultimo_lunes = (hoy - timedelta(days=dias_desde_lunes + 7)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "ultimo_domingo = ultimo_lunes + timedelta(days=6)\n",
    "\n",
    "print(f\"Última semana completa:\")\n",
    "print(f\"  Lunes: {ultimo_lunes.strftime('%Y-%m-%d %A')}\")\n",
    "print(f\"  Domingo: {ultimo_domingo.strftime('%Y-%m-%d %A')}\")\n",
    "print(f\"  Rango: {ultimo_lunes.strftime('%Y-%m-%d')} a {ultimo_domingo.strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a81d706b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando datos desde BigQuery porque descargar_bq=True o no existe el archivo C:\\Workspace\\mlops_fleca_project\\data\\raw\\raw_data_bq_forecasting.parquet\n",
      "Iniciando conexión con BigQuery...\n",
      "Conexión establecida.\n",
      "Usando fechas en consulta SQL: fecha_inicio='2025-09-15' y fecha_fin='2025-09-21'\n",
      "Descargando datos de fleca-del-port.fleca_ventas_dia.t_facturas_dia_extendida_2023 ...\n",
      "Ejecutando consulta SQL:\n",
      "\n",
      "    SELECT \n",
      "        fecha,\n",
      "        n_factura,\n",
      "        zona_de_venta,\n",
      "        producto,\n",
      "        familia,\n",
      "        cantidad,\n",
      "        base_imponible,\n",
      "        tipo_IVA,\n",
      "        total\n",
      "    FROM `fleca-del-port.fleca_ventas_dia.t_facturas_dia_extendida_2023`\n",
      "    WHERE fecha >= '2025-09-15' AND fecha <= '2025-09-21'\n",
      "Conexión establecida.\n",
      "Usando fechas en consulta SQL: fecha_inicio='2025-09-15' y fecha_fin='2025-09-21'\n",
      "Descargando datos de fleca-del-port.fleca_ventas_dia.t_facturas_dia_extendida_2023 ...\n",
      "Ejecutando consulta SQL:\n",
      "\n",
      "    SELECT \n",
      "        fecha,\n",
      "        n_factura,\n",
      "        zona_de_venta,\n",
      "        producto,\n",
      "        familia,\n",
      "        cantidad,\n",
      "        base_imponible,\n",
      "        tipo_IVA,\n",
      "        total\n",
      "    FROM `fleca-del-port.fleca_ventas_dia.t_facturas_dia_extendida_2023`\n",
      "    WHERE fecha >= '2025-09-15' AND fecha <= '2025-09-21'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas descargadas de la segunda tabla: 2443\n",
      "Guardando archivo en C:\\Workspace\\mlops_fleca_project\\data\\raw\\raw_data_bq_forecasting_20250928.parquet ...\n",
      "Archivo guardado correctamente.\n",
      "Usando archivo recién generado: C:\\Workspace\\mlops_fleca_project\\data\\raw\\raw_data_bq_forecasting_20250928.parquet\n",
      "Cargando datos desde: C:\\Workspace\\mlops_fleca_project\\data\\raw\\raw_data_bq_forecasting_20250928.parquet\n",
      "Validando fechas entre 2025-09-15 y 2025-09-21 (7 días)\n",
      "Total de fechas faltantes: 0\n",
      "No faltan fechas en el rango especificado.\n",
      "Datos descargados: (2443, 11)\n"
     ]
    }
   ],
   "source": [
    "# 3. Descargar y cargar datos de BigQuery para esa semana\n",
    "df = load_raw_data(\n",
    "    fecha_inicio=ultimo_lunes.strftime('%Y-%m-%d'),\n",
    "    fecha_fin=ultimo_domingo.strftime('%Y-%m-%d'),\n",
    "    descargar_bq=True\n",
    ")\n",
    "print('Datos descargados:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "882b4ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series temporales generadas: (1, 8)\n",
      "   year  week   familia  base_imponible  is_summer_peak  is_easter  \\\n",
      "0  2025    38  BOLLERIA          734.32               0          0   \n",
      "\n",
      "   dias_semana week_start  \n",
      "0            7 2025-09-15  \n"
     ]
    }
   ],
   "source": [
    "# 4. Transformar a series temporales semanales solo para la familia BOLLERIA\n",
    "df_ts = transformar_a_series_temporales(df, familia=\"BOLLERIA\")\n",
    "print('Series temporales generadas:', df_ts.shape)\n",
    "print(df_ts.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c09cb7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year                       int64\n",
      "week                       int64\n",
      "familia           string[python]\n",
      "base_imponible           float64\n",
      "is_summer_peak             int32\n",
      "is_easter                  int64\n",
      "dias_semana                int64\n",
      "week_start        datetime64[ns]\n",
      "dtype: object\n",
      "   year  week   familia  base_imponible  is_summer_peak  is_easter  \\\n",
      "0  2025    38  BOLLERIA          734.32               0          0   \n",
      "\n",
      "   dias_semana week_start  \n",
      "0            7 2025-09-15  \n"
     ]
    }
   ],
   "source": [
    "# Eliminar columna 'fecha' si existe\n",
    "if 'fecha' in df_ts.columns:\n",
    "    df_ts = df_ts.drop(columns=['fecha'])\n",
    "\n",
    "# Ajustar tipos para coincidir con el schema del Feature Group histórico\n",
    "df_ts['year'] = df_ts['year'].astype('int64')  # bigint\n",
    "df_ts['week'] = df_ts['week'].astype('int64')  # bigint\n",
    "df_ts['familia'] = df_ts['familia'].astype('string')  # string\n",
    "df_ts['base_imponible'] = df_ts['base_imponible'].astype('float64')  # double\n",
    "df_ts['is_summer_peak'] = df_ts['is_summer_peak'].astype('int32')  # int\n",
    "df_ts['is_easter'] = df_ts['is_easter'].astype('int64')  # bigint\n",
    "df_ts['week_start'] = pd.to_datetime(df_ts['week_start'])  # timestamp\n",
    "\n",
    "print(df_ts.dtypes)\n",
    "print(df_ts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5142035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-28 20:32:08,115 INFO: Initializing external client\n",
      "2025-09-28 20:32:08,116 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-09-28 20:32:08,116 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "UserWarning: The installed hopsworks client version 4.4.2 may not be compatible with the connected Hopsworks backend version 4.2.2. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-28 20:32:09,239 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1242272\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1242272\n"
     ]
    }
   ],
   "source": [
    "# 5. Conectar a hopsworks\n",
    "project = hopsworks.login(\n",
    "    api_key_value=config.HOPSWORKS_API_KEY, \n",
    "    project=config.HOPSWORKS_PROJECT_NAME)\n",
    "\n",
    "# Conectar al feature store\n",
    "feature_store = project.get_feature_store()\n",
    "\n",
    "# Conectar al Feature Group histórico\n",
    "try:\n",
    "    feature_group = feature_store.get_feature_group(\n",
    "        name=config.FEATURE_GROUP_NAME,\n",
    "        version=config.FEATURE_GROUP_VERSION,\n",
    "        \n",
    "    )\n",
    "    if feature_group is None:\n",
    "        raise Exception(\"El Feature Group histórico no existe o el nombre/version no coinciden exactamente. Verifica en Hopsworks.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al crear/conectar el Feature Group: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f320788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semana a insertar: 2025-09-15 00:00:00\n",
      "year                       int64\n",
      "week                       int64\n",
      "familia           string[python]\n",
      "base_imponible           float64\n",
      "is_summer_peak             int32\n",
      "is_easter                  int64\n",
      "dias_semana                int64\n",
      "week_start        datetime64[ns]\n",
      "dtype: object\n",
      "   year  week   familia  base_imponible  is_summer_peak  is_easter  \\\n",
      "0  2025    38  BOLLERIA          734.32               0          0   \n",
      "\n",
      "   dias_semana week_start  \n",
      "0            7 2025-09-15  \n",
      "2025-09-28 20:32:17,970 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-09-28 20:32:17,972 INFO: Initializing external client\n",
      "2025-09-28 20:32:17,972 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-09-28 20:32:17,970 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-09-28 20:32:17,972 INFO: Initializing external client\n",
      "2025-09-28 20:32:17,972 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "UserWarning: The installed hopsworks client version 4.4.2 may not be compatible with the connected Hopsworks backend version 4.2.2. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-28 20:32:19,296 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1242272\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1242272\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.62s) \n",
      "Insertando 1 filas nuevas en el Feature Group.\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.62s) \n",
      "Insertando 1 filas nuevas en el Feature Group.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 1/1 | Elapsed Time: 00:00 | Remaining Time: 00:00\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: times_series_bolleria_feature_group_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1242272/jobs/named/times_series_bolleria_feature_group_1_offline_fg_materialization/executions\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1242272/jobs/named/times_series_bolleria_feature_group_1_offline_fg_materialization/executions\n",
      "2025-09-28 20:32:37,328 INFO: Waiting for execution to finish. Current state: INITIALIZING. Final status: UNDEFINED\n",
      "2025-09-28 20:32:37,328 INFO: Waiting for execution to finish. Current state: INITIALIZING. Final status: UNDEFINED\n",
      "2025-09-28 20:32:40,600 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2025-09-28 20:32:40,600 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2025-09-28 20:32:43,820 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2025-09-28 20:32:43,820 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2025-09-28 20:34:23,404 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS. Final status: SUCCEEDED\n",
      "2025-09-28 20:34:23,404 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS. Final status: SUCCEEDED\n",
      "2025-09-28 20:34:23,561 INFO: Waiting for log aggregation to finish.\n",
      "2025-09-28 20:34:23,561 INFO: Waiting for log aggregation to finish.\n",
      "2025-09-28 20:34:38,920 INFO: Execution finished successfully.\n",
      "Inserción incremental realizada correctamente.\n",
      "2025-09-28 20:34:38,920 INFO: Execution finished successfully.\n",
      "Inserción incremental realizada correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Añadir solo la nueva semana al Feature Group histórico (evitando duplicados)\n",
    "from src.data_utils import transformar_a_series_temporales\n",
    "import pandas as pd\n",
    "\n",
    "# Supón que df contiene solo la última semana descargada\n",
    "df_ts = transformar_a_series_temporales(df, familia=\"BOLLERIA\")\n",
    "ultima_semana = df_ts['week_start'].max()\n",
    "print(f\"Semana a insertar: {ultima_semana}\")\n",
    "\n",
    "# --- FIX: Conversión explícita de tipos antes del insert ---\n",
    "# Eliminar columna 'fecha' si existe\n",
    "if 'fecha' in df_ts.columns:\n",
    "    df_ts = df_ts.drop(columns=['fecha'])\n",
    "# Ajustar tipos para coincidir con el schema del Feature Group histórico\n",
    "# (esto es CRÍTICO para evitar errores de tipo en Hopsworks)\n",
    "df_ts['year'] = df_ts['year'].astype('int64')  # bigint\n",
    "df_ts['week'] = df_ts['week'].astype('int64')  # bigint\n",
    "df_ts['familia'] = df_ts['familia'].astype('string')  # string\n",
    "df_ts['base_imponible'] = df_ts['base_imponible'].astype('float64')  # double\n",
    "df_ts['is_summer_peak'] = df_ts['is_summer_peak'].astype('int32')  # int (¡IMPORTANTE!)\n",
    "df_ts['is_easter'] = df_ts['is_easter'].astype('int64')  # bigint\n",
    "df_ts['week_start'] = pd.to_datetime(df_ts['week_start'])  # timestamp\n",
    "print(df_ts.dtypes)\n",
    "print(df_ts.head())\n",
    "# --- FIN FIX ---\n",
    "\n",
    "# Conectar a Hopsworks y al Feature Store\n",
    "from src.inference import conectar_hopsworks_feature_store\n",
    "import src.config as config\n",
    "proyecto, feature_store = conectar_hopsworks_feature_store()\n",
    "feature_group = feature_store.get_feature_group(\n",
    "    name=config.FEATURE_GROUP_NAME,\n",
    "    version=config.FEATURE_GROUP_VERSION,\n",
    ")\n",
    "\n",
    "# Leer semanas ya presentes en el Feature Group\n",
    "df_hopsworks = feature_group.read()\n",
    "semanas_existentes = set(df_hopsworks['week_start'].values)\n",
    "\n",
    "# Filtrar solo las filas de la nueva semana que no estén ya en Hopsworks\n",
    "nuevas_filas = df_ts[~df_ts['week_start'].isin(semanas_existentes)]\n",
    "if nuevas_filas.empty:\n",
    "    print(\"La semana ya existe en el Feature Group. No se inserta nada.\")\n",
    "else:\n",
    "    print(f\"Insertando {len(nuevas_filas)} filas nuevas en el Feature Group.\")\n",
    "    feature_group.insert(nuevas_filas, write_options={'wait_for_job': True})\n",
    "    print(\"Inserción incremental realizada correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f741ada1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-28 20:34:42,701 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-09-28 20:34:42,704 INFO: Initializing external client\n",
      "2025-09-28 20:34:42,705 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "Connection closed.\n",
      "2025-09-28 20:34:42,704 INFO: Initializing external client\n",
      "2025-09-28 20:34:42,705 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-28 20:34:43,911 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1242272\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1242272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JobWarning: All jobs associated to feature view `times_series_bolleria_feature_view`, version `1` will be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature View 'times_series_bolleria_feature_view' eliminado correctamente.\n",
      "Feature view created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1242272/fs/1224799/fv/times_series_bolleria_feature_view/version/1\n",
      "Feature View 'times_series_bolleria_feature_view' creado y actualizado correctamente.\n",
      "Feature View de histórico actualizado correctamente.\n",
      "Feature view created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1242272/fs/1224799/fv/times_series_bolleria_feature_view/version/1\n",
      "Feature View 'times_series_bolleria_feature_view' creado y actualizado correctamente.\n",
      "Feature View de histórico actualizado correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Actualizar el Feature View de histórico tras insertar los datos en el Feature Group\n",
    "from src.inference import conectar_hopsworks_feature_store\n",
    "from scripts.update_feature_view import update_feature_view\n",
    "import src.config as config\n",
    "\n",
    "# Conexión a Hopsworks\n",
    "proyecto, feature_store = conectar_hopsworks_feature_store()\n",
    "update_feature_view(\n",
    "    feature_store,\n",
    "    config.HISTORICAL_FEATURE_GROUP_METADATA,\n",
    "    config.HISTORICAL_FEATURE_VIEW_METADATA\n",
    " )\n",
    "print(\"Feature View de histórico actualizado correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb24ce5",
   "metadata": {},
   "source": [
    "FIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e68140a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.2-cp311-cp311-win_amd64.whl (11.3 MB)\n",
      "     --------------------------------------- 11.3/11.3 MB 54.7 MB/s eta 0:00:00\n",
      "Collecting numpy>=1.23.2\n",
      "  Downloading numpy-2.3.3-cp311-cp311-win_amd64.whl (13.1 MB)\n",
      "     --------------------------------------- 13.1/13.1 MB 54.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.3.3 pandas-2.3.2 pytz-2025.2 tzdata-2025.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "# Instalar pandas en el entorno del notebook gestionado por Poetry\n",
    "!poetry run pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7d288b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-win_amd64.whl (26.2 MB)\n",
      "     --------------------------------------- 26.2/26.2 MB 46.9 MB/s eta 0:00:00\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-21.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "# Instalar pyarrow en el entorno del notebook gestionado por Poetry\n",
    "!poetry run pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68fa2916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-bigquery\n",
      "  Downloading google_cloud_bigquery-3.38.0-py3-none-any.whl (259 kB)\n",
      "     -------------------------------------- 259.3/259.3 kB 5.3 MB/s eta 0:00:00\n",
      "Collecting google-api-core[grpc]<3.0.0,>=2.11.1\n",
      "  Downloading google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
      "     -------------------------------------- 160.8/160.8 kB 9.4 MB/s eta 0:00:00\n",
      "Collecting google-auth<3.0.0,>=2.14.1\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "     ------------------------------------- 216.1/216.1 kB 12.9 MB/s eta 0:00:00\n",
      "Collecting google-cloud-core<3.0.0,>=2.4.1\n",
      "  Downloading google_cloud_core-2.4.3-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-resumable-media<3.0.0,>=2.0.0\n",
      "  Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "     ---------------------------------------- 81.3/81.3 kB 4.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=24.2.0 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from google-cloud-bigquery) (25.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from google-cloud-bigquery) (2.9.0.post0)\n",
      "Collecting requests<3.0.0,>=2.21.0\n",
      "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "     ---------------------------------------- 64.7/64.7 kB ? eta 0:00:00\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "     ------------------------------------- 294.5/294.5 kB 17.8 MB/s eta 0:00:00\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5\n",
      "  Downloading protobuf-6.32.1-cp310-abi3-win_amd64.whl (435 kB)\n",
      "     ------------------------------------- 435.7/435.7 kB 26.6 MB/s eta 0:00:00\n",
      "Collecting proto-plus<2.0.0,>=1.22.3\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "     ---------------------------------------- 50.2/50.2 kB ? eta 0:00:00\n",
      "Collecting grpcio<2.0.0,>=1.33.2\n",
      "  Downloading grpcio-1.75.1-cp311-cp311-win_amd64.whl (4.6 MB)\n",
      "     ---------------------------------------- 4.6/4.6 MB 49.3 MB/s eta 0:00:00\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2\n",
      "  Downloading grpcio_status-1.75.1-py3-none-any.whl (14 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "     ---------------------------------------- 181.3/181.3 kB ? eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.7.1-cp311-cp311-win_amd64.whl (33 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery) (1.17.0)\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.3-cp311-cp311-win_amd64.whl (107 kB)\n",
      "     ---------------------------------------- 107.1/107.1 kB ? eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "     ---------------------------------------- 70.4/70.4 kB ? eta 0:00:00\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "     ---------------------------------------- 129.8/129.8 kB ? eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "     ---------------------------------------- 161.2/161.2 kB ? eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions~=4.12 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from grpcio<2.0.0,>=1.33.2->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (4.15.0)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "     ---------------------------------------- 83.1/83.1 kB 4.9 MB/s eta 0:00:00\n",
      "Installing collected packages: urllib3, pyasn1, protobuf, idna, grpcio, google-crc32c, charset_normalizer, certifi, cachetools, rsa, requests, pyasn1-modules, proto-plus, googleapis-common-protos, google-resumable-media, grpcio-status, google-auth, google-api-core, google-cloud-core, google-cloud-bigquery\n",
      "Successfully installed cachetools-5.5.2 certifi-2025.8.3 charset_normalizer-3.4.3 google-api-core-2.25.1 google-auth-2.40.3 google-cloud-bigquery-3.38.0 google-cloud-core-2.4.3 google-crc32c-1.7.1 google-resumable-media-2.7.2 googleapis-common-protos-1.70.0 grpcio-1.75.1 grpcio-status-1.75.1 idna-3.10 proto-plus-1.26.1 protobuf-6.32.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 requests-2.32.5 rsa-4.9.1 urllib3-2.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "# Instalar google-cloud-bigquery en el entorno del notebook gestionado por Poetry\n",
    "!poetry run pip install google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10ea258c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "# Instalar python-dotenv en el entorno del notebook gestionado por Poetry\n",
    "!poetry run pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d0002dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hopsworks"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached hopsworks-4.4.2-py3-none-any.whl (691 kB)\n",
      "Requirement already satisfied: pyhumps==1.6.1 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from hopsworks) (1.6.1)\n",
      "Requirement already satisfied: requests in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from hopsworks) (2.32.3)\n",
      "Collecting furl\n",
      "  Using cached furl-2.1.4-py2.py3-none-any.whl (27 kB)\n",
      "Collecting boto3\n",
      "  Using cached boto3-1.40.40-py3-none-any.whl (139 kB)\n",
      "Requirement already satisfied: pandas[mysql]<2.3.0 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from hopsworks) (2.2.3)\n",
      "Requirement already satisfied: numpy<2 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from hopsworks) (1.26.4)\n",
      "Requirement already satisfied: pyjks in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from hopsworks) (20.0.0)\n",
      "Requirement already satisfied: mock in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from hopsworks) (5.2.0)\n",
      "Requirement already satisfied: avro==1.11.3 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from hopsworks) (1.11.3)\n",
      "Requirement already satisfied: PyMySQL[rsa] in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from hopsworks) (1.1.2)\n",
      "Requirement already satisfied: tzlocal in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from hopsworks) (5.3.1)\n",
      "Requirement already satisfied: fsspec in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from hopsworks) (2025.9.0)\n",
      "Requirement already satisfied: retrying in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from hopsworks) (1.4.2)\n",
      "Requirement already satisfied: hopsworks_aiomysql[sa]==0.2.1 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from hopsworks) (0.2.1)\n",
      "Collecting opensearch-py<=2.4.2,>=1.1.0\n",
      "  Using cached opensearch_py-2.4.2-py2.py3-none-any.whl (258 kB)\n",
      "Requirement already satisfied: tqdm in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from hopsworks) (4.67.1)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.49.1 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from hopsworks) (1.75.1)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.25.4 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from hopsworks) (4.25.8)\n",
      "Requirement already satisfied: packaging in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from hopsworks) (24.2)\n",
      "Requirement already satisfied: sqlalchemy<=2.0.29,>=1.3 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from hopsworks_aiomysql[sa]==0.2.1->hopsworks) (2.0.29)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from grpcio<2.0.0,>=1.49.1->hopsworks) (4.13.0)\n",
      "Requirement already satisfied: urllib3>=1.26.18 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks) (2.3.0)\n",
      "Requirement already satisfied: six in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks) (2.9.0.post0)\n",
      "Requirement already satisfied: certifi>=2022.12.07 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks) (2025.1.31)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from pandas[mysql]<2.3.0->hopsworks) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from pandas[mysql]<2.3.0->hopsworks) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from requests->hopsworks) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from requests->hopsworks) (3.10)\n",
      "Collecting botocore<1.41.0,>=1.40.40\n",
      "  Using cached botocore-1.40.40-py3-none-any.whl (14.0 MB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from boto3->hopsworks) (1.0.1)\n",
      "Collecting s3transfer<0.15.0,>=0.14.0\n",
      "  Using cached s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
      "Requirement already satisfied: orderedmultidict>=1.0.1 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from furl->hopsworks) (1.0.1)\n",
      "Requirement already satisfied: javaobj-py3 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from pyjks->hopsworks) (0.4.4)\n",
      "Requirement already satisfied: pyasn1>=0.3.5 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from pyjks->hopsworks) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from pyjks->hopsworks) (0.4.2)\n",
      "Requirement already satisfied: pycryptodomex in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from pyjks->hopsworks) (3.23.0)\n",
      "Requirement already satisfied: twofish in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from pyjks->hopsworks) (0.3.0)\n",
      "Requirement already satisfied: cryptography in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from PyMySQL[rsa]->hopsworks) (45.0.6)\n",
      "Requirement already satisfied: colorama in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from tqdm->hopsworks) (0.4.6)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from sqlalchemy<=2.0.29,>=1.3->hopsworks_aiomysql[sa]==0.2.1->hopsworks) (3.2.4)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from cryptography->PyMySQL[rsa]->hopsworks) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\workspace\\mlops_fleca_project\\.venv\\lib\\site-packages (from cffi>=1.14->cryptography->PyMySQL[rsa]->hopsworks) (2.22)\n",
      "Installing collected packages: opensearch-py, furl, botocore, s3transfer, boto3, hopsworks\n",
      "Successfully installed boto3-1.40.40 botocore-1.40.40 furl-2.1.4 hopsworks-4.4.2 opensearch-py-2.4.2 s3transfer-0.14.0\n"
     ]
    }
   ],
   "source": [
    "# Instalar hopsworks en el entorno del notebook gestionado por Poetry\n",
    "!poetry run pip install hopsworks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
