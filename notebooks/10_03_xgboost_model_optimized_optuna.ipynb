{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c651d49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Añade src al path para importar los módulos\n",
    "sys.path.append(str(Path().resolve().parent / 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b64e220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparación de datos con el pipeline completo desde model.py\n",
    "from src.model import preparar_datos_pipeline, train_evaluate_xgboost\n",
    "\n",
    "# Obtén los datos listos para modelar\n",
    "X_train, y_train, X_test, y_test = preparar_datos_pipeline(\n",
    "    parquet_file='ts_df_bolleria_20250803.parquet',\n",
    "    split_date='2025-03-03',\n",
    "    target='base_imponible'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c23d4ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-05 20:47:27,089] A new study created in memory with name: no-name-a50eeedf-47d6-4ae7-b2a2-681c5e8528b8\n",
      "[I 2025-08-05 20:47:27,412] Trial 0 finished with value: 84.61112955729168 and parameters: {'booster': 'dart', 'lambda': 0.00011385484587041947, 'alpha': 6.115999639819006e-06, 'subsample': 0.6296700514721693, 'colsample_bytree': 0.5333837151542318, 'n_estimators': 263, 'max_depth': 3, 'learning_rate': 0.2577472928263704}. Best is trial 0 with value: 84.61112955729168.\n",
      "[I 2025-08-05 20:47:27,412] Trial 0 finished with value: 84.61112955729168 and parameters: {'booster': 'dart', 'lambda': 0.00011385484587041947, 'alpha': 6.115999639819006e-06, 'subsample': 0.6296700514721693, 'colsample_bytree': 0.5333837151542318, 'n_estimators': 263, 'max_depth': 3, 'learning_rate': 0.2577472928263704}. Best is trial 0 with value: 84.61112955729168.\n",
      "c:\\Workspace\\mlops_fleca_project\\.venv\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:47:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"subsample\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-05 20:47:27,434] Trial 1 finished with value: 97.60888020833333 and parameters: {'booster': 'gblinear', 'lambda': 5.847786909486395e-06, 'alpha': 1.651656568138082e-07, 'subsample': 0.2608860808291292, 'colsample_bytree': 0.5679022336566164, 'n_estimators': 292, 'max_depth': 9, 'learning_rate': 0.06912897025240099}. Best is trial 0 with value: 84.61112955729168.\n",
      "[I 2025-08-05 20:47:27,447] Trial 2 finished with value: 65.93610860188802 and parameters: {'booster': 'gbtree', 'lambda': 0.00312058933444922, 'alpha': 0.4392924728319066, 'subsample': 0.7622028333214863, 'colsample_bytree': 0.6747528270199387, 'n_estimators': 64, 'max_depth': 2, 'learning_rate': 0.12916270519772616}. Best is trial 2 with value: 65.93610860188802.\n",
      "c:\\Workspace\\mlops_fleca_project\\.venv\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:47:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"subsample\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-05 20:47:27,434] Trial 1 finished with value: 97.60888020833333 and parameters: {'booster': 'gblinear', 'lambda': 5.847786909486395e-06, 'alpha': 1.651656568138082e-07, 'subsample': 0.2608860808291292, 'colsample_bytree': 0.5679022336566164, 'n_estimators': 292, 'max_depth': 9, 'learning_rate': 0.06912897025240099}. Best is trial 0 with value: 84.61112955729168.\n",
      "[I 2025-08-05 20:47:27,447] Trial 2 finished with value: 65.93610860188802 and parameters: {'booster': 'gbtree', 'lambda': 0.00312058933444922, 'alpha': 0.4392924728319066, 'subsample': 0.7622028333214863, 'colsample_bytree': 0.6747528270199387, 'n_estimators': 64, 'max_depth': 2, 'learning_rate': 0.12916270519772616}. Best is trial 2 with value: 65.93610860188802.\n",
      "[I 2025-08-05 20:47:27,482] Trial 3 finished with value: 70.27899271647136 and parameters: {'booster': 'gbtree', 'lambda': 5.0542448372889e-06, 'alpha': 7.952843766388357e-08, 'subsample': 0.691428575758154, 'colsample_bytree': 0.3484232718607275, 'n_estimators': 213, 'max_depth': 3, 'learning_rate': 0.177037036135061}. Best is trial 2 with value: 65.93610860188802.\n",
      "[I 2025-08-05 20:47:27,482] Trial 3 finished with value: 70.27899271647136 and parameters: {'booster': 'gbtree', 'lambda': 5.0542448372889e-06, 'alpha': 7.952843766388357e-08, 'subsample': 0.691428575758154, 'colsample_bytree': 0.3484232718607275, 'n_estimators': 213, 'max_depth': 3, 'learning_rate': 0.177037036135061}. Best is trial 2 with value: 65.93610860188802.\n",
      "[I 2025-08-05 20:47:27,527] Trial 4 finished with value: 64.22589843749999 and parameters: {'booster': 'gbtree', 'lambda': 9.544677424129506e-06, 'alpha': 0.10305413969465506, 'subsample': 0.5205062182318805, 'colsample_bytree': 0.7236963741373585, 'n_estimators': 137, 'max_depth': 10, 'learning_rate': 0.130199867021632}. Best is trial 4 with value: 64.22589843749999.\n",
      "[I 2025-08-05 20:47:27,527] Trial 4 finished with value: 64.22589843749999 and parameters: {'booster': 'gbtree', 'lambda': 9.544677424129506e-06, 'alpha': 0.10305413969465506, 'subsample': 0.5205062182318805, 'colsample_bytree': 0.7236963741373585, 'n_estimators': 137, 'max_depth': 10, 'learning_rate': 0.130199867021632}. Best is trial 4 with value: 64.22589843749999.\n",
      "[I 2025-08-05 20:47:27,564] Trial 5 finished with value: 80.99329935709635 and parameters: {'booster': 'gbtree', 'lambda': 1.0152979011169633e-06, 'alpha': 2.0301566690955804e-07, 'subsample': 0.8404098517114311, 'colsample_bytree': 0.9677144740000045, 'n_estimators': 155, 'max_depth': 8, 'learning_rate': 0.17972075806901666}. Best is trial 4 with value: 64.22589843749999.\n",
      "[I 2025-08-05 20:47:27,564] Trial 5 finished with value: 80.99329935709635 and parameters: {'booster': 'gbtree', 'lambda': 1.0152979011169633e-06, 'alpha': 2.0301566690955804e-07, 'subsample': 0.8404098517114311, 'colsample_bytree': 0.9677144740000045, 'n_estimators': 155, 'max_depth': 8, 'learning_rate': 0.17972075806901666}. Best is trial 4 with value: 64.22589843749999.\n",
      "[I 2025-08-05 20:47:27,582] Trial 6 finished with value: 67.3753839111328 and parameters: {'booster': 'gbtree', 'lambda': 0.04712676120439737, 'alpha': 2.8367949398713497e-07, 'subsample': 0.5948230632738275, 'colsample_bytree': 0.20366034123038101, 'n_estimators': 123, 'max_depth': 2, 'learning_rate': 0.15197832939219047}. Best is trial 4 with value: 64.22589843749999.\n",
      "c:\\Workspace\\mlops_fleca_project\\.venv\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:47:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"subsample\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-05 20:47:27,601] Trial 7 finished with value: 69.14667521158853 and parameters: {'booster': 'gblinear', 'lambda': 2.424861519071678e-08, 'alpha': 0.00032357003549030137, 'subsample': 0.8626737970413438, 'colsample_bytree': 0.6480467125610276, 'n_estimators': 224, 'max_depth': 5, 'learning_rate': 0.20904330599611765}. Best is trial 4 with value: 64.22589843749999.\n",
      "[I 2025-08-05 20:47:27,582] Trial 6 finished with value: 67.3753839111328 and parameters: {'booster': 'gbtree', 'lambda': 0.04712676120439737, 'alpha': 2.8367949398713497e-07, 'subsample': 0.5948230632738275, 'colsample_bytree': 0.20366034123038101, 'n_estimators': 123, 'max_depth': 2, 'learning_rate': 0.15197832939219047}. Best is trial 4 with value: 64.22589843749999.\n",
      "c:\\Workspace\\mlops_fleca_project\\.venv\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:47:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"subsample\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-05 20:47:27,601] Trial 7 finished with value: 69.14667521158853 and parameters: {'booster': 'gblinear', 'lambda': 2.424861519071678e-08, 'alpha': 0.00032357003549030137, 'subsample': 0.8626737970413438, 'colsample_bytree': 0.6480467125610276, 'n_estimators': 224, 'max_depth': 5, 'learning_rate': 0.20904330599611765}. Best is trial 4 with value: 64.22589843749999.\n",
      "[I 2025-08-05 20:47:27,665] Trial 8 finished with value: 116.4260080973307 and parameters: {'booster': 'dart', 'lambda': 3.482276363624769e-08, 'alpha': 2.232350314999639e-07, 'subsample': 0.7225896001952794, 'colsample_bytree': 0.6909123679221543, 'n_estimators': 87, 'max_depth': 9, 'learning_rate': 0.016830975811459634}. Best is trial 4 with value: 64.22589843749999.\n",
      "[I 2025-08-05 20:47:27,665] Trial 8 finished with value: 116.4260080973307 and parameters: {'booster': 'dart', 'lambda': 3.482276363624769e-08, 'alpha': 2.232350314999639e-07, 'subsample': 0.7225896001952794, 'colsample_bytree': 0.6909123679221543, 'n_estimators': 87, 'max_depth': 9, 'learning_rate': 0.016830975811459634}. Best is trial 4 with value: 64.22589843749999.\n",
      "[I 2025-08-05 20:47:27,729] Trial 9 finished with value: 64.54842854817709 and parameters: {'booster': 'gbtree', 'lambda': 9.900732016516605e-07, 'alpha': 5.6870601914847354e-05, 'subsample': 0.3182806592564523, 'colsample_bytree': 0.6767398928163224, 'n_estimators': 297, 'max_depth': 6, 'learning_rate': 0.12984336556515003}. Best is trial 4 with value: 64.22589843749999.\n",
      "[I 2025-08-05 20:47:27,729] Trial 9 finished with value: 64.54842854817709 and parameters: {'booster': 'gbtree', 'lambda': 9.900732016516605e-07, 'alpha': 5.6870601914847354e-05, 'subsample': 0.3182806592564523, 'colsample_bytree': 0.6767398928163224, 'n_estimators': 297, 'max_depth': 6, 'learning_rate': 0.12984336556515003}. Best is trial 4 with value: 64.22589843749999.\n",
      "c:\\Workspace\\mlops_fleca_project\\.venv\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:47:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"subsample\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-05 20:47:27,753] Trial 10 finished with value: 67.73914347330728 and parameters: {'booster': 'gblinear', 'lambda': 0.0005117651786256379, 'alpha': 0.9329486331228936, 'subsample': 0.449992403643807, 'colsample_bytree': 0.9066212750775163, 'n_estimators': 158, 'max_depth': 10, 'learning_rate': 0.2794733635855193}. Best is trial 4 with value: 64.22589843749999.\n",
      "c:\\Workspace\\mlops_fleca_project\\.venv\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:47:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"subsample\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-05 20:47:27,753] Trial 10 finished with value: 67.73914347330728 and parameters: {'booster': 'gblinear', 'lambda': 0.0005117651786256379, 'alpha': 0.9329486331228936, 'subsample': 0.449992403643807, 'colsample_bytree': 0.9066212750775163, 'n_estimators': 158, 'max_depth': 10, 'learning_rate': 0.2794733635855193}. Best is trial 4 with value: 64.22589843749999.\n",
      "[I 2025-08-05 20:47:27,807] Trial 11 finished with value: 65.9205098470052 and parameters: {'booster': 'gbtree', 'lambda': 6.847209336101542e-07, 'alpha': 0.0016620430961299378, 'subsample': 0.32642254076800525, 'colsample_bytree': 0.8106338210432852, 'n_estimators': 204, 'max_depth': 6, 'learning_rate': 0.09993796719738662}. Best is trial 4 with value: 64.22589843749999.\n",
      "[I 2025-08-05 20:47:27,807] Trial 11 finished with value: 65.9205098470052 and parameters: {'booster': 'gbtree', 'lambda': 6.847209336101542e-07, 'alpha': 0.0016620430961299378, 'subsample': 0.32642254076800525, 'colsample_bytree': 0.8106338210432852, 'n_estimators': 204, 'max_depth': 6, 'learning_rate': 0.09993796719738662}. Best is trial 4 with value: 64.22589843749999.\n",
      "[I 2025-08-05 20:47:27,852] Trial 12 finished with value: 66.54901346842448 and parameters: {'booster': 'gbtree', 'lambda': 9.930194172813057e-06, 'alpha': 0.00899695808522892, 'subsample': 0.4478033759338359, 'colsample_bytree': 0.8126985023494531, 'n_estimators': 127, 'max_depth': 7, 'learning_rate': 0.07828678446193221}. Best is trial 4 with value: 64.22589843749999.\n",
      "[I 2025-08-05 20:47:27,852] Trial 12 finished with value: 66.54901346842448 and parameters: {'booster': 'gbtree', 'lambda': 9.930194172813057e-06, 'alpha': 0.00899695808522892, 'subsample': 0.4478033759338359, 'colsample_bytree': 0.8126985023494531, 'n_estimators': 127, 'max_depth': 7, 'learning_rate': 0.07828678446193221}. Best is trial 4 with value: 64.22589843749999.\n",
      "[I 2025-08-05 20:47:27,912] Trial 13 finished with value: 91.93200948079426 and parameters: {'booster': 'gbtree', 'lambda': 0.7939030150389842, 'alpha': 1.2438339564658632e-05, 'subsample': 0.46821270411148735, 'colsample_bytree': 0.4774194678871026, 'n_estimators': 261, 'max_depth': 5, 'learning_rate': 0.22499808632198615}. Best is trial 4 with value: 64.22589843749999.\n",
      "[I 2025-08-05 20:47:27,912] Trial 13 finished with value: 91.93200948079426 and parameters: {'booster': 'gbtree', 'lambda': 0.7939030150389842, 'alpha': 1.2438339564658632e-05, 'subsample': 0.46821270411148735, 'colsample_bytree': 0.4774194678871026, 'n_estimators': 261, 'max_depth': 5, 'learning_rate': 0.22499808632198615}. Best is trial 4 with value: 64.22589843749999.\n",
      "[I 2025-08-05 20:47:28,004] Trial 14 finished with value: 68.43909912109375 and parameters: {'booster': 'dart', 'lambda': 2.643197199918151e-07, 'alpha': 0.043263077353626445, 'subsample': 0.3539552039854233, 'colsample_bytree': 0.8098523432719889, 'n_estimators': 111, 'max_depth': 7, 'learning_rate': 0.11983568889832247}. Best is trial 4 with value: 64.22589843749999.\n",
      "[I 2025-08-05 20:47:28,004] Trial 14 finished with value: 68.43909912109375 and parameters: {'booster': 'dart', 'lambda': 2.643197199918151e-07, 'alpha': 0.043263077353626445, 'subsample': 0.3539552039854233, 'colsample_bytree': 0.8098523432719889, 'n_estimators': 111, 'max_depth': 7, 'learning_rate': 0.11983568889832247}. Best is trial 4 with value: 64.22589843749999.\n",
      "[I 2025-08-05 20:47:28,047] Trial 15 finished with value: 81.50060119628907 and parameters: {'booster': 'gbtree', 'lambda': 4.4397514004597954e-05, 'alpha': 5.92295171958797e-05, 'subsample': 0.5449333820349817, 'colsample_bytree': 0.4400560511070716, 'n_estimators': 171, 'max_depth': 5, 'learning_rate': 0.0388763131080692}. Best is trial 4 with value: 64.22589843749999.\n",
      "[I 2025-08-05 20:47:28,047] Trial 15 finished with value: 81.50060119628907 and parameters: {'booster': 'gbtree', 'lambda': 4.4397514004597954e-05, 'alpha': 5.92295171958797e-05, 'subsample': 0.5449333820349817, 'colsample_bytree': 0.4400560511070716, 'n_estimators': 171, 'max_depth': 5, 'learning_rate': 0.0388763131080692}. Best is trial 4 with value: 64.22589843749999.\n",
      "[I 2025-08-05 20:47:28,111] Trial 16 finished with value: 82.44900268554686 and parameters: {'booster': 'gbtree', 'lambda': 0.0011035755664711867, 'alpha': 0.0017131404298675792, 'subsample': 0.9752024668464478, 'colsample_bytree': 0.7478828968785296, 'n_estimators': 193, 'max_depth': 10, 'learning_rate': 0.14880367211819479}. Best is trial 4 with value: 64.22589843749999.\n",
      "[I 2025-08-05 20:47:28,111] Trial 16 finished with value: 82.44900268554686 and parameters: {'booster': 'gbtree', 'lambda': 0.0011035755664711867, 'alpha': 0.0017131404298675792, 'subsample': 0.9752024668464478, 'colsample_bytree': 0.7478828968785296, 'n_estimators': 193, 'max_depth': 10, 'learning_rate': 0.14880367211819479}. Best is trial 4 with value: 64.22589843749999.\n",
      "[I 2025-08-05 20:47:28,139] Trial 17 finished with value: 58.911400349934894 and parameters: {'booster': 'gbtree', 'lambda': 1.5217227415395248e-07, 'alpha': 3.805804544409099e-06, 'subsample': 0.2068525521593345, 'colsample_bytree': 0.8857508899692184, 'n_estimators': 51, 'max_depth': 7, 'learning_rate': 0.08962161965694515}. Best is trial 17 with value: 58.911400349934894.\n",
      "[I 2025-08-05 20:47:28,139] Trial 17 finished with value: 58.911400349934894 and parameters: {'booster': 'gbtree', 'lambda': 1.5217227415395248e-07, 'alpha': 3.805804544409099e-06, 'subsample': 0.2068525521593345, 'colsample_bytree': 0.8857508899692184, 'n_estimators': 51, 'max_depth': 7, 'learning_rate': 0.08962161965694515}. Best is trial 17 with value: 58.911400349934894.\n",
      "[I 2025-08-05 20:47:28,180] Trial 18 finished with value: 67.75514017740885 and parameters: {'booster': 'dart', 'lambda': 8.638730270669354e-08, 'alpha': 1.3354183863560678e-06, 'subsample': 0.27895614391747364, 'colsample_bytree': 0.9115600747392343, 'n_estimators': 54, 'max_depth': 8, 'learning_rate': 0.06194368459245028}. Best is trial 17 with value: 58.911400349934894.\n",
      "c:\\Workspace\\mlops_fleca_project\\.venv\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:47:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"subsample\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-05 20:47:28,202] Trial 19 finished with value: 132.2791583251953 and parameters: {'booster': 'gblinear', 'lambda': 1.0682607876487134e-08, 'alpha': 1.3170299632189073e-08, 'subsample': 0.3923605950877366, 'colsample_bytree': 0.9214261947518879, 'n_estimators': 89, 'max_depth': 9, 'learning_rate': 0.09330068088362489}. Best is trial 17 with value: 58.911400349934894.\n",
      "[I 2025-08-05 20:47:28,180] Trial 18 finished with value: 67.75514017740885 and parameters: {'booster': 'dart', 'lambda': 8.638730270669354e-08, 'alpha': 1.3354183863560678e-06, 'subsample': 0.27895614391747364, 'colsample_bytree': 0.9115600747392343, 'n_estimators': 54, 'max_depth': 8, 'learning_rate': 0.06194368459245028}. Best is trial 17 with value: 58.911400349934894.\n",
      "c:\\Workspace\\mlops_fleca_project\\.venv\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:47:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"subsample\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-05 20:47:28,202] Trial 19 finished with value: 132.2791583251953 and parameters: {'booster': 'gblinear', 'lambda': 1.0682607876487134e-08, 'alpha': 1.3170299632189073e-08, 'subsample': 0.3923605950877366, 'colsample_bytree': 0.9214261947518879, 'n_estimators': 89, 'max_depth': 9, 'learning_rate': 0.09330068088362489}. Best is trial 17 with value: 58.911400349934894.\n",
      "[I 2025-08-05 20:47:28,237] Trial 20 finished with value: 59.64859232584635 and parameters: {'booster': 'gbtree', 'lambda': 4.260596072729068e-05, 'alpha': 0.04280620375846331, 'subsample': 0.2258132892358758, 'colsample_bytree': 0.8438070894124535, 'n_estimators': 87, 'max_depth': 8, 'learning_rate': 0.04762814115407952}. Best is trial 17 with value: 58.911400349934894.\n",
      "[I 2025-08-05 20:47:28,237] Trial 20 finished with value: 59.64859232584635 and parameters: {'booster': 'gbtree', 'lambda': 4.260596072729068e-05, 'alpha': 0.04280620375846331, 'subsample': 0.2258132892358758, 'colsample_bytree': 0.8438070894124535, 'n_estimators': 87, 'max_depth': 8, 'learning_rate': 0.04762814115407952}. Best is trial 17 with value: 58.911400349934894.\n",
      "[I 2025-08-05 20:47:28,269] Trial 21 finished with value: 61.91517903645832 and parameters: {'booster': 'gbtree', 'lambda': 6.215058919680447e-05, 'alpha': 0.08923362288261368, 'subsample': 0.20020326897647378, 'colsample_bytree': 0.7880729736167216, 'n_estimators': 84, 'max_depth': 8, 'learning_rate': 0.03994738040859542}. Best is trial 17 with value: 58.911400349934894.\n",
      "[I 2025-08-05 20:47:28,269] Trial 21 finished with value: 61.91517903645832 and parameters: {'booster': 'gbtree', 'lambda': 6.215058919680447e-05, 'alpha': 0.08923362288261368, 'subsample': 0.20020326897647378, 'colsample_bytree': 0.7880729736167216, 'n_estimators': 84, 'max_depth': 8, 'learning_rate': 0.03994738040859542}. Best is trial 17 with value: 58.911400349934894.\n",
      "[I 2025-08-05 20:47:28,300] Trial 22 finished with value: 158.31182739257812 and parameters: {'booster': 'gbtree', 'lambda': 0.016824598757581907, 'alpha': 0.021143684473227196, 'subsample': 0.2218985966623015, 'colsample_bytree': 0.9860815160614083, 'n_estimators': 83, 'max_depth': 7, 'learning_rate': 0.011220804195165263}. Best is trial 17 with value: 58.911400349934894.\n",
      "[I 2025-08-05 20:47:28,300] Trial 22 finished with value: 158.31182739257812 and parameters: {'booster': 'gbtree', 'lambda': 0.016824598757581907, 'alpha': 0.021143684473227196, 'subsample': 0.2218985966623015, 'colsample_bytree': 0.9860815160614083, 'n_estimators': 83, 'max_depth': 7, 'learning_rate': 0.011220804195165263}. Best is trial 17 with value: 58.911400349934894.\n",
      "[I 2025-08-05 20:47:28,325] Trial 23 finished with value: 79.56682189941405 and parameters: {'booster': 'gbtree', 'lambda': 9.689714986213635e-05, 'alpha': 0.0021710479323171087, 'subsample': 0.20504987893347273, 'colsample_bytree': 0.846228971735693, 'n_estimators': 51, 'max_depth': 8, 'learning_rate': 0.039640064824105196}. Best is trial 17 with value: 58.911400349934894.\n",
      "[I 2025-08-05 20:47:28,325] Trial 23 finished with value: 79.56682189941405 and parameters: {'booster': 'gbtree', 'lambda': 9.689714986213635e-05, 'alpha': 0.0021710479323171087, 'subsample': 0.20504987893347273, 'colsample_bytree': 0.846228971735693, 'n_estimators': 51, 'max_depth': 8, 'learning_rate': 0.039640064824105196}. Best is trial 17 with value: 58.911400349934894.\n",
      "[I 2025-08-05 20:47:28,361] Trial 24 finished with value: 62.91514302571614 and parameters: {'booster': 'gbtree', 'lambda': 3.2834691542030384e-05, 'alpha': 0.17872868008542828, 'subsample': 0.2577087745012158, 'colsample_bytree': 0.7690950797013112, 'n_estimators': 101, 'max_depth': 7, 'learning_rate': 0.042995355880658705}. Best is trial 17 with value: 58.911400349934894.\n",
      "[I 2025-08-05 20:47:28,361] Trial 24 finished with value: 62.91514302571614 and parameters: {'booster': 'gbtree', 'lambda': 3.2834691542030384e-05, 'alpha': 0.17872868008542828, 'subsample': 0.2577087745012158, 'colsample_bytree': 0.7690950797013112, 'n_estimators': 101, 'max_depth': 7, 'learning_rate': 0.042995355880658705}. Best is trial 17 with value: 58.911400349934894.\n",
      "[I 2025-08-05 20:47:28,396] Trial 25 finished with value: 71.73380249023437 and parameters: {'booster': 'gbtree', 'lambda': 0.00039838880781966295, 'alpha': 0.0002935554377702, 'subsample': 0.3907032462266894, 'colsample_bytree': 0.8563194974455366, 'n_estimators': 71, 'max_depth': 8, 'learning_rate': 0.05350248206422935}. Best is trial 17 with value: 58.911400349934894.\n",
      "[I 2025-08-05 20:47:28,396] Trial 25 finished with value: 71.73380249023437 and parameters: {'booster': 'gbtree', 'lambda': 0.00039838880781966295, 'alpha': 0.0002935554377702, 'subsample': 0.3907032462266894, 'colsample_bytree': 0.8563194974455366, 'n_estimators': 71, 'max_depth': 8, 'learning_rate': 0.05350248206422935}. Best is trial 17 with value: 58.911400349934894.\n",
      "[I 2025-08-05 20:47:28,436] Trial 26 finished with value: 71.38709818522135 and parameters: {'booster': 'gbtree', 'lambda': 0.005863161682999057, 'alpha': 0.008141061584201961, 'subsample': 0.2088710765406016, 'colsample_bytree': 0.6138352161595078, 'n_estimators': 106, 'max_depth': 6, 'learning_rate': 0.09426969232877798}. Best is trial 17 with value: 58.911400349934894.\n",
      "[I 2025-08-05 20:47:28,436] Trial 26 finished with value: 71.38709818522135 and parameters: {'booster': 'gbtree', 'lambda': 0.005863161682999057, 'alpha': 0.008141061584201961, 'subsample': 0.2088710765406016, 'colsample_bytree': 0.6138352161595078, 'n_estimators': 106, 'max_depth': 6, 'learning_rate': 0.09426969232877798}. Best is trial 17 with value: 58.911400349934894.\n",
      "[I 2025-08-05 20:47:28,468] Trial 27 finished with value: 80.85170756022134 and parameters: {'booster': 'gbtree', 'lambda': 1.8077704185452603e-07, 'alpha': 2.6878548171329334e-06, 'subsample': 0.3032526227620517, 'colsample_bytree': 0.8802974713282414, 'n_estimators': 71, 'max_depth': 9, 'learning_rate': 0.02880882658217463}. Best is trial 17 with value: 58.911400349934894.\n",
      "[I 2025-08-05 20:47:28,468] Trial 27 finished with value: 80.85170756022134 and parameters: {'booster': 'gbtree', 'lambda': 1.8077704185452603e-07, 'alpha': 2.6878548171329334e-06, 'subsample': 0.3032526227620517, 'colsample_bytree': 0.8802974713282414, 'n_estimators': 71, 'max_depth': 9, 'learning_rate': 0.02880882658217463}. Best is trial 17 with value: 58.911400349934894.\n",
      "[I 2025-08-05 20:47:28,597] Trial 28 finished with value: 66.99379191080729 and parameters: {'booster': 'dart', 'lambda': 0.13744937310072064, 'alpha': 0.09809385431359308, 'subsample': 0.3756050985467465, 'colsample_bytree': 0.7691027704437973, 'n_estimators': 140, 'max_depth': 8, 'learning_rate': 0.07609425175645702}. Best is trial 17 with value: 58.911400349934894.\n",
      "[I 2025-08-05 20:47:28,597] Trial 28 finished with value: 66.99379191080729 and parameters: {'booster': 'dart', 'lambda': 0.13744937310072064, 'alpha': 0.09809385431359308, 'subsample': 0.3756050985467465, 'colsample_bytree': 0.7691027704437973, 'n_estimators': 140, 'max_depth': 8, 'learning_rate': 0.07609425175645702}. Best is trial 17 with value: 58.911400349934894.\n",
      "c:\\Workspace\\mlops_fleca_project\\.venv\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:47:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"subsample\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-05 20:47:28,620] Trial 29 finished with value: 128.40846130371094 and parameters: {'booster': 'gblinear', 'lambda': 0.00022655949640583024, 'alpha': 1.3933161892953887e-05, 'subsample': 0.25588375719042444, 'colsample_bytree': 0.9964747341099951, 'n_estimators': 92, 'max_depth': 4, 'learning_rate': 0.10049711246708981}. Best is trial 17 with value: 58.911400349934894.\n",
      "c:\\Workspace\\mlops_fleca_project\\.venv\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:47:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"subsample\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-08-05 20:47:28,620] Trial 29 finished with value: 128.40846130371094 and parameters: {'booster': 'gblinear', 'lambda': 0.00022655949640583024, 'alpha': 1.3933161892953887e-05, 'subsample': 0.25588375719042444, 'colsample_bytree': 0.9964747341099951, 'n_estimators': 92, 'max_depth': 4, 'learning_rate': 0.10049711246708981}. Best is trial 17 with value: 58.911400349934894.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros Optuna (validación interna): {'booster': 'gbtree', 'lambda': 1.5217227415395248e-07, 'alpha': 3.805804544409099e-06, 'subsample': 0.2068525521593345, 'colsample_bytree': 0.8857508899692184, 'n_estimators': 51, 'max_depth': 7, 'learning_rate': 0.08962161965694515}\n",
      "Mejor MAE Optuna (validación interna): 58.911400349934894\n"
     ]
    }
   ],
   "source": [
    "# Función objetivo Optuna con parámetros recomendados para XGBoost (regresión)\n",
    "# Como hay pocas filas incluimos set de validación fijo (20% del train) en lugar de incluir splits\n",
    "\n",
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Hiperparámetros a optimizar\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart']),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    # Split interno para validación (20% del train)\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
    "    result = train_evaluate_xgboost(X_tr, y_tr, X_val, y_val, params)\n",
    "    return result['mae']\n",
    "\n",
    "# Lanzar el estudio de optimización\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "print('Mejores hiperparámetros Optuna (validación interna):', study.best_params)\n",
    "print('Mejor MAE Optuna (validación interna):', study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "259c196b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Workspace\\\\mlops_fleca_project\\\\models\\\\xgboost_optimized.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "from paths import MODELS_DIR\n",
    "import os\n",
    "\n",
    "best_model = train_evaluate_xgboost(X_train, y_train, X_test, y_test, study.best_params)['model']\n",
    "joblib.dump(best_model, MODELS_DIR / 'xgboost_optimized.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-project-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
