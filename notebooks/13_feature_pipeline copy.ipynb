{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48543363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo 1: Usando load_raw_data con fechas string\n",
      "Llamando a load_raw_data con fechas: 2025-07-30 hasta 2025-08-04\n",
      "\n",
      "Ejemplo 2: Usando load_raw_data con objetos datetime\n",
      "Llamando a load_raw_data con fechas: 2025-07-30 00:00:00 hasta 2025-08-04 00:00:00\n",
      "\n",
      "Ejemplo 3: Usando load_raw_data con fechas en formato DD/MM/YYYY\n",
      "Llamando a load_raw_data con fechas: 2025-07-30 00:00:00 hasta 2025-08-04 00:00:00\n",
      "\n",
      "Ejemplo 4: Usando directamente descargar_datos_bigquery\n",
      "Llamando a descargar_datos_bigquery con fechas: 2025-07-30 hasta 2025-08-04\n",
      "\n",
      "Todas estas opciones ahora funcionan correctamente con la nueva versión de las funciones.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Añadir directorio raíz al path para importar correctamente los módulos\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Importar funciones de data_utils\n",
    "from src.data_utils import load_raw_data, descargar_datos_bigquery\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Ejemplo 1: Usar load_raw_data directamente con fechas en diferentes formatos\n",
    "# Esto demostrará la nueva capacidad de manejar fechas de cualquier formato\n",
    "\n",
    "# Con fechas como string en formato YYYY-MM-DD\n",
    "print(\"Ejemplo 1: Usando load_raw_data con fechas string\")\n",
    "fecha_inicio = '2025-07-30'\n",
    "fecha_fin = '2025-08-04'\n",
    "# Solo para demostración - usar parámetro descargar_bq=True para descargar realmente\n",
    "# df = load_raw_data(fecha_inicio, fecha_fin, descargar_bq=True)\n",
    "print(f\"Llamando a load_raw_data con fechas: {fecha_inicio} hasta {fecha_fin}\")\n",
    "\n",
    "# Ejemplo 2: Con fechas como objetos datetime\n",
    "print(\"\\nEjemplo 2: Usando load_raw_data con objetos datetime\")\n",
    "fecha_inicio = datetime(2025, 7, 30)\n",
    "fecha_fin = datetime(2025, 8, 4)\n",
    "# df = load_raw_data(fecha_inicio, fecha_fin, descargar_bq=True)\n",
    "print(f\"Llamando a load_raw_data con fechas: {fecha_inicio} hasta {fecha_fin}\")\n",
    "\n",
    "# Ejemplo 3: Con fechas en formato DD/MM/YYYY\n",
    "print(\"\\nEjemplo 3: Usando load_raw_data con fechas en formato DD/MM/YYYY\")\n",
    "fecha_inicio = '30/07/2025'\n",
    "fecha_fin = '04/08/2025'\n",
    "# Para este caso, necesitaríamos convertir previamente las fechas\n",
    "fecha_inicio_dt = pd.to_datetime(fecha_inicio, format='%d/%m/%Y')\n",
    "fecha_fin_dt = pd.to_datetime(fecha_fin, format='%d/%m/%Y')\n",
    "# df = load_raw_data(fecha_inicio_dt, fecha_fin_dt, descargar_bq=True)\n",
    "print(f\"Llamando a load_raw_data con fechas: {fecha_inicio_dt} hasta {fecha_fin_dt}\")\n",
    "\n",
    "# Ejemplo 4: Usar directamente descargar_datos_bigquery\n",
    "print(\"\\nEjemplo 4: Usando directamente descargar_datos_bigquery\")\n",
    "fecha_inicio = '2025-07-30'\n",
    "fecha_fin = '2025-08-04'\n",
    "# query = descargar_datos_bigquery(fecha_inicio, fecha_fin)\n",
    "# df = query.to_dataframe()\n",
    "print(f\"Llamando a descargar_datos_bigquery con fechas: {fecha_inicio} hasta {fecha_fin}\")\n",
    "\n",
    "print(\"\\nTodas estas opciones ahora funcionan correctamente con la nueva versión de las funciones.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45dc1248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando datos desde 2025-07-25 hasta 2025-08-10\n",
      "Descargando datos desde BigQuery porque descargar_bq=True o no existe el archivo 2025-07-25\n",
      "Iniciando conexión con BigQuery...\n",
      "Conexión establecida.\n",
      "Usando fechas en consulta SQL: fecha_inicio='2025-08-10' y fecha_fin='2025-08-17'\n",
      "Descargando datos de fleca-del-port.fleca_ventas_dia.t_facturas_dia_extendida_2023 ...\n",
      "Conexión establecida.\n",
      "Usando fechas en consulta SQL: fecha_inicio='2025-08-10' y fecha_fin='2025-08-17'\n",
      "Descargando datos de fleca-del-port.fleca_ventas_dia.t_facturas_dia_extendida_2023 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas descargadas de la segunda tabla: 2196\n",
      "Guardando archivo en C:\\Workspace\\mlops_fleca_project\\data\\raw\\raw_data_bq_forecasting_20250817.parquet ...\n",
      "Archivo guardado correctamente.\n",
      "Usando archivo recién generado: C:\\Workspace\\mlops_fleca_project\\data\\raw\\raw_data_bq_forecasting_20250817.parquet\n",
      "Cargando datos desde: C:\\Workspace\\mlops_fleca_project\\data\\raw\\raw_data_bq_forecasting_20250817.parquet\n",
      "Validando fechas entre 2025-08-10 y 2025-08-17 (8 días)\n",
      "Total de fechas faltantes: 3\n",
      "Fechas faltantes: ['2025-08-15T00:00:00.000000000' '2025-08-16T00:00:00.000000000'\n",
      " '2025-08-17T00:00:00.000000000']\n",
      "\n",
      "Información del DataFrame descargado:\n",
      "Forma del DataFrame: (2196, 11)\n",
      "Rango de fechas en los datos:\n",
      "  - Fecha mínima: 2025-08-10 00:00:00\n",
      "  - Fecha máxima: 2025-08-14 00:00:00\n",
      "\n",
      "Registros antes del 04/08/2025: 0\n"
     ]
    }
   ],
   "source": [
    "# Caso práctico completo: Descargar datos y procesarlos\n",
    "\n",
    "# Desactivado por defecto para evitar descargar datos durante demostraciones\n",
    "ejecutar_descarga = True\n",
    "\n",
    "if ejecutar_descarga:\n",
    "    # Definir fechas - incluyendo fechas antes del 04/08/2025 que antes fallaban\n",
    "    fecha_inicio = '2025-07-25'  # Antes del 04/08\n",
    "    fecha_fin = '2025-08-10'     # Después del 04/08\n",
    "    \n",
    "    print(f\"Descargando datos desde {fecha_inicio} hasta {fecha_fin}\")\n",
    "    \n",
    "    # Descargar datos brutos directamente desde BigQuery\n",
    "    df_raw = load_raw_data(fecha_inicio, fecha_fin, descargar_bq=True)\n",
    "    \n",
    "    # Mostrar la información de las fechas en el DataFrame resultante\n",
    "    print(\"\\nInformación del DataFrame descargado:\")\n",
    "    print(f\"Forma del DataFrame: {df_raw.shape}\")\n",
    "    print(f\"Rango de fechas en los datos:\")\n",
    "    print(f\"  - Fecha mínima: {df_raw['fecha'].min()}\")\n",
    "    print(f\"  - Fecha máxima: {df_raw['fecha'].max()}\")\n",
    "    \n",
    "    # Verificar específicamente las fechas antes del 04/08/2025\n",
    "    fechas_antes_04 = df_raw[df_raw['fecha'] < '2025-08-04']\n",
    "    print(f\"\\nRegistros antes del 04/08/2025: {len(fechas_antes_04)}\")\n",
    "    if not fechas_antes_04.empty:\n",
    "        print(\"Fechas únicas antes del 04/08/2025:\")\n",
    "        print(fechas_antes_04['fecha'].dt.date.unique())\n",
    "else:\n",
    "    print(\"Ejecución de descarga desactivada. Cambia 'ejecutar_descarga = True' para probar la descarga real.\")\n",
    "    print(\"NOTA: Este es solo un ejemplo. El código está listo para descargar correctamente datos de cualquier fecha.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9488974",
   "metadata": {},
   "source": [
    "## Nota sobre los nombres de columnas\n",
    "\n",
    "En los datos descargados de BigQuery, la columna de fecha se llama `fecha` (en minúscula), no `FECHA`. Es importante respetar este nombre al trabajar con los datos, ya que de lo contrario se generarán errores de `KeyError` como el que acabamos de resolver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad3e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de utilidad para inspeccionar las columnas del DataFrame\n",
    "def inspeccionar_df(df, mostrar_primeras_filas=True):\n",
    "    \"\"\"\n",
    "    Muestra información útil sobre un DataFrame:\n",
    "    - Nombres de columnas y tipos de datos\n",
    "    - Valores únicos para columnas categóricas\n",
    "    - Rango de valores para columnas numéricas/fecha\n",
    "    - Opcionalmente las primeras filas del DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"Dimensiones del DataFrame: {df.shape}\")\n",
    "    print(\"\\nColumnas y tipos de datos:\")\n",
    "    for col, dtype in df.dtypes.items():\n",
    "        print(f\"- {col}: {dtype}\")\n",
    "    \n",
    "    print(\"\\nInformación detallada por columna:\")\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object' or df[col].dtype == 'string' or df[col].nunique() < 20:\n",
    "            n_valores = df[col].nunique()\n",
    "            if n_valores < 10:  # Para columnas con pocos valores únicos\n",
    "                valores = sorted(df[col].unique())\n",
    "                print(f\"- {col}: {n_valores} valores únicos: {valores}\")\n",
    "            else:\n",
    "                print(f\"- {col}: {n_valores} valores únicos\")\n",
    "        elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "            print(f\"- {col}: Min={df[col].min()}, Max={df[col].max()}, Media={df[col].mean():.2f}\")\n",
    "        elif pd.api.types.is_datetime64_dtype(df[col]):\n",
    "            print(f\"- {col}: Desde {df[col].min()} hasta {df[col].max()}\")\n",
    "    \n",
    "    if mostrar_primeras_filas:\n",
    "        print(\"\\nPrimeras 5 filas del DataFrame:\")\n",
    "        display(df.head())\n",
    "\n",
    "# Ejecutar la función si hay datos descargados\n",
    "if ejecutar_descarga and 'df_raw' in locals():\n",
    "    inspeccionar_df(df_raw)\n",
    "else:\n",
    "    print(\"No hay datos cargados para inspeccionar. Ejecuta la celda anterior con ejecutar_descarga=True primero.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b69e35",
   "metadata": {},
   "source": [
    "## Resumen del problema resuelto\n",
    "\n",
    "El problema principal era que las fechas se estaban enviando a BigQuery con componentes de tiempo (`2025-08-17 00:00:00`), cuando BigQuery esperaba fechas en formato puro `YYYY-MM-DD` para campos de tipo `DATE`.\n",
    "\n",
    "### La solución implementada:\n",
    "\n",
    "1. **En `descargar_datos_bigquery()`**:\n",
    "   - Ahora normaliza automáticamente las fechas al formato `YYYY-MM-DD`\n",
    "   - Maneja múltiples formatos de entrada (string, datetime, pandas.Timestamp)\n",
    "   - Asegura que las fechas en la consulta SQL estén en el formato correcto\n",
    "\n",
    "2. **En `load_raw_data()`**:\n",
    "   - Pasa directamente los parámetros de fecha a `descargar_datos_bigquery()`\n",
    "   - No requiere formateo previo de las fechas\n",
    "\n",
    "### Solución de problemas comunes:\n",
    "\n",
    "- **Si sigues viendo datos faltantes para fechas específicas**: Verifica que existan datos para esas fechas en la fuente de BigQuery\n",
    "- **Si obtienes errores de formato de fecha**: Asegúrate de que estás usando la versión actualizada de `data_utils.py`\n",
    "- **Para casos especiales**: Puedes formatear manualmente las fechas antes de pasarlas usando `pd.to_datetime().strftime('%Y-%m-%d')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cd32205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "\n",
    "from datetime import datetime, timedelta, date\n",
    "from src.data_utils import load_raw_data, transformar_a_series_temporales, impute_missing_dates\n",
    "from src import config\n",
    "import hopsworks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35223e0c",
   "metadata": {},
   "source": [
    "# Feature Pipeline - Versión mejorada\n",
    "\n",
    "> **ACTUALIZACIÓN**: Se ha mejorado el manejo de fechas para BigQuery en `src/data_utils.py`. Ahora puedes simplemente llamar a `load_raw_data` con los parámetros de fecha y la opción `descargar_bq=True`, sin preocuparte por el formato de las fechas.\n",
    "\n",
    "## Mejoras implementadas:\n",
    "1. **Normalización automática de fechas**: Cualquier formato de fecha es convertido automáticamente al formato `YYYY-MM-DD` que espera BigQuery.\n",
    "2. **Encapsulación del manejo de fechas**: Toda la lógica compleja está ahora en `data_utils.py`, no en el notebook.\n",
    "3. **Paso directo de parámetros**: Se pasan directamente `fecha_inicio` y `fecha_fin` a `descargar_datos_bigquery()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9005c029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Código actual de la función descargar_datos_bigquery:\n",
      "def descargar_datos_bigquery():\n",
      "    \"\"\"\n",
      "    Descarga datos de BigQuery usando el formato de fecha correcto para los campos DATE.\n",
      "    BigQuery espera fechas en formato 'YYYY-MM-DD' estricto para campos DATE.\n",
      "    \"\"\"\n",
      "    from datetime import datetime\n",
      "    print(\"Iniciando conexión con BigQuery...\")\n",
      "    client = bigquery.Client()\n",
      "    print(\"Conexión establecida.\")\n",
      "\n",
      "    # Permitir pasar fecha_inicio y fecha_fin como argumentos\n",
      "    import inspect\n",
      "    frame = inspect.currentframe().f_back\n",
      "    fecha_inicio = frame.f_locals.get('fecha_inicio', '2023-01-02')  # Valor predeterminado seguro\n",
      "    fecha_fin = frame.f_locals.get('fecha_fin', None)\n",
      "\n",
      "    # CORRECCIÓN PRINCIPAL: Asegurar formato YYYY-MM-DD para BigQuery\n",
      "    # 1. Convertir cualquier objeto fecha a string en formato correcto\n",
      "    if fecha_inicio is not None:\n",
      "        if not isinstance(fecha_inicio, str):\n",
      "            try:\n",
      "                fecha_inicio = fecha_inicio.strftime('%Y-%m-%d') if hasattr(fecha_inicio, 'strftime') else str(fecha_inicio)\n",
      "            except:\n",
      "                fecha_inicio = '2023-01-02'\n",
      "        # 2. Si es string pero tiene más de 10 caracteres (tiene hora), truncarlo\n",
      "        if len(fecha_inicio) > 10:\n",
      "            fecha_inicio = fecha_inicio[:10]\n",
      "    \n",
      "    # Hacer lo mismo con fecha_fin\n",
      "    if fecha_fin is not None:\n",
      "        if not isinstance(fecha_fin, str):\n",
      "            try:\n",
      "                fecha_fin = fecha_fin.strftime('%Y-%m-%d') if hasattr(fecha_fin, 'strftime') else str(fecha_fin)\n",
      "            except:\n",
      "                fecha_fin = None\n",
      "        if fecha_fin and len(fecha_fin) > 10:\n",
      "            fecha_fin = fecha_fin[:10]\n",
      "    \n",
      "    print(f\"Usando fechas en consulta SQL: fecha_inicio='{fecha_inicio}' y fecha_fin='{fecha_fin}'\")\n",
      "    print(\"Descargando datos de fleca-del-port.fleca_ventas_dia.t_facturas_dia_extendida_2023 ...\")\n",
      "    \n",
      "    # Construir la consulta con fechas literales ya formateadas correctamente\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "        fecha,\n",
      "        n_factura,\n",
      "        zona_de_venta,\n",
      "        producto,\n",
      "        familia,\n",
      "        cantidad,\n",
      "        base_imponible,\n",
      "        tipo_IVA,\n",
      "        total\n",
      "    FROM `fleca-del-port.fleca_ventas_dia.t_facturas_dia_extendida_2023`\n",
      "    WHERE fecha >= '{fecha_inicio}'\"\"\"\n",
      "    \n",
      "    if fecha_fin:\n",
      "        query += f\" AND fecha <= '{fecha_fin}'\"\n",
      "    df = client.query(query).to_dataframe()\n",
      "    print(f\"Filas descargadas de la segunda tabla: {len(df)}\")\n",
      "\n",
      "    # Guardar el DataFrame en la carpeta RAW con fecha en el nombre\n",
      "    from pathlib import Path\n",
      "    fecha_actual = datetime.now().strftime(\"%Y%m%d\")\n",
      "    # Determinar la ruta absoluta a la carpeta data/raw\n",
      "    module_path = Path(__file__).resolve().parent  # src/\n",
      "    project_root = module_path.parent  # raíz del proyecto\n",
      "    raw_dir = project_root / 'data' / 'raw'\n",
      "    raw_dir.mkdir(parents=True, exist_ok=True)\n",
      "    output_path = raw_dir / f\"raw_data_bq_forecasting_{fecha_actual}.parquet\"\n",
      "    print(f\"Guardando archivo en {output_path} ...\")\n",
      "    df.to_parquet(str(output_path), index=False)\n",
      "    print(\"Archivo guardado correctamente.\")\n",
      "    return df\n",
      "\n",
      "\n",
      "Probando versión corregida...\n",
      "Iniciando conexión con BigQuery...\n",
      "Conexión establecida.\n",
      "Query SQL a ejecutar: \n",
      "    SELECT \n",
      "        fecha,\n",
      "        n_factura,\n",
      "        zona_de_venta,\n",
      "        producto,\n",
      "        familia,\n",
      "        cantidad,\n",
      "        base_imponible,\n",
      "        tipo_IVA,\n",
      "        total\n",
      "    FROM `fleca-del-port.fleca_ventas_dia.t_facturas_dia_extendida_2023`\n",
      "    WHERE fecha >= '2023-01-02'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas descargadas: 209295\n",
      "Archivo guardado en: raw_data_bq_forecasting_20250817.parquet\n",
      "¡ÉXITO! La consulta se ejecutó correctamente\n",
      "Datos descargados: (209295, 9)\n"
     ]
    }
   ],
   "source": [
    "# 2a. Diagnosticar y corregir el problema de fechas en BigQuery directamente\n",
    "\n",
    "# 1. Mostrar la función actual para ver cómo construye la query\n",
    "import inspect\n",
    "from src.data_utils import descargar_datos_bigquery\n",
    "print(\"Código actual de la función descargar_datos_bigquery:\")\n",
    "print(inspect.getsource(descargar_datos_bigquery))\n",
    "\n",
    "# 2. Crear una versión corregida que podamos usar directamente\n",
    "def descargar_datos_bigquery_corregido():\n",
    "    \"\"\"Versión corregida que garantiza el formato YYYY-MM-DD para BigQuery\"\"\"\n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    print(\"Iniciando conexión con BigQuery...\")\n",
    "    client = bigquery.Client()\n",
    "    print(\"Conexión establecida.\")\n",
    "    \n",
    "    # Obtener fechas del contexto superior\n",
    "    fecha_inicio = '2023-01-02'  # Fecha inicial fija en formato correcto\n",
    "    fecha_fin = None  # No limitamos la fecha final\n",
    "    \n",
    "    # Construir la consulta SQL con fechas en formato correcto para BigQuery DATE\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        fecha,\n",
    "        n_factura,\n",
    "        zona_de_venta,\n",
    "        producto,\n",
    "        familia,\n",
    "        cantidad,\n",
    "        base_imponible,\n",
    "        tipo_IVA,\n",
    "        total\n",
    "    FROM `fleca-del-port.fleca_ventas_dia.t_facturas_dia_extendida_2023`\n",
    "    WHERE fecha >= '{fecha_inicio}'\"\"\"\n",
    "    \n",
    "    print(f\"Query SQL a ejecutar: {query}\")\n",
    "    \n",
    "    # Ejecutar la consulta y convertir a DataFrame\n",
    "    df = client.query(query).to_dataframe()\n",
    "    print(f\"Filas descargadas: {len(df)}\")\n",
    "    \n",
    "    # Guardar el DataFrame en disco\n",
    "    fecha_actual = datetime.now().strftime(\"%Y%m%d\")\n",
    "    output_path = f\"raw_data_bq_forecasting_{fecha_actual}.parquet\"\n",
    "    df.to_parquet(output_path, index=False)\n",
    "    print(f\"Archivo guardado en: {output_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 3. Probar la versión corregida\n",
    "print(\"\\nProbando versión corregida...\")\n",
    "try:\n",
    "    df_corregido = descargar_datos_bigquery_corregido()\n",
    "    print(\"¡ÉXITO! La consulta se ejecutó correctamente\")\n",
    "    print(f\"Datos descargados: {df_corregido.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: {type(e).__name__}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d4cd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando fechas literales: FECHA_INICIO='2023-01-02', FECHA_FIN='2025-08-17'\n",
      "Ejecutando consulta SQL directamente...\n",
      "Query: \n",
      "SELECT \n",
      "    fecha,\n",
      "    n_factura,\n",
      "    zona_de_venta,\n",
      "    producto,\n",
      "    familia,\n",
      "    cantidad,\n",
      "    base_imponible,\n",
      "    tipo_IVA,\n",
      "    total\n",
      "FROM `fleca-del-port.fleca_ventas_dia.t_facturas_dia_extendida_2023`\n",
      "WHERE fecha >= '2023-01-02' AND fecha <= '2025-08-17'\n",
      "¡ÉXITO! La consulta se ejecutó correctamente\n",
      "Datos descargados: (209295, 9)\n",
      "Fechas descargadas:\n",
      "- Rango: 2024-01-02 a 2025-08-14\n",
      "- Días únicos: 589\n"
     ]
    }
   ],
   "source": [
    "# 2b. Solución simplificada: descargar datos directamente sin funciones intermedias\n",
    "\n",
    "# Importar solo lo necesario\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Fechas fijas en formato correcto YYYY-MM-DD para BigQuery\n",
    "FECHA_INICIO = '2023-01-02'\n",
    "FECHA_FIN = '2025-08-17'  # Opcional\n",
    "\n",
    "print(f\"Usando fechas literales: FECHA_INICIO='{FECHA_INICIO}', FECHA_FIN='{FECHA_FIN}'\")\n",
    "\n",
    "# Crear cliente de BigQuery\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Construir consulta SQL con fechas literales\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    fecha,\n",
    "    n_factura,\n",
    "    zona_de_venta,\n",
    "    producto,\n",
    "    familia,\n",
    "    cantidad,\n",
    "    base_imponible,\n",
    "    tipo_IVA,\n",
    "    total\n",
    "FROM `fleca-del-port.fleca_ventas_dia.t_facturas_dia_extendida_2023`\n",
    "WHERE fecha >= '{FECHA_INICIO}'\"\"\"\n",
    "\n",
    "if FECHA_FIN:\n",
    "    query += f\" AND fecha <= '{FECHA_FIN}'\"\n",
    "\n",
    "print(\"Ejecutando consulta SQL directamente...\")\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Ejecutar consulta\n",
    "try:\n",
    "    df_directo = client.query(query).to_dataframe()\n",
    "    print(\"¡ÉXITO! La consulta se ejecutó correctamente\")\n",
    "    print(f\"Datos descargados: {df_directo.shape}\")\n",
    "    \n",
    "    # Verificar fechas disponibles\n",
    "    print(\"Fechas descargadas:\")\n",
    "    print(f\"- Rango: {df_directo['fecha'].min()} a {df_directo['fecha'].max()}\")\n",
    "    print(f\"- Días únicos: {df_directo['fecha'].nunique()}\")\n",
    "    \n",
    "    # Guardar para usar en el resto del notebook\n",
    "    df_preview = df_directo\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: {type(e).__name__}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b10c1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando formato de fechas...\n",
      "Fechas disponibles: 589 días\n",
      "Primera fecha: 2024-01-02, Última fecha: 2025-08-14\n",
      "Fechas faltantes en el rango: 370 días\n",
      "Primeras 5 fechas faltantes: [datetime.date(2023, 1, 2), datetime.date(2023, 1, 3), datetime.date(2023, 1, 4), datetime.date(2023, 1, 5), datetime.date(2023, 1, 6)]\n",
      "\n",
      "Rango completo para análisis: 2023-01-02 a 2025-08-17\n",
      "Total de días disponibles: 589\n",
      "\n",
      "¡Datos cargados correctamente! Ahora puedes continuar con la celda 4 (análisis)\n",
      "(Puedes saltar la celda 3 que vuelve a descargar datos)\n"
     ]
    }
   ],
   "source": [
    "# 2c. Continuar con el flujo normal usando los datos descargados directamente\n",
    "\n",
    "# Asegurarnos que las fechas estén en el formato correcto para análisis\n",
    "print(\"Normalizando formato de fechas...\")\n",
    "df_preview['fecha'] = pd.to_datetime(df_preview['fecha']).dt.date\n",
    "\n",
    "# Verificar días únicos disponibles\n",
    "fechas_unicas = sorted(df_preview['fecha'].unique())\n",
    "print(f\"Fechas disponibles: {len(fechas_unicas)} días\")\n",
    "print(f\"Primera fecha: {fechas_unicas[0]}, Última fecha: {fechas_unicas[-1]}\")\n",
    "\n",
    "# Encontrar fechas faltantes en el rango\n",
    "fecha_inicio_dt = pd.to_datetime(FECHA_INICIO).date()\n",
    "fecha_fin_dt = pd.to_datetime(FECHA_FIN).date() if FECHA_FIN else fechas_unicas[-1]\n",
    "fechas_completas = pd.date_range(start=fecha_inicio_dt, end=fecha_fin_dt, freq='D').date\n",
    "fechas_faltantes = set(fechas_completas) - set(fechas_unicas)\n",
    "print(f\"Fechas faltantes en el rango: {len(fechas_faltantes)} días\")\n",
    "if fechas_faltantes and len(fechas_faltantes) < 10:\n",
    "    print(f\"Fechas faltantes: {sorted(fechas_faltantes)}\")\n",
    "elif fechas_faltantes:\n",
    "    print(f\"Primeras 5 fechas faltantes: {sorted(fechas_faltantes)[:5]}\")\n",
    "\n",
    "# Para seguir con el flujo normal, establecemos las variables que necesita el resto del notebook\n",
    "fecha_inicio = FECHA_INICIO\n",
    "fecha_fin = FECHA_FIN if FECHA_FIN else fecha_fin_dt.strftime('%Y-%m-%d')\n",
    "print(f\"\\nRango completo para análisis: {fecha_inicio} a {fecha_fin}\")\n",
    "print(f\"Total de días disponibles: {len(fechas_unicas)}\")\n",
    "\n",
    "# También guardamos las variables con el sufijo _bq para compatibilidad con el resto del notebook\n",
    "fecha_inicio_bq = fecha_inicio\n",
    "fecha_fin_bq = fecha_fin\n",
    "\n",
    "# Para la siguiente celda, ya tenemos df_preview cargado\n",
    "df = df_preview.copy()\n",
    "\n",
    "print(\"\\n¡Datos cargados correctamente! Ahora puedes continuar con la celda 4 (análisis)\")\n",
    "print(\"(Puedes saltar la celda 3 que vuelve a descargar datos)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a81d706b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando datos con fecha_inicio=2023-01-02 y fecha_fin=2025-08-14...\n",
      "Descargando datos desde BigQuery porque descargar_bq=True o no existe el archivo C:\\Workspace\\mlops_fleca_project\\data\\raw\\raw_data_bq_forecasting.parquet\n",
      "Iniciando conexión con BigQuery...\n",
      "Conexión establecida.\n",
      "Descargando datos de fleca-del-port.fleca_ventas_dia.t_facturas_dia_extendida_2023 ...\n",
      "Conexión establecida.\n",
      "Descargando datos de fleca-del-port.fleca_ventas_dia.t_facturas_dia_extendida_2023 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas descargadas de la segunda tabla: 5422\n",
      "Guardando archivo en C:\\Workspace\\mlops_fleca_project\\data\\raw\\raw_data_bq_forecasting_20250817.parquet ...\n",
      "Archivo guardado correctamente.\n",
      "Usando archivo recién generado: C:\\Workspace\\mlops_fleca_project\\data\\raw\\raw_data_bq_forecasting_20250817.parquet\n",
      "Cargando datos desde: C:\\Workspace\\mlops_fleca_project\\data\\raw\\raw_data_bq_forecasting_20250817.parquet\n",
      "Validando fechas entre 2023-01-02 y 2025-08-14 (956 días)\n",
      "Total de fechas faltantes: 945\n",
      "Fechas faltantes: ['2023-01-02T00:00:00.000000000' '2023-01-03T00:00:00.000000000'\n",
      " '2023-01-04T00:00:00.000000000' '2023-01-05T00:00:00.000000000'\n",
      " '2023-01-06T00:00:00.000000000' '2023-01-07T00:00:00.000000000'\n",
      " '2023-01-08T00:00:00.000000000' '2023-01-09T00:00:00.000000000'\n",
      " '2023-01-10T00:00:00.000000000' '2023-01-11T00:00:00.000000000'\n",
      " '2023-01-12T00:00:00.000000000' '2023-01-13T00:00:00.000000000'\n",
      " '2023-01-14T00:00:00.000000000' '2023-01-15T00:00:00.000000000'\n",
      " '2023-01-16T00:00:00.000000000' '2023-01-17T00:00:00.000000000'\n",
      " '2023-01-18T00:00:00.000000000' '2023-01-19T00:00:00.000000000'\n",
      " '2023-01-20T00:00:00.000000000' '2023-01-21T00:00:00.000000000'\n",
      " '2023-01-22T00:00:00.000000000' '2023-01-23T00:00:00.000000000'\n",
      " '2023-01-24T00:00:00.000000000' '2023-01-25T00:00:00.000000000'\n",
      " '2023-01-26T00:00:00.000000000' '2023-01-27T00:00:00.000000000'\n",
      " '2023-01-28T00:00:00.000000000' '2023-01-29T00:00:00.000000000'\n",
      " '2023-01-30T00:00:00.000000000' '2023-01-31T00:00:00.000000000'\n",
      " '2023-02-01T00:00:00.000000000' '2023-02-02T00:00:00.000000000'\n",
      " '2023-02-03T00:00:00.000000000' '2023-02-04T00:00:00.000000000'\n",
      " '2023-02-05T00:00:00.000000000' '2023-02-06T00:00:00.000000000'\n",
      " '2023-02-07T00:00:00.000000000' '2023-02-08T00:00:00.000000000'\n",
      " '2023-02-09T00:00:00.000000000' '2023-02-10T00:00:00.000000000'\n",
      " '2023-02-11T00:00:00.000000000' '2023-02-12T00:00:00.000000000'\n",
      " '2023-02-13T00:00:00.000000000' '2023-02-14T00:00:00.000000000'\n",
      " '2023-02-15T00:00:00.000000000' '2023-02-16T00:00:00.000000000'\n",
      " '2023-02-17T00:00:00.000000000' '2023-02-18T00:00:00.000000000'\n",
      " '2023-02-19T00:00:00.000000000' '2023-02-20T00:00:00.000000000'\n",
      " '2023-02-21T00:00:00.000000000' '2023-02-22T00:00:00.000000000'\n",
      " '2023-02-23T00:00:00.000000000' '2023-02-24T00:00:00.000000000'\n",
      " '2023-02-25T00:00:00.000000000' '2023-02-26T00:00:00.000000000'\n",
      " '2023-02-27T00:00:00.000000000' '2023-02-28T00:00:00.000000000'\n",
      " '2023-03-01T00:00:00.000000000' '2023-03-02T00:00:00.000000000'\n",
      " '2023-03-03T00:00:00.000000000' '2023-03-04T00:00:00.000000000'\n",
      " '2023-03-05T00:00:00.000000000' '2023-03-06T00:00:00.000000000'\n",
      " '2023-03-07T00:00:00.000000000' '2023-03-08T00:00:00.000000000'\n",
      " '2023-03-09T00:00:00.000000000' '2023-03-10T00:00:00.000000000'\n",
      " '2023-03-11T00:00:00.000000000' '2023-03-12T00:00:00.000000000'\n",
      " '2023-03-13T00:00:00.000000000' '2023-03-14T00:00:00.000000000'\n",
      " '2023-03-15T00:00:00.000000000' '2023-03-16T00:00:00.000000000'\n",
      " '2023-03-17T00:00:00.000000000' '2023-03-18T00:00:00.000000000'\n",
      " '2023-03-19T00:00:00.000000000' '2023-03-20T00:00:00.000000000'\n",
      " '2023-03-21T00:00:00.000000000' '2023-03-22T00:00:00.000000000'\n",
      " '2023-03-23T00:00:00.000000000' '2023-03-24T00:00:00.000000000'\n",
      " '2023-03-25T00:00:00.000000000' '2023-03-26T00:00:00.000000000'\n",
      " '2023-03-27T00:00:00.000000000' '2023-03-28T00:00:00.000000000'\n",
      " '2023-03-29T00:00:00.000000000' '2023-03-30T00:00:00.000000000'\n",
      " '2023-03-31T00:00:00.000000000' '2023-04-01T00:00:00.000000000'\n",
      " '2023-04-02T00:00:00.000000000' '2023-04-03T00:00:00.000000000'\n",
      " '2023-04-04T00:00:00.000000000' '2023-04-05T00:00:00.000000000'\n",
      " '2023-04-06T00:00:00.000000000' '2023-04-07T00:00:00.000000000'\n",
      " '2023-04-08T00:00:00.000000000' '2023-04-09T00:00:00.000000000'\n",
      " '2023-04-10T00:00:00.000000000' '2023-04-11T00:00:00.000000000'\n",
      " '2023-04-12T00:00:00.000000000' '2023-04-13T00:00:00.000000000'\n",
      " '2023-04-14T00:00:00.000000000' '2023-04-15T00:00:00.000000000'\n",
      " '2023-04-16T00:00:00.000000000' '2023-04-17T00:00:00.000000000'\n",
      " '2023-04-18T00:00:00.000000000' '2023-04-19T00:00:00.000000000'\n",
      " '2023-04-20T00:00:00.000000000' '2023-04-21T00:00:00.000000000'\n",
      " '2023-04-22T00:00:00.000000000' '2023-04-23T00:00:00.000000000'\n",
      " '2023-04-24T00:00:00.000000000' '2023-04-25T00:00:00.000000000'\n",
      " '2023-04-26T00:00:00.000000000' '2023-04-27T00:00:00.000000000'\n",
      " '2023-04-28T00:00:00.000000000' '2023-04-29T00:00:00.000000000'\n",
      " '2023-04-30T00:00:00.000000000' '2023-05-01T00:00:00.000000000'\n",
      " '2023-05-02T00:00:00.000000000' '2023-05-03T00:00:00.000000000'\n",
      " '2023-05-04T00:00:00.000000000' '2023-05-05T00:00:00.000000000'\n",
      " '2023-05-06T00:00:00.000000000' '2023-05-07T00:00:00.000000000'\n",
      " '2023-05-08T00:00:00.000000000' '2023-05-09T00:00:00.000000000'\n",
      " '2023-05-10T00:00:00.000000000' '2023-05-11T00:00:00.000000000'\n",
      " '2023-05-12T00:00:00.000000000' '2023-05-13T00:00:00.000000000'\n",
      " '2023-05-14T00:00:00.000000000' '2023-05-15T00:00:00.000000000'\n",
      " '2023-05-16T00:00:00.000000000' '2023-05-17T00:00:00.000000000'\n",
      " '2023-05-18T00:00:00.000000000' '2023-05-19T00:00:00.000000000'\n",
      " '2023-05-20T00:00:00.000000000' '2023-05-21T00:00:00.000000000'\n",
      " '2023-05-22T00:00:00.000000000' '2023-05-23T00:00:00.000000000'\n",
      " '2023-05-24T00:00:00.000000000' '2023-05-25T00:00:00.000000000'\n",
      " '2023-05-26T00:00:00.000000000' '2023-05-27T00:00:00.000000000'\n",
      " '2023-05-28T00:00:00.000000000' '2023-05-29T00:00:00.000000000'\n",
      " '2023-05-30T00:00:00.000000000' '2023-05-31T00:00:00.000000000'\n",
      " '2023-06-01T00:00:00.000000000' '2023-06-02T00:00:00.000000000'\n",
      " '2023-06-03T00:00:00.000000000' '2023-06-04T00:00:00.000000000'\n",
      " '2023-06-05T00:00:00.000000000' '2023-06-06T00:00:00.000000000'\n",
      " '2023-06-07T00:00:00.000000000' '2023-06-08T00:00:00.000000000'\n",
      " '2023-06-09T00:00:00.000000000' '2023-06-10T00:00:00.000000000'\n",
      " '2023-06-11T00:00:00.000000000' '2023-06-12T00:00:00.000000000'\n",
      " '2023-06-13T00:00:00.000000000' '2023-06-14T00:00:00.000000000'\n",
      " '2023-06-15T00:00:00.000000000' '2023-06-16T00:00:00.000000000'\n",
      " '2023-06-17T00:00:00.000000000' '2023-06-18T00:00:00.000000000'\n",
      " '2023-06-19T00:00:00.000000000' '2023-06-20T00:00:00.000000000'\n",
      " '2023-06-21T00:00:00.000000000' '2023-06-22T00:00:00.000000000'\n",
      " '2023-06-23T00:00:00.000000000' '2023-06-24T00:00:00.000000000'\n",
      " '2023-06-25T00:00:00.000000000' '2023-06-26T00:00:00.000000000'\n",
      " '2023-06-27T00:00:00.000000000' '2023-06-28T00:00:00.000000000'\n",
      " '2023-06-29T00:00:00.000000000' '2023-06-30T00:00:00.000000000'\n",
      " '2023-07-01T00:00:00.000000000' '2023-07-02T00:00:00.000000000'\n",
      " '2023-07-03T00:00:00.000000000' '2023-07-04T00:00:00.000000000'\n",
      " '2023-07-05T00:00:00.000000000' '2023-07-06T00:00:00.000000000'\n",
      " '2023-07-07T00:00:00.000000000' '2023-07-08T00:00:00.000000000'\n",
      " '2023-07-09T00:00:00.000000000' '2023-07-10T00:00:00.000000000'\n",
      " '2023-07-11T00:00:00.000000000' '2023-07-12T00:00:00.000000000'\n",
      " '2023-07-13T00:00:00.000000000' '2023-07-14T00:00:00.000000000'\n",
      " '2023-07-15T00:00:00.000000000' '2023-07-16T00:00:00.000000000'\n",
      " '2023-07-17T00:00:00.000000000' '2023-07-18T00:00:00.000000000'\n",
      " '2023-07-19T00:00:00.000000000' '2023-07-20T00:00:00.000000000'\n",
      " '2023-07-21T00:00:00.000000000' '2023-07-22T00:00:00.000000000'\n",
      " '2023-07-23T00:00:00.000000000' '2023-07-24T00:00:00.000000000'\n",
      " '2023-07-25T00:00:00.000000000' '2023-07-26T00:00:00.000000000'\n",
      " '2023-07-27T00:00:00.000000000' '2023-07-28T00:00:00.000000000'\n",
      " '2023-07-29T00:00:00.000000000' '2023-07-30T00:00:00.000000000'\n",
      " '2023-07-31T00:00:00.000000000' '2023-08-01T00:00:00.000000000'\n",
      " '2023-08-02T00:00:00.000000000' '2023-08-03T00:00:00.000000000'\n",
      " '2023-08-04T00:00:00.000000000' '2023-08-05T00:00:00.000000000'\n",
      " '2023-08-06T00:00:00.000000000' '2023-08-07T00:00:00.000000000'\n",
      " '2023-08-08T00:00:00.000000000' '2023-08-09T00:00:00.000000000'\n",
      " '2023-08-10T00:00:00.000000000' '2023-08-11T00:00:00.000000000'\n",
      " '2023-08-12T00:00:00.000000000' '2023-08-13T00:00:00.000000000'\n",
      " '2023-08-14T00:00:00.000000000' '2023-08-15T00:00:00.000000000'\n",
      " '2023-08-16T00:00:00.000000000' '2023-08-17T00:00:00.000000000'\n",
      " '2023-08-18T00:00:00.000000000' '2023-08-19T00:00:00.000000000'\n",
      " '2023-08-20T00:00:00.000000000' '2023-08-21T00:00:00.000000000'\n",
      " '2023-08-22T00:00:00.000000000' '2023-08-23T00:00:00.000000000'\n",
      " '2023-08-24T00:00:00.000000000' '2023-08-25T00:00:00.000000000'\n",
      " '2023-08-26T00:00:00.000000000' '2023-08-27T00:00:00.000000000'\n",
      " '2023-08-28T00:00:00.000000000' '2023-08-29T00:00:00.000000000'\n",
      " '2023-08-30T00:00:00.000000000' '2023-08-31T00:00:00.000000000'\n",
      " '2023-09-01T00:00:00.000000000' '2023-09-02T00:00:00.000000000'\n",
      " '2023-09-03T00:00:00.000000000' '2023-09-04T00:00:00.000000000'\n",
      " '2023-09-05T00:00:00.000000000' '2023-09-06T00:00:00.000000000'\n",
      " '2023-09-07T00:00:00.000000000' '2023-09-08T00:00:00.000000000'\n",
      " '2023-09-09T00:00:00.000000000' '2023-09-10T00:00:00.000000000'\n",
      " '2023-09-11T00:00:00.000000000' '2023-09-12T00:00:00.000000000'\n",
      " '2023-09-13T00:00:00.000000000' '2023-09-14T00:00:00.000000000'\n",
      " '2023-09-15T00:00:00.000000000' '2023-09-16T00:00:00.000000000'\n",
      " '2023-09-17T00:00:00.000000000' '2023-09-18T00:00:00.000000000'\n",
      " '2023-09-19T00:00:00.000000000' '2023-09-20T00:00:00.000000000'\n",
      " '2023-09-21T00:00:00.000000000' '2023-09-22T00:00:00.000000000'\n",
      " '2023-09-23T00:00:00.000000000' '2023-09-24T00:00:00.000000000'\n",
      " '2023-09-25T00:00:00.000000000' '2023-09-26T00:00:00.000000000'\n",
      " '2023-09-27T00:00:00.000000000' '2023-09-28T00:00:00.000000000'\n",
      " '2023-09-29T00:00:00.000000000' '2023-09-30T00:00:00.000000000'\n",
      " '2023-10-01T00:00:00.000000000' '2023-10-02T00:00:00.000000000'\n",
      " '2023-10-03T00:00:00.000000000' '2023-10-04T00:00:00.000000000'\n",
      " '2023-10-05T00:00:00.000000000' '2023-10-06T00:00:00.000000000'\n",
      " '2023-10-07T00:00:00.000000000' '2023-10-08T00:00:00.000000000'\n",
      " '2023-10-09T00:00:00.000000000' '2023-10-10T00:00:00.000000000'\n",
      " '2023-10-11T00:00:00.000000000' '2023-10-12T00:00:00.000000000'\n",
      " '2023-10-13T00:00:00.000000000' '2023-10-14T00:00:00.000000000'\n",
      " '2023-10-15T00:00:00.000000000' '2023-10-16T00:00:00.000000000'\n",
      " '2023-10-17T00:00:00.000000000' '2023-10-18T00:00:00.000000000'\n",
      " '2023-10-19T00:00:00.000000000' '2023-10-20T00:00:00.000000000'\n",
      " '2023-10-21T00:00:00.000000000' '2023-10-22T00:00:00.000000000'\n",
      " '2023-10-23T00:00:00.000000000' '2023-10-24T00:00:00.000000000'\n",
      " '2023-10-25T00:00:00.000000000' '2023-10-26T00:00:00.000000000'\n",
      " '2023-10-27T00:00:00.000000000' '2023-10-28T00:00:00.000000000'\n",
      " '2023-10-29T00:00:00.000000000' '2023-10-30T00:00:00.000000000'\n",
      " '2023-10-31T00:00:00.000000000' '2023-11-01T00:00:00.000000000'\n",
      " '2023-11-02T00:00:00.000000000' '2023-11-03T00:00:00.000000000'\n",
      " '2023-11-04T00:00:00.000000000' '2023-11-05T00:00:00.000000000'\n",
      " '2023-11-06T00:00:00.000000000' '2023-11-07T00:00:00.000000000'\n",
      " '2023-11-08T00:00:00.000000000' '2023-11-09T00:00:00.000000000'\n",
      " '2023-11-10T00:00:00.000000000' '2023-11-11T00:00:00.000000000'\n",
      " '2023-11-12T00:00:00.000000000' '2023-11-13T00:00:00.000000000'\n",
      " '2023-11-14T00:00:00.000000000' '2023-11-15T00:00:00.000000000'\n",
      " '2023-11-16T00:00:00.000000000' '2023-11-17T00:00:00.000000000'\n",
      " '2023-11-18T00:00:00.000000000' '2023-11-19T00:00:00.000000000'\n",
      " '2023-11-20T00:00:00.000000000' '2023-11-21T00:00:00.000000000'\n",
      " '2023-11-22T00:00:00.000000000' '2023-11-23T00:00:00.000000000'\n",
      " '2023-11-24T00:00:00.000000000' '2023-11-25T00:00:00.000000000'\n",
      " '2023-11-26T00:00:00.000000000' '2023-11-27T00:00:00.000000000'\n",
      " '2023-11-28T00:00:00.000000000' '2023-11-29T00:00:00.000000000'\n",
      " '2023-11-30T00:00:00.000000000' '2023-12-01T00:00:00.000000000'\n",
      " '2023-12-02T00:00:00.000000000' '2023-12-03T00:00:00.000000000'\n",
      " '2023-12-04T00:00:00.000000000' '2023-12-05T00:00:00.000000000'\n",
      " '2023-12-06T00:00:00.000000000' '2023-12-07T00:00:00.000000000'\n",
      " '2023-12-08T00:00:00.000000000' '2023-12-09T00:00:00.000000000'\n",
      " '2023-12-10T00:00:00.000000000' '2023-12-11T00:00:00.000000000'\n",
      " '2023-12-12T00:00:00.000000000' '2023-12-13T00:00:00.000000000'\n",
      " '2023-12-14T00:00:00.000000000' '2023-12-15T00:00:00.000000000'\n",
      " '2023-12-16T00:00:00.000000000' '2023-12-17T00:00:00.000000000'\n",
      " '2023-12-18T00:00:00.000000000' '2023-12-19T00:00:00.000000000'\n",
      " '2023-12-20T00:00:00.000000000' '2023-12-21T00:00:00.000000000'\n",
      " '2023-12-22T00:00:00.000000000' '2023-12-23T00:00:00.000000000'\n",
      " '2023-12-24T00:00:00.000000000' '2023-12-25T00:00:00.000000000'\n",
      " '2023-12-26T00:00:00.000000000' '2023-12-27T00:00:00.000000000'\n",
      " '2023-12-28T00:00:00.000000000' '2023-12-29T00:00:00.000000000'\n",
      " '2023-12-30T00:00:00.000000000' '2023-12-31T00:00:00.000000000'\n",
      " '2024-01-01T00:00:00.000000000' '2024-01-02T00:00:00.000000000'\n",
      " '2024-01-03T00:00:00.000000000' '2024-01-04T00:00:00.000000000'\n",
      " '2024-01-05T00:00:00.000000000' '2024-01-06T00:00:00.000000000'\n",
      " '2024-01-07T00:00:00.000000000' '2024-01-08T00:00:00.000000000'\n",
      " '2024-01-09T00:00:00.000000000' '2024-01-10T00:00:00.000000000'\n",
      " '2024-01-11T00:00:00.000000000' '2024-01-12T00:00:00.000000000'\n",
      " '2024-01-13T00:00:00.000000000' '2024-01-14T00:00:00.000000000'\n",
      " '2024-01-15T00:00:00.000000000' '2024-01-16T00:00:00.000000000'\n",
      " '2024-01-17T00:00:00.000000000' '2024-01-18T00:00:00.000000000'\n",
      " '2024-01-19T00:00:00.000000000' '2024-01-20T00:00:00.000000000'\n",
      " '2024-01-21T00:00:00.000000000' '2024-01-22T00:00:00.000000000'\n",
      " '2024-01-23T00:00:00.000000000' '2024-01-24T00:00:00.000000000'\n",
      " '2024-01-25T00:00:00.000000000' '2024-01-26T00:00:00.000000000'\n",
      " '2024-01-27T00:00:00.000000000' '2024-01-28T00:00:00.000000000'\n",
      " '2024-01-29T00:00:00.000000000' '2024-01-30T00:00:00.000000000'\n",
      " '2024-01-31T00:00:00.000000000' '2024-02-01T00:00:00.000000000'\n",
      " '2024-02-02T00:00:00.000000000' '2024-02-03T00:00:00.000000000'\n",
      " '2024-02-04T00:00:00.000000000' '2024-02-05T00:00:00.000000000'\n",
      " '2024-02-06T00:00:00.000000000' '2024-02-07T00:00:00.000000000'\n",
      " '2024-02-08T00:00:00.000000000' '2024-02-09T00:00:00.000000000'\n",
      " '2024-02-10T00:00:00.000000000' '2024-02-11T00:00:00.000000000'\n",
      " '2024-02-12T00:00:00.000000000' '2024-02-13T00:00:00.000000000'\n",
      " '2024-02-14T00:00:00.000000000' '2024-02-15T00:00:00.000000000'\n",
      " '2024-02-16T00:00:00.000000000' '2024-02-17T00:00:00.000000000'\n",
      " '2024-02-18T00:00:00.000000000' '2024-02-19T00:00:00.000000000'\n",
      " '2024-02-20T00:00:00.000000000' '2024-02-21T00:00:00.000000000'\n",
      " '2024-02-22T00:00:00.000000000' '2024-02-23T00:00:00.000000000'\n",
      " '2024-02-24T00:00:00.000000000' '2024-02-25T00:00:00.000000000'\n",
      " '2024-02-26T00:00:00.000000000' '2024-02-27T00:00:00.000000000'\n",
      " '2024-02-28T00:00:00.000000000' '2024-02-29T00:00:00.000000000'\n",
      " '2024-03-01T00:00:00.000000000' '2024-03-02T00:00:00.000000000'\n",
      " '2024-03-03T00:00:00.000000000' '2024-03-04T00:00:00.000000000'\n",
      " '2024-03-05T00:00:00.000000000' '2024-03-06T00:00:00.000000000'\n",
      " '2024-03-07T00:00:00.000000000' '2024-03-08T00:00:00.000000000'\n",
      " '2024-03-09T00:00:00.000000000' '2024-03-10T00:00:00.000000000'\n",
      " '2024-03-11T00:00:00.000000000' '2024-03-12T00:00:00.000000000'\n",
      " '2024-03-13T00:00:00.000000000' '2024-03-14T00:00:00.000000000'\n",
      " '2024-03-15T00:00:00.000000000' '2024-03-16T00:00:00.000000000'\n",
      " '2024-03-17T00:00:00.000000000' '2024-03-18T00:00:00.000000000'\n",
      " '2024-03-19T00:00:00.000000000' '2024-03-20T00:00:00.000000000'\n",
      " '2024-03-21T00:00:00.000000000' '2024-03-22T00:00:00.000000000'\n",
      " '2024-03-23T00:00:00.000000000' '2024-03-24T00:00:00.000000000'\n",
      " '2024-03-25T00:00:00.000000000' '2024-03-26T00:00:00.000000000'\n",
      " '2024-03-27T00:00:00.000000000' '2024-03-28T00:00:00.000000000'\n",
      " '2024-03-29T00:00:00.000000000' '2024-03-30T00:00:00.000000000'\n",
      " '2024-03-31T00:00:00.000000000' '2024-04-01T00:00:00.000000000'\n",
      " '2024-04-02T00:00:00.000000000' '2024-04-03T00:00:00.000000000'\n",
      " '2024-04-04T00:00:00.000000000' '2024-04-05T00:00:00.000000000'\n",
      " '2024-04-06T00:00:00.000000000' '2024-04-07T00:00:00.000000000'\n",
      " '2024-04-08T00:00:00.000000000' '2024-04-09T00:00:00.000000000'\n",
      " '2024-04-10T00:00:00.000000000' '2024-04-11T00:00:00.000000000'\n",
      " '2024-04-12T00:00:00.000000000' '2024-04-13T00:00:00.000000000'\n",
      " '2024-04-14T00:00:00.000000000' '2024-04-15T00:00:00.000000000'\n",
      " '2024-04-16T00:00:00.000000000' '2024-04-17T00:00:00.000000000'\n",
      " '2024-04-18T00:00:00.000000000' '2024-04-19T00:00:00.000000000'\n",
      " '2024-04-20T00:00:00.000000000' '2024-04-21T00:00:00.000000000'\n",
      " '2024-04-22T00:00:00.000000000' '2024-04-23T00:00:00.000000000'\n",
      " '2024-04-24T00:00:00.000000000' '2024-04-25T00:00:00.000000000'\n",
      " '2024-04-26T00:00:00.000000000' '2024-04-27T00:00:00.000000000'\n",
      " '2024-04-28T00:00:00.000000000' '2024-04-29T00:00:00.000000000'\n",
      " '2024-04-30T00:00:00.000000000' '2024-05-01T00:00:00.000000000'\n",
      " '2024-05-02T00:00:00.000000000' '2024-05-03T00:00:00.000000000'\n",
      " '2024-05-04T00:00:00.000000000' '2024-05-05T00:00:00.000000000'\n",
      " '2024-05-06T00:00:00.000000000' '2024-05-07T00:00:00.000000000'\n",
      " '2024-05-08T00:00:00.000000000' '2024-05-09T00:00:00.000000000'\n",
      " '2024-05-10T00:00:00.000000000' '2024-05-11T00:00:00.000000000'\n",
      " '2024-05-12T00:00:00.000000000' '2024-05-13T00:00:00.000000000'\n",
      " '2024-05-14T00:00:00.000000000' '2024-05-15T00:00:00.000000000'\n",
      " '2024-05-16T00:00:00.000000000' '2024-05-17T00:00:00.000000000'\n",
      " '2024-05-18T00:00:00.000000000' '2024-05-19T00:00:00.000000000'\n",
      " '2024-05-20T00:00:00.000000000' '2024-05-21T00:00:00.000000000'\n",
      " '2024-05-22T00:00:00.000000000' '2024-05-23T00:00:00.000000000'\n",
      " '2024-05-24T00:00:00.000000000' '2024-05-25T00:00:00.000000000'\n",
      " '2024-05-26T00:00:00.000000000' '2024-05-27T00:00:00.000000000'\n",
      " '2024-05-28T00:00:00.000000000' '2024-05-29T00:00:00.000000000'\n",
      " '2024-05-30T00:00:00.000000000' '2024-05-31T00:00:00.000000000'\n",
      " '2024-06-01T00:00:00.000000000' '2024-06-02T00:00:00.000000000'\n",
      " '2024-06-03T00:00:00.000000000' '2024-06-04T00:00:00.000000000'\n",
      " '2024-06-05T00:00:00.000000000' '2024-06-06T00:00:00.000000000'\n",
      " '2024-06-07T00:00:00.000000000' '2024-06-08T00:00:00.000000000'\n",
      " '2024-06-09T00:00:00.000000000' '2024-06-10T00:00:00.000000000'\n",
      " '2024-06-11T00:00:00.000000000' '2024-06-12T00:00:00.000000000'\n",
      " '2024-06-13T00:00:00.000000000' '2024-06-14T00:00:00.000000000'\n",
      " '2024-06-15T00:00:00.000000000' '2024-06-16T00:00:00.000000000'\n",
      " '2024-06-17T00:00:00.000000000' '2024-06-18T00:00:00.000000000'\n",
      " '2024-06-19T00:00:00.000000000' '2024-06-20T00:00:00.000000000'\n",
      " '2024-06-21T00:00:00.000000000' '2024-06-22T00:00:00.000000000'\n",
      " '2024-06-23T00:00:00.000000000' '2024-06-24T00:00:00.000000000'\n",
      " '2024-06-25T00:00:00.000000000' '2024-06-26T00:00:00.000000000'\n",
      " '2024-06-27T00:00:00.000000000' '2024-06-28T00:00:00.000000000'\n",
      " '2024-06-29T00:00:00.000000000' '2024-06-30T00:00:00.000000000'\n",
      " '2024-07-01T00:00:00.000000000' '2024-07-02T00:00:00.000000000'\n",
      " '2024-07-03T00:00:00.000000000' '2024-07-04T00:00:00.000000000'\n",
      " '2024-07-05T00:00:00.000000000' '2024-07-06T00:00:00.000000000'\n",
      " '2024-07-07T00:00:00.000000000' '2024-07-08T00:00:00.000000000'\n",
      " '2024-07-09T00:00:00.000000000' '2024-07-10T00:00:00.000000000'\n",
      " '2024-07-11T00:00:00.000000000' '2024-07-12T00:00:00.000000000'\n",
      " '2024-07-13T00:00:00.000000000' '2024-07-14T00:00:00.000000000'\n",
      " '2024-07-15T00:00:00.000000000' '2024-07-16T00:00:00.000000000'\n",
      " '2024-07-17T00:00:00.000000000' '2024-07-18T00:00:00.000000000'\n",
      " '2024-07-19T00:00:00.000000000' '2024-07-20T00:00:00.000000000'\n",
      " '2024-07-21T00:00:00.000000000' '2024-07-22T00:00:00.000000000'\n",
      " '2024-07-23T00:00:00.000000000' '2024-07-24T00:00:00.000000000'\n",
      " '2024-07-25T00:00:00.000000000' '2024-07-26T00:00:00.000000000'\n",
      " '2024-07-27T00:00:00.000000000' '2024-07-28T00:00:00.000000000'\n",
      " '2024-07-29T00:00:00.000000000' '2024-07-30T00:00:00.000000000'\n",
      " '2024-07-31T00:00:00.000000000' '2024-08-01T00:00:00.000000000'\n",
      " '2024-08-02T00:00:00.000000000' '2024-08-03T00:00:00.000000000'\n",
      " '2024-08-04T00:00:00.000000000' '2024-08-05T00:00:00.000000000'\n",
      " '2024-08-06T00:00:00.000000000' '2024-08-07T00:00:00.000000000'\n",
      " '2024-08-08T00:00:00.000000000' '2024-08-09T00:00:00.000000000'\n",
      " '2024-08-10T00:00:00.000000000' '2024-08-11T00:00:00.000000000'\n",
      " '2024-08-12T00:00:00.000000000' '2024-08-13T00:00:00.000000000'\n",
      " '2024-08-14T00:00:00.000000000' '2024-08-15T00:00:00.000000000'\n",
      " '2024-08-16T00:00:00.000000000' '2024-08-17T00:00:00.000000000'\n",
      " '2024-08-18T00:00:00.000000000' '2024-08-19T00:00:00.000000000'\n",
      " '2024-08-20T00:00:00.000000000' '2024-08-21T00:00:00.000000000'\n",
      " '2024-08-22T00:00:00.000000000' '2024-08-23T00:00:00.000000000'\n",
      " '2024-08-24T00:00:00.000000000' '2024-08-25T00:00:00.000000000'\n",
      " '2024-08-26T00:00:00.000000000' '2024-08-27T00:00:00.000000000'\n",
      " '2024-08-28T00:00:00.000000000' '2024-08-29T00:00:00.000000000'\n",
      " '2024-08-30T00:00:00.000000000' '2024-08-31T00:00:00.000000000'\n",
      " '2024-09-01T00:00:00.000000000' '2024-09-02T00:00:00.000000000'\n",
      " '2024-09-03T00:00:00.000000000' '2024-09-04T00:00:00.000000000'\n",
      " '2024-09-05T00:00:00.000000000' '2024-09-06T00:00:00.000000000'\n",
      " '2024-09-07T00:00:00.000000000' '2024-09-08T00:00:00.000000000'\n",
      " '2024-09-09T00:00:00.000000000' '2024-09-10T00:00:00.000000000'\n",
      " '2024-09-11T00:00:00.000000000' '2024-09-12T00:00:00.000000000'\n",
      " '2024-09-13T00:00:00.000000000' '2024-09-14T00:00:00.000000000'\n",
      " '2024-09-15T00:00:00.000000000' '2024-09-16T00:00:00.000000000'\n",
      " '2024-09-17T00:00:00.000000000' '2024-09-18T00:00:00.000000000'\n",
      " '2024-09-19T00:00:00.000000000' '2024-09-20T00:00:00.000000000'\n",
      " '2024-09-21T00:00:00.000000000' '2024-09-22T00:00:00.000000000'\n",
      " '2024-09-23T00:00:00.000000000' '2024-09-24T00:00:00.000000000'\n",
      " '2024-09-25T00:00:00.000000000' '2024-09-26T00:00:00.000000000'\n",
      " '2024-09-27T00:00:00.000000000' '2024-09-28T00:00:00.000000000'\n",
      " '2024-09-29T00:00:00.000000000' '2024-09-30T00:00:00.000000000'\n",
      " '2024-10-01T00:00:00.000000000' '2024-10-02T00:00:00.000000000'\n",
      " '2024-10-03T00:00:00.000000000' '2024-10-04T00:00:00.000000000'\n",
      " '2024-10-05T00:00:00.000000000' '2024-10-06T00:00:00.000000000'\n",
      " '2024-10-07T00:00:00.000000000' '2024-10-08T00:00:00.000000000'\n",
      " '2024-10-09T00:00:00.000000000' '2024-10-10T00:00:00.000000000'\n",
      " '2024-10-11T00:00:00.000000000' '2024-10-12T00:00:00.000000000'\n",
      " '2024-10-13T00:00:00.000000000' '2024-10-14T00:00:00.000000000'\n",
      " '2024-10-15T00:00:00.000000000' '2024-10-16T00:00:00.000000000'\n",
      " '2024-10-17T00:00:00.000000000' '2024-10-18T00:00:00.000000000'\n",
      " '2024-10-19T00:00:00.000000000' '2024-10-20T00:00:00.000000000'\n",
      " '2024-10-21T00:00:00.000000000' '2024-10-22T00:00:00.000000000'\n",
      " '2024-10-23T00:00:00.000000000' '2024-10-24T00:00:00.000000000'\n",
      " '2024-10-25T00:00:00.000000000' '2024-10-26T00:00:00.000000000'\n",
      " '2024-10-27T00:00:00.000000000' '2024-10-28T00:00:00.000000000'\n",
      " '2024-10-29T00:00:00.000000000' '2024-10-30T00:00:00.000000000'\n",
      " '2024-10-31T00:00:00.000000000' '2024-11-01T00:00:00.000000000'\n",
      " '2024-11-02T00:00:00.000000000' '2024-11-03T00:00:00.000000000'\n",
      " '2024-11-04T00:00:00.000000000' '2024-11-05T00:00:00.000000000'\n",
      " '2024-11-06T00:00:00.000000000' '2024-11-07T00:00:00.000000000'\n",
      " '2024-11-08T00:00:00.000000000' '2024-11-09T00:00:00.000000000'\n",
      " '2024-11-10T00:00:00.000000000' '2024-11-11T00:00:00.000000000'\n",
      " '2024-11-12T00:00:00.000000000' '2024-11-13T00:00:00.000000000'\n",
      " '2024-11-14T00:00:00.000000000' '2024-11-15T00:00:00.000000000'\n",
      " '2024-11-16T00:00:00.000000000' '2024-11-17T00:00:00.000000000'\n",
      " '2024-11-18T00:00:00.000000000' '2024-11-19T00:00:00.000000000'\n",
      " '2024-11-20T00:00:00.000000000' '2024-11-21T00:00:00.000000000'\n",
      " '2024-11-22T00:00:00.000000000' '2024-11-23T00:00:00.000000000'\n",
      " '2024-11-24T00:00:00.000000000' '2024-11-25T00:00:00.000000000'\n",
      " '2024-11-26T00:00:00.000000000' '2024-11-27T00:00:00.000000000'\n",
      " '2024-11-28T00:00:00.000000000' '2024-11-29T00:00:00.000000000'\n",
      " '2024-11-30T00:00:00.000000000' '2024-12-01T00:00:00.000000000'\n",
      " '2024-12-02T00:00:00.000000000' '2024-12-03T00:00:00.000000000'\n",
      " '2024-12-04T00:00:00.000000000' '2024-12-05T00:00:00.000000000'\n",
      " '2024-12-06T00:00:00.000000000' '2024-12-07T00:00:00.000000000'\n",
      " '2024-12-08T00:00:00.000000000' '2024-12-09T00:00:00.000000000'\n",
      " '2024-12-10T00:00:00.000000000' '2024-12-11T00:00:00.000000000'\n",
      " '2024-12-12T00:00:00.000000000' '2024-12-13T00:00:00.000000000'\n",
      " '2024-12-14T00:00:00.000000000' '2024-12-15T00:00:00.000000000'\n",
      " '2024-12-16T00:00:00.000000000' '2024-12-17T00:00:00.000000000'\n",
      " '2024-12-18T00:00:00.000000000' '2024-12-19T00:00:00.000000000'\n",
      " '2024-12-20T00:00:00.000000000' '2024-12-21T00:00:00.000000000'\n",
      " '2024-12-22T00:00:00.000000000' '2024-12-23T00:00:00.000000000'\n",
      " '2024-12-24T00:00:00.000000000' '2024-12-25T00:00:00.000000000'\n",
      " '2024-12-26T00:00:00.000000000' '2024-12-27T00:00:00.000000000'\n",
      " '2024-12-28T00:00:00.000000000' '2024-12-29T00:00:00.000000000'\n",
      " '2024-12-30T00:00:00.000000000' '2024-12-31T00:00:00.000000000'\n",
      " '2025-01-01T00:00:00.000000000' '2025-01-02T00:00:00.000000000'\n",
      " '2025-01-03T00:00:00.000000000' '2025-01-04T00:00:00.000000000'\n",
      " '2025-01-05T00:00:00.000000000' '2025-01-06T00:00:00.000000000'\n",
      " '2025-01-07T00:00:00.000000000' '2025-01-08T00:00:00.000000000'\n",
      " '2025-01-09T00:00:00.000000000' '2025-01-10T00:00:00.000000000'\n",
      " '2025-01-11T00:00:00.000000000' '2025-01-12T00:00:00.000000000'\n",
      " '2025-01-13T00:00:00.000000000' '2025-01-14T00:00:00.000000000'\n",
      " '2025-01-15T00:00:00.000000000' '2025-01-16T00:00:00.000000000'\n",
      " '2025-01-17T00:00:00.000000000' '2025-01-18T00:00:00.000000000'\n",
      " '2025-01-19T00:00:00.000000000' '2025-01-20T00:00:00.000000000'\n",
      " '2025-01-21T00:00:00.000000000' '2025-01-22T00:00:00.000000000'\n",
      " '2025-01-23T00:00:00.000000000' '2025-01-24T00:00:00.000000000'\n",
      " '2025-01-25T00:00:00.000000000' '2025-01-26T00:00:00.000000000'\n",
      " '2025-01-27T00:00:00.000000000' '2025-01-28T00:00:00.000000000'\n",
      " '2025-01-29T00:00:00.000000000' '2025-01-30T00:00:00.000000000'\n",
      " '2025-01-31T00:00:00.000000000' '2025-02-01T00:00:00.000000000'\n",
      " '2025-02-02T00:00:00.000000000' '2025-02-03T00:00:00.000000000'\n",
      " '2025-02-04T00:00:00.000000000' '2025-02-05T00:00:00.000000000'\n",
      " '2025-02-06T00:00:00.000000000' '2025-02-07T00:00:00.000000000'\n",
      " '2025-02-08T00:00:00.000000000' '2025-02-09T00:00:00.000000000'\n",
      " '2025-02-10T00:00:00.000000000' '2025-02-11T00:00:00.000000000'\n",
      " '2025-02-12T00:00:00.000000000' '2025-02-13T00:00:00.000000000'\n",
      " '2025-02-14T00:00:00.000000000' '2025-02-15T00:00:00.000000000'\n",
      " '2025-02-16T00:00:00.000000000' '2025-02-17T00:00:00.000000000'\n",
      " '2025-02-18T00:00:00.000000000' '2025-02-19T00:00:00.000000000'\n",
      " '2025-02-20T00:00:00.000000000' '2025-02-21T00:00:00.000000000'\n",
      " '2025-02-22T00:00:00.000000000' '2025-02-23T00:00:00.000000000'\n",
      " '2025-02-24T00:00:00.000000000' '2025-02-25T00:00:00.000000000'\n",
      " '2025-02-26T00:00:00.000000000' '2025-02-27T00:00:00.000000000'\n",
      " '2025-02-28T00:00:00.000000000' '2025-03-01T00:00:00.000000000'\n",
      " '2025-03-02T00:00:00.000000000' '2025-03-03T00:00:00.000000000'\n",
      " '2025-03-04T00:00:00.000000000' '2025-03-05T00:00:00.000000000'\n",
      " '2025-03-06T00:00:00.000000000' '2025-03-07T00:00:00.000000000'\n",
      " '2025-03-08T00:00:00.000000000' '2025-03-09T00:00:00.000000000'\n",
      " '2025-03-10T00:00:00.000000000' '2025-03-11T00:00:00.000000000'\n",
      " '2025-03-12T00:00:00.000000000' '2025-03-13T00:00:00.000000000'\n",
      " '2025-03-14T00:00:00.000000000' '2025-03-15T00:00:00.000000000'\n",
      " '2025-03-16T00:00:00.000000000' '2025-03-17T00:00:00.000000000'\n",
      " '2025-03-18T00:00:00.000000000' '2025-03-19T00:00:00.000000000'\n",
      " '2025-03-20T00:00:00.000000000' '2025-03-21T00:00:00.000000000'\n",
      " '2025-03-22T00:00:00.000000000' '2025-03-23T00:00:00.000000000'\n",
      " '2025-03-24T00:00:00.000000000' '2025-03-25T00:00:00.000000000'\n",
      " '2025-03-26T00:00:00.000000000' '2025-03-27T00:00:00.000000000'\n",
      " '2025-03-28T00:00:00.000000000' '2025-03-29T00:00:00.000000000'\n",
      " '2025-03-30T00:00:00.000000000' '2025-03-31T00:00:00.000000000'\n",
      " '2025-04-01T00:00:00.000000000' '2025-04-02T00:00:00.000000000'\n",
      " '2025-04-03T00:00:00.000000000' '2025-04-04T00:00:00.000000000'\n",
      " '2025-04-05T00:00:00.000000000' '2025-04-06T00:00:00.000000000'\n",
      " '2025-04-07T00:00:00.000000000' '2025-04-08T00:00:00.000000000'\n",
      " '2025-04-09T00:00:00.000000000' '2025-04-10T00:00:00.000000000'\n",
      " '2025-04-11T00:00:00.000000000' '2025-04-12T00:00:00.000000000'\n",
      " '2025-04-13T00:00:00.000000000' '2025-04-14T00:00:00.000000000'\n",
      " '2025-04-15T00:00:00.000000000' '2025-04-16T00:00:00.000000000'\n",
      " '2025-04-17T00:00:00.000000000' '2025-04-18T00:00:00.000000000'\n",
      " '2025-04-19T00:00:00.000000000' '2025-04-20T00:00:00.000000000'\n",
      " '2025-04-21T00:00:00.000000000' '2025-04-22T00:00:00.000000000'\n",
      " '2025-04-23T00:00:00.000000000' '2025-04-24T00:00:00.000000000'\n",
      " '2025-04-25T00:00:00.000000000' '2025-04-26T00:00:00.000000000'\n",
      " '2025-04-27T00:00:00.000000000' '2025-04-28T00:00:00.000000000'\n",
      " '2025-04-29T00:00:00.000000000' '2025-04-30T00:00:00.000000000'\n",
      " '2025-05-01T00:00:00.000000000' '2025-05-02T00:00:00.000000000'\n",
      " '2025-05-03T00:00:00.000000000' '2025-05-04T00:00:00.000000000'\n",
      " '2025-05-05T00:00:00.000000000' '2025-05-06T00:00:00.000000000'\n",
      " '2025-05-07T00:00:00.000000000' '2025-05-08T00:00:00.000000000'\n",
      " '2025-05-09T00:00:00.000000000' '2025-05-10T00:00:00.000000000'\n",
      " '2025-05-11T00:00:00.000000000' '2025-05-12T00:00:00.000000000'\n",
      " '2025-05-13T00:00:00.000000000' '2025-05-14T00:00:00.000000000'\n",
      " '2025-05-15T00:00:00.000000000' '2025-05-16T00:00:00.000000000'\n",
      " '2025-05-17T00:00:00.000000000' '2025-05-18T00:00:00.000000000'\n",
      " '2025-05-19T00:00:00.000000000' '2025-05-20T00:00:00.000000000'\n",
      " '2025-05-21T00:00:00.000000000' '2025-05-22T00:00:00.000000000'\n",
      " '2025-05-23T00:00:00.000000000' '2025-05-24T00:00:00.000000000'\n",
      " '2025-05-25T00:00:00.000000000' '2025-05-26T00:00:00.000000000'\n",
      " '2025-05-27T00:00:00.000000000' '2025-05-28T00:00:00.000000000'\n",
      " '2025-05-29T00:00:00.000000000' '2025-05-30T00:00:00.000000000'\n",
      " '2025-05-31T00:00:00.000000000' '2025-06-01T00:00:00.000000000'\n",
      " '2025-06-02T00:00:00.000000000' '2025-06-03T00:00:00.000000000'\n",
      " '2025-06-04T00:00:00.000000000' '2025-06-05T00:00:00.000000000'\n",
      " '2025-06-06T00:00:00.000000000' '2025-06-07T00:00:00.000000000'\n",
      " '2025-06-08T00:00:00.000000000' '2025-06-09T00:00:00.000000000'\n",
      " '2025-06-10T00:00:00.000000000' '2025-06-11T00:00:00.000000000'\n",
      " '2025-06-12T00:00:00.000000000' '2025-06-13T00:00:00.000000000'\n",
      " '2025-06-14T00:00:00.000000000' '2025-06-15T00:00:00.000000000'\n",
      " '2025-06-16T00:00:00.000000000' '2025-06-17T00:00:00.000000000'\n",
      " '2025-06-18T00:00:00.000000000' '2025-06-19T00:00:00.000000000'\n",
      " '2025-06-20T00:00:00.000000000' '2025-06-21T00:00:00.000000000'\n",
      " '2025-06-22T00:00:00.000000000' '2025-06-23T00:00:00.000000000'\n",
      " '2025-06-24T00:00:00.000000000' '2025-06-25T00:00:00.000000000'\n",
      " '2025-06-26T00:00:00.000000000' '2025-06-27T00:00:00.000000000'\n",
      " '2025-06-28T00:00:00.000000000' '2025-06-29T00:00:00.000000000'\n",
      " '2025-06-30T00:00:00.000000000' '2025-07-01T00:00:00.000000000'\n",
      " '2025-07-02T00:00:00.000000000' '2025-07-03T00:00:00.000000000'\n",
      " '2025-07-04T00:00:00.000000000' '2025-07-05T00:00:00.000000000'\n",
      " '2025-07-06T00:00:00.000000000' '2025-07-07T00:00:00.000000000'\n",
      " '2025-07-08T00:00:00.000000000' '2025-07-09T00:00:00.000000000'\n",
      " '2025-07-10T00:00:00.000000000' '2025-07-11T00:00:00.000000000'\n",
      " '2025-07-12T00:00:00.000000000' '2025-07-13T00:00:00.000000000'\n",
      " '2025-07-14T00:00:00.000000000' '2025-07-15T00:00:00.000000000'\n",
      " '2025-07-16T00:00:00.000000000' '2025-07-17T00:00:00.000000000'\n",
      " '2025-07-18T00:00:00.000000000' '2025-07-19T00:00:00.000000000'\n",
      " '2025-07-20T00:00:00.000000000' '2025-07-21T00:00:00.000000000'\n",
      " '2025-07-22T00:00:00.000000000' '2025-07-23T00:00:00.000000000'\n",
      " '2025-07-24T00:00:00.000000000' '2025-07-25T00:00:00.000000000'\n",
      " '2025-07-26T00:00:00.000000000' '2025-07-27T00:00:00.000000000'\n",
      " '2025-07-28T00:00:00.000000000' '2025-07-29T00:00:00.000000000'\n",
      " '2025-07-30T00:00:00.000000000' '2025-07-31T00:00:00.000000000'\n",
      " '2025-08-01T00:00:00.000000000' '2025-08-02T00:00:00.000000000'\n",
      " '2025-08-03T00:00:00.000000000']\n",
      "Datos descargados: (5422, 11)\n",
      "Fechas disponibles: 2025-08-04 a 2025-08-14\n",
      "Número de días únicos: 11\n",
      "\n",
      "Distribución de datos por día:\n",
      "fecha\n",
      "2025-08-04    517\n",
      "2025-08-05    534\n",
      "2025-08-06    547\n",
      "2025-08-07    545\n",
      "2025-08-08    537\n",
      "2025-08-09    546\n",
      "2025-08-10    574\n",
      "2025-08-11    459\n",
      "2025-08-12    498\n",
      "2025-08-13    605\n",
      "Name: count, dtype: int64\n",
      "Total de días con datos: 11\n",
      "\n",
      "Días esperados en el rango: 956\n",
      "Días con datos: 11\n",
      "Días sin datos: 945\n",
      "\n",
      "Imputando fechas faltantes...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot compare Timestamp with datetime.date. Use ts == pd.Timestamp(date) or ts.date() == date instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m fechas_faltantes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(fechas_completas) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfecha\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m     38\u001b[0m missing_dates \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[38;5;28mlist\u001b[39m(fechas_faltantes))\n\u001b[1;32m---> 39\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mimpute_missing_dates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_dates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatos después de imputación: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDías únicos después de imputación: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfecha\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Workspace\\mlops_fleca_project\\src\\data_utils.py:379\u001b[0m, in \u001b[0;36mimpute_missing_dates\u001b[1;34m(df, missing_dates)\u001b[0m\n\u001b[0;32m    376\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_closed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;66;03m# Concatenar ambos DataFrames\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m df_out \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, df_missing_dates], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# Eliminar duplicados por fecha, dejando la primera ocurrencia\u001b[39;00m\n\u001b[0;32m    382\u001b[0m df_out \u001b[38;5;241m=\u001b[39m df_out\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfecha\u001b[39m\u001b[38;5;124m'\u001b[39m], keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Workspace\\mlops_fleca_project\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:6955\u001b[0m, in \u001b[0;36mDataFrame.sort_values\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   6952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ascending, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m   6953\u001b[0m         ascending \u001b[38;5;241m=\u001b[39m ascending[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 6955\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[43mnargsort\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   6956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mascending\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\n\u001b[0;32m   6957\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6958\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6959\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inplace:\n",
      "File \u001b[1;32mc:\\Workspace\\mlops_fleca_project\\.venv\\lib\\site-packages\\pandas\\core\\sorting.py:483\u001b[0m, in \u001b[0;36mnargsort\u001b[1;34m(items, kind, ascending, na_position, key, mask)\u001b[0m\n\u001b[0;32m    481\u001b[0m     non_nans \u001b[38;5;241m=\u001b[39m non_nans[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    482\u001b[0m     non_nan_idx \u001b[38;5;241m=\u001b[39m non_nan_idx[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 483\u001b[0m indexer \u001b[38;5;241m=\u001b[39m non_nan_idx[\u001b[43mnon_nans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ascending:\n\u001b[0;32m    485\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mtimestamps.pyx:378\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.timestamps._Timestamp.__richcmp__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot compare Timestamp with datetime.date. Use ts == pd.Timestamp(date) or ts.date() == date instead."
     ]
    }
   ],
   "source": [
    "# 3. Descargar y cargar los datos de BigQuery para todo el rango disponible\n",
    "# Función auxiliar para normalizar fechas para BigQuery (la misma que se agregó en data_utils.py)\n",
    "def normalizar_fecha_bigquery(fecha):\n",
    "    \"\"\"Garantiza formato YYYY-MM-DD para campos DATE en BigQuery\"\"\"\n",
    "    if fecha is None:\n",
    "        return None\n",
    "    try:\n",
    "        return pd.to_datetime(fecha).strftime('%Y-%m-%d')\n",
    "    except Exception as e:\n",
    "        print(f\"Error al normalizar fecha '{fecha}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Normalizar ambas fechas explícitamente\n",
    "fecha_inicio_bq = normalizar_fecha_bigquery(fecha_inicio)\n",
    "fecha_fin_bq = normalizar_fecha_bigquery(fecha_fin)\n",
    "\n",
    "print(f\"Descargando datos con fecha_inicio='{fecha_inicio_bq}' y fecha_fin='{fecha_fin_bq}'...\")\n",
    "\n",
    "# Descargamos los datos usando las fechas normalizadas para BigQuery\n",
    "df = load_raw_data(\n",
    "    fecha_inicio=fecha_inicio_bq,  # Fecha normalizada a 'YYYY-MM-DD' para BigQuery DATE\n",
    "    fecha_fin=fecha_fin_bq,        # Fecha normalizada a 'YYYY-MM-DD' para BigQuery DATE\n",
    "    descargar_bq=True\n",
    ")\n",
    "print('Datos descargados:', df.shape)\n",
    "\n",
    "# Normalizar formato de fechas para análisis consistente\n",
    "df['fecha_original'] = df['fecha'].copy()  # Guardar la fecha original por si acaso\n",
    "df['fecha'] = pd.to_datetime(df['fecha']).dt.date  # Convertir a objetos date para evitar problemas de formato\n",
    "\n",
    "print('Fechas disponibles:', min(df['fecha']), 'a', max(df['fecha']))\n",
    "dias_unicos = df['fecha'].nunique()\n",
    "print('Número de días únicos:', dias_unicos)\n",
    "\n",
    "# Analizar la completitud de los datos por día\n",
    "print(\"\\nDistribución de datos por día:\")\n",
    "conteo_por_dia = df['fecha'].value_counts().sort_index()\n",
    "print(conteo_por_dia.head(10))  # Mostrar los primeros 10 días\n",
    "print(f\"Total de días con datos: {len(conteo_por_dia)}\")\n",
    "\n",
    "# Verificar si todos los días esperados tienen datos\n",
    "fecha_inicio_dt = pd.to_datetime(fecha_inicio_bq).date()\n",
    "fecha_fin_dt = pd.to_datetime(fecha_fin_bq).date()\n",
    "dias_esperados = (fecha_fin_dt - fecha_inicio_dt).days + 1\n",
    "print(f\"\\nDías esperados en el rango: {dias_esperados}\")\n",
    "print(f\"Días con datos: {dias_unicos}\")\n",
    "print(f\"Días sin datos: {dias_esperados - dias_unicos}\")\n",
    "\n",
    "# Imputar fechas faltantes si son necesarias\n",
    "if dias_unicos < dias_esperados and 'impute_missing_dates' in globals():\n",
    "    print(\"\\nImputando fechas faltantes...\")\n",
    "    fechas_completas = pd.date_range(start=fecha_inicio_dt, end=fecha_fin_dt, freq='D').date\n",
    "    fechas_faltantes = set(fechas_completas) - set(df['fecha'].unique())\n",
    "    missing_dates = pd.to_datetime(list(fechas_faltantes))\n",
    "    df = impute_missing_dates(df, missing_dates)\n",
    "    print(f\"Datos después de imputación: {df.shape}\")\n",
    "    print(f\"Días únicos después de imputación: {df['fecha'].nunique()}\")\n",
    "else:\n",
    "    print(\"\\nNo es necesario imputar fechas faltantes o la función no está disponible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "882b4ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipos de fecha en el DataFrame:\n",
      "- Tipo de fecha original: <class 'datetime.date'>\n",
      "- Tipo de fecha_dt: <class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "\n",
      "Rango de semanas en los datos: Semana 1 a 52 del año 2025\n",
      "\n",
      "Distribución de días por semana (todas las familias):\n",
      "dias_semana\n",
      "1     17\n",
      "2     16\n",
      "3     38\n",
      "4     57\n",
      "5     64\n",
      "6    117\n",
      "7    804\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribución de días por semana para BOLLERIA:\n",
      "dias_semana\n",
      "7    81\n",
      "6     3\n",
      "4     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Detalle de semanas para BOLLERIA:\n",
      "     year  week   fecha_min   fecha_max  dias_semana\n",
      "4    2024     1  2024-01-02  2024-01-07            6\n",
      "17   2024     2  2024-01-08  2024-01-14            7\n",
      "30   2024     3  2024-01-15  2024-01-21            7\n",
      "42   2024     4  2024-01-22  2024-01-28            7\n",
      "55   2024     5  2024-01-29  2024-02-04            7\n",
      "68   2024     6  2024-02-05  2024-02-11            7\n",
      "81   2024     7  2024-02-12  2024-02-18            7\n",
      "94   2024     8  2024-02-19  2024-02-25            7\n",
      "107  2024     9  2024-02-26  2024-03-03            7\n",
      "120  2024    10  2024-03-04  2024-03-10            7\n",
      "132  2024    11  2024-03-11  2024-03-17            7\n",
      "145  2024    12  2024-03-18  2024-03-24            7\n",
      "158  2024    13  2024-03-25  2024-03-31            7\n",
      "171  2024    14  2024-04-01  2024-04-07            7\n",
      "184  2024    15  2024-04-08  2024-04-14            7\n",
      "197  2024    16  2024-04-15  2024-04-21            7\n",
      "212  2024    17  2024-04-22  2024-04-28            7\n",
      "225  2024    18  2024-04-29  2024-05-05            7\n",
      "238  2024    19  2024-05-06  2024-05-12            7\n",
      "251  2024    20  2024-05-13  2024-05-19            7\n",
      "\n",
      "Número total de semanas para BOLLERIA: 85\n",
      "Transformando a series temporales para BOLLERIA desde 2023-01-02\n",
      "Utilizando min_dias_semana=1 para incluir todas las semanas con al menos un día\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "isinstance() arg 2 must be a type, a tuple of types, or a union",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m df_transform \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Asegurar que la fecha está en el formato correcto para transformación\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf_transform\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfecha\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatetime64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvirtiendo fechas a formato datetime para transformación...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m     df_transform[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfecha\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_transform[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfecha\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: isinstance() arg 2 must be a type, a tuple of types, or a union"
     ]
    }
   ],
   "source": [
    "# 4. Inspección: Análisis de los datos por semana antes de transformar\n",
    "# Convertimos las fechas a datetime para cálculos correctos de semana ISO\n",
    "df['fecha_dt'] = pd.to_datetime(df['fecha'])  # Convertir 'fecha' a datetime para cálculo de semana\n",
    "print(f\"Tipos de fecha en el DataFrame:\")\n",
    "print(f\"- Tipo de fecha original: {type(df['fecha'].iloc[0])}\")\n",
    "print(f\"- Tipo de fecha_dt: {type(df['fecha_dt'].iloc[0])}\")\n",
    "\n",
    "# Calculamos año y semana ISO para todo el DataFrame\n",
    "df['week'] = df['fecha_dt'].dt.isocalendar().week\n",
    "df['year'] = df['fecha_dt'].dt.isocalendar().year\n",
    "print(f\"\\nRango de semanas en los datos: Semana {df['week'].min()} a {df['week'].max()} del año {df['year'].max()}\")\n",
    "\n",
    "# Contamos cuántos días hay por cada combinación de año, semana y familia\n",
    "conteo_dias = df.groupby(['year', 'week', 'familia'])['fecha'].nunique().reset_index(name='dias_semana')\n",
    "conteo_dias['fecha_min'] = df.groupby(['year', 'week', 'familia'])['fecha'].min().values\n",
    "conteo_dias['fecha_max'] = df.groupby(['year', 'week', 'familia'])['fecha'].max().values\n",
    "\n",
    "print(f\"\\nDistribución de días por semana (todas las familias):\")\n",
    "print(conteo_dias['dias_semana'].value_counts().sort_index())\n",
    "\n",
    "# Análisis específico para BOLLERIA\n",
    "bolleria_dias = conteo_dias[conteo_dias['familia'] == 'BOLLERIA'].sort_values(['year', 'week'])\n",
    "print(\"\\nDistribución de días por semana para BOLLERIA:\")\n",
    "print(bolleria_dias['dias_semana'].value_counts())\n",
    "\n",
    "print(\"\\nDetalle de semanas para BOLLERIA:\")\n",
    "print(bolleria_dias[['year', 'week', 'fecha_min', 'fecha_max', 'dias_semana']].head(20))\n",
    "\n",
    "print(\"\\nNúmero total de semanas para BOLLERIA:\", len(bolleria_dias))\n",
    "\n",
    "# 4b. Transformar a series temporales semanales para la familia BOLLERIA\n",
    "# Importante: Asegurarnos que la función reciba fechas en formato datetime correcto\n",
    "fecha_inicio_dt = pd.to_datetime(fecha_inicio_bq)\n",
    "print(f\"Transformando a series temporales para BOLLERIA desde {fecha_inicio_dt.date()}\")\n",
    "print(f\"Utilizando min_dias_semana=1 para incluir todas las semanas con al menos un día\")\n",
    "\n",
    "# Crear una copia del dataframe para evitar problemas\n",
    "df_transform = df.copy()\n",
    "\n",
    "# Asegurar que la fecha está en el formato correcto para transformación\n",
    "if isinstance(df_transform['fecha'].iloc[0], (datetime.date, np.datetime64)):\n",
    "    print(\"Convirtiendo fechas a formato datetime para transformación...\")\n",
    "    df_transform['fecha'] = pd.to_datetime(df_transform['fecha'])\n",
    "\n",
    "# Aplicar la transformación con min_dias_semana=1\n",
    "df_ts = transformar_a_series_temporales(\n",
    "    df_transform, \n",
    "    familia=\"BOLLERIA\", \n",
    "    min_dias_semana=1,  # Esto permitirá incluir todas las semanas, incluso las que no tienen 7 días\n",
    "    fecha_inicio=fecha_inicio_dt  # Usar fecha_inicio como datetime para transformación\n",
    ")\n",
    "\n",
    "print('\\nSeries temporales generadas:', df_ts.shape)\n",
    "print('Rango de semanas:', df_ts['week_start'].min(), 'a', df_ts['week_start'].max())\n",
    "print('Número de semanas en datos transformados:', df_ts['week'].nunique())\n",
    "\n",
    "# Mostrar resumen de las semanas incluidas\n",
    "print('\\nDetalle de semanas transformadas (ordenadas cronológicamente):')\n",
    "resumen_semanas = df_ts[['year', 'week', 'week_start', 'base_imponible', 'dias_semana']].sort_values(['year', 'week'])\n",
    "print(resumen_semanas.head(20))  # Mostramos solo 20 primeras para no saturar la salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e060a50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Series temporales generadas con función flexible: (1, 8)\n",
      "Rango de semanas: 2025-08-04 00:00:00 a 2025-08-04 00:00:00\n",
      "Número de semanas: 1\n",
      "Detalle de semanas:\n",
      "   year  week week_start  base_imponible  dias_semana\n",
      "0  2025    32 2025-08-04         1609.03            7\n"
     ]
    }
   ],
   "source": [
    "# 4c. Alternativa: usar funciones temporales modificadas si la transformación estándar no incluye todas las semanas\n",
    "# Descomenta este código si necesitas usar las funciones modificadas\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "#Asegurarse de que podemos importar desde notebooks/\n",
    "notebooks_dir = Path(\".\").resolve()\n",
    "if notebooks_dir not in sys.path:\n",
    "    sys.path.append(str(notebooks_dir))\n",
    "\n",
    "# Importar funciones modificadas\n",
    "from temp_functions import transformar_a_series_temporales_flexible\n",
    "\n",
    "# Usar la versión flexible que acepta semanas incompletas\n",
    "print(\"\\nUsando función flexible alternativa para comparación...\")\n",
    "df_transform_flexible = df.copy()\n",
    "if isinstance(df_transform_flexible['fecha'].iloc[0], (datetime.date, np.datetime64)):\n",
    "    df_transform_flexible['fecha'] = pd.to_datetime(df_transform_flexible['fecha'])\n",
    "\n",
    "df_ts_flexible = transformar_a_series_temporales_flexible(\n",
    "    df_transform_flexible, \n",
    "    familia=\"BOLLERIA\", \n",
    "    min_dias_semana=1,\n",
    "    fecha_inicio=fecha_inicio_dt\n",
    ")\n",
    "print('\\nSeries temporales generadas con función flexible:', df_ts_flexible.shape)\n",
    "print('Rango de semanas:', df_ts_flexible['week_start'].min(), 'a', df_ts_flexible['week_start'].max())\n",
    "print('Número de semanas:', df_ts_flexible['week'].nunique())\n",
    "print('Detalle de semanas:')\n",
    "print(df_ts_flexible[['year', 'week', 'week_start', 'base_imponible', 'dias_semana']].sort_values(['year', 'week']).head(100))\n",
    "\n",
    "# Usar el resultado de la función flexible en lugar del método estándar\n",
    "# Esta línea reemplaza los datos procesados con los de la función flexible\n",
    "df_ts = df_ts_flexible.copy()\n",
    "print(\"\\nUsando los datos de la función flexible para continuar. Filas:\", df_ts.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09cb7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4e. Preparación final para Hopsworks\n",
    "# Mostrar cuántas filas tenemos antes de preparar los datos\n",
    "print(f\"Número de filas antes de preparar para Hopsworks: {df_ts.shape[0]}\")\n",
    "\n",
    "# Eliminar columnas innecesarias\n",
    "columnas_a_eliminar = ['fecha', 'fecha_dt'] if 'fecha_dt' in df_ts.columns else ['fecha'] if 'fecha' in df_ts.columns else []\n",
    "if columnas_a_eliminar:\n",
    "    df_ts = df_ts.drop(columns=columnas_a_eliminar)\n",
    "    print(f\"Columnas eliminadas: {columnas_a_eliminar}\")\n",
    "\n",
    "# Ajustar tipos para coincidir con el schema del Feature Group histórico\n",
    "df_ts['year'] = df_ts['year'].astype('int64')  # bigint\n",
    "df_ts['week'] = df_ts['week'].astype('int64')  # bigint\n",
    "df_ts['familia'] = df_ts['familia'].astype('string')  # string\n",
    "df_ts['base_imponible'] = df_ts['base_imponible'].astype('float64')  # double\n",
    "df_ts['is_summer_peak'] = df_ts['is_summer_peak'].astype('int32')  # int\n",
    "df_ts['is_easter'] = df_ts['is_easter'].astype('int64')  # bigint\n",
    "df_ts['week_start'] = pd.to_datetime(df_ts['week_start'])  # timestamp\n",
    "\n",
    "# Verificar si dias_semana existe en el DataFrame\n",
    "if 'dias_semana' not in df_ts.columns:\n",
    "    print(\"¡ALERTA! No se encontró la columna dias_semana en los datos finales\")\n",
    "else:\n",
    "    df_ts['dias_semana'] = df_ts['dias_semana'].astype('int32')  # int\n",
    "\n",
    "# Mostrar información final\n",
    "print(\"\\nTipos de datos finales:\")\n",
    "print(df_ts.dtypes)\n",
    "print(\"\\nPrimeras filas:\")\n",
    "print(df_ts.head())\n",
    "print(\"\\nResumen de los datos:\")\n",
    "print(f\"- Número total de semanas: {df_ts['week'].nunique()}\")\n",
    "print(f\"- Rango de fechas: {df_ts['week_start'].min()} a {df_ts['week_start'].max()}\")\n",
    "print(f\"- Total de filas: {df_ts.shape[0]}\")\n",
    "\n",
    "# Guardar copia de respaldo de los datos\n",
    "df_ts.to_csv(\"datos_antes_de_subir_a_hopsworks.csv\", index=False)\n",
    "print(\"\\nCopia de seguridad guardada en 'datos_antes_de_subir_a_hopsworks.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b650d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4d. Diagnóstico: comprobar si estamos procesando todas las semanas disponibles\n",
    "# Comparamos las semanas en los datos originales con las procesadas\n",
    "print(\"\\nDIAGNÓSTICO DE SEMANAS PROCESADAS:\")\n",
    "print(\"Semanas disponibles en datos originales para BOLLERIA:\", len(bolleria_dias))\n",
    "print(\"Semanas procesadas:\", df_ts['week'].nunique())\n",
    "\n",
    "# Crear listas para verificación\n",
    "semanas_originales = [(int(year), int(week)) for year, week in zip(bolleria_dias['year'], bolleria_dias['week'])]\n",
    "semanas_procesadas = [(int(year), int(week)) for year, week in zip(df_ts['year'], df_ts['week'])]\n",
    "\n",
    "# Convertir a conjuntos para comparación\n",
    "set_originales = set(semanas_originales)\n",
    "set_procesadas = set(semanas_procesadas)\n",
    "\n",
    "# Encontrar semanas faltantes\n",
    "semanas_faltantes = set_originales - set_procesadas\n",
    "print(f\"\\nSemanas originales: {sorted(set_originales)}\")\n",
    "print(f\"Semanas procesadas: {sorted(set_procesadas)}\")\n",
    "\n",
    "if semanas_faltantes:\n",
    "    print(f\"\\n¡ALERTA! Hay {len(semanas_faltantes)} semanas que no fueron procesadas:\")\n",
    "    for year, week in sorted(semanas_faltantes):\n",
    "        dias = bolleria_dias[(bolleria_dias['year'] == year) & (bolleria_dias['week'] == week)]['dias_semana'].values[0]\n",
    "        fecha_min = bolleria_dias[(bolleria_dias['year'] == year) & (bolleria_dias['week'] == week)]['fecha_min'].values[0]\n",
    "        print(f\"   - Año {year}, Semana {week}: {dias} días, inicia {fecha_min}\")\n",
    "else:\n",
    "    print(\"\\n✅ ÉXITO: Todas las semanas originales fueron procesadas correctamente.\")\n",
    "    \n",
    "# Verificar si hay semanas adicionales en los datos procesados\n",
    "semanas_adicionales = set_procesadas - set_originales\n",
    "if semanas_adicionales:\n",
    "    print(f\"\\nNOTA: Hay {len(semanas_adicionales)} semanas adicionales en los datos procesados:\")\n",
    "    for year, week in sorted(semanas_adicionales):\n",
    "        print(f\"   + Año {year}, Semana {week}\")\n",
    "        \n",
    "print(\"\\nVerificación completa de diagnóstico.\")\n",
    "\n",
    "# Verificar que tenemos todas las columnas necesarias\n",
    "columnas_requeridas = ['year', 'week', 'familia', 'base_imponible', 'is_summer_peak', 'is_easter', 'week_start', 'dias_semana']\n",
    "for col in columnas_requeridas:\n",
    "    if col not in df_ts.columns:\n",
    "        print(f\"¡ALERTA! Falta la columna '{col}' en los datos procesados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5142035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Conectar a hopsworks\n",
    "project = hopsworks.login(\n",
    "    api_key_value=config.HOPSWORKS_API_KEY, \n",
    "    project=config.HOPSWORKS_PROJECT_NAME)\n",
    "\n",
    "# Conectar al feature store\n",
    "feature_store = project.get_feature_store()\n",
    "\n",
    "# Conectar al Feature Group histórico\n",
    "try:\n",
    "    feature_group = feature_store.get_feature_group(\n",
    "        name=config.FEATURE_GROUP_NAME,\n",
    "        version=config.FEATURE_GROUP_VERSION,\n",
    "        \n",
    "    )\n",
    "    if feature_group is None:\n",
    "        raise Exception(\"El Feature Group histórico no existe o el nombre/version no coinciden exactamente. Verifica en Hopsworks.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al crear/conectar el Feature Group: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f320788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insertar los datos en el Feature Group\n",
    "feature_group.insert(\n",
    "    df_ts,\n",
    "    write_options={'wait_for_job': True}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
