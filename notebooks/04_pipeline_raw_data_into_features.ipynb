{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4c92dd3",
   "metadata": {},
   "source": [
    "# Pipeline de Series Temporales: Carga, Transformación y Feature Engineering\n",
    "\n",
    "Este notebook encapsula el proceso de carga de datos, transformación a series temporales y creación de características y target en funciones reutilizables para forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeeea92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Importar librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "from dateutil.easter import easter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "965b21f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths disponibles:\n",
      "- RAW_BQ_PARQUET: C:\\Workspace\\mlops_fleca_project\\data\\raw\\raw_data_bq_forecasting_20250630.parquet\n",
      "- VALIDATED_RANGE_FECHA_FAMILIA: C:\\Workspace\\mlops_fleca_project\\data\\interim\\validated_range_fecha_familia_20250630.parquet\n",
      "- TS_DF_BOLLERIA_SEMANAL: C:\\Workspace\\mlops_fleca_project\\data\\processed\\ts_df_bolleria_semanal.parquet\n",
      "- TS_DF_BOLLERIA_BASELINE: C:\\Workspace\\mlops_fleca_project\\data\\processed\\ts_df_bolleria_baseline.parquet\n"
     ]
    }
   ],
   "source": [
    "# Importación robusta de paths centralizados\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Definir función para obtener paths de manera robusta\n",
    "def get_robust_paths():\n",
    "    \"\"\"Obtiene paths centralizados de manera robusta, con fallback a construcción manual si falla la importación\"\"\"\n",
    "    try:\n",
    "        from src.paths import (RAW_BQ_PARQUET, VALIDATED_RANGE_FECHA_FAMILIA, \n",
    "                             TS_DF_BOLLERIA_SEMANAL, TS_DF_BOLLERIA_BASELINE)\n",
    "    except ImportError:\n",
    "        import sys\n",
    "        sys.path.append('..')\n",
    "        try:\n",
    "            from src.paths import (RAW_BQ_PARQUET, VALIDATED_RANGE_FECHA_FAMILIA, \n",
    "                                 TS_DF_BOLLERIA_SEMANAL, TS_DF_BOLLERIA_BASELINE)\n",
    "        except ImportError:\n",
    "            # Fallback: Construir paths manualmente\n",
    "            print(\"AVISO: Usando rutas alternativas para paths centralizados\")\n",
    "            data_dir = Path('..') / 'data'\n",
    "            RAW_BQ_PARQUET = data_dir / 'raw' / 'raw_data_bq_forecasting_20250630.parquet'\n",
    "            VALIDATED_RANGE_FECHA_FAMILIA = data_dir / 'interim' / 'validated_range_fecha_familia_20250630.parquet'\n",
    "            TS_DF_BOLLERIA_SEMANAL = data_dir / 'processed' / 'df_bolleria_semanal.parquet'\n",
    "            TS_DF_BOLLERIA_BASELINE = data_dir / 'processed' / 'ts_df_bolleria_baseline.parquet'\n",
    "    \n",
    "    return {\n",
    "        'RAW_BQ_PARQUET': RAW_BQ_PARQUET,\n",
    "        'VALIDATED_RANGE_FECHA_FAMILIA': VALIDATED_RANGE_FECHA_FAMILIA,\n",
    "        'TS_DF_BOLLERIA_SEMANAL': TS_DF_BOLLERIA_SEMANAL,\n",
    "        'TS_DF_BOLLERIA_BASELINE': TS_DF_BOLLERIA_BASELINE\n",
    "    }\n",
    "\n",
    "# Obtener los paths\n",
    "PATHS = get_robust_paths()\n",
    "print(\"Paths disponibles:\")\n",
    "for name, path in PATHS.items():\n",
    "    print(f\"- {name}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83e690de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Función para cargar datos raw\n",
    "def cargar_datos_raw(parquet_file):\n",
    "    \"\"\"\n",
    "    Carga un archivo parquet, convierte columnas 'dbdate' a string si existen y devuelve un DataFrame limpio.\n",
    "    \n",
    "    Parámetros:\n",
    "    - parquet_file: Ruta al archivo parquet (str o Path), preferiblemente desde paths centralizados\n",
    "    \n",
    "    Retorna:\n",
    "    - DataFrame con los datos cargados\n",
    "    \"\"\"\n",
    "    print(f\"Cargando datos desde: {parquet_file}\")\n",
    "    table = pq.read_table(parquet_file)\n",
    "    schema = table.schema\n",
    "    dbdate_cols = [field.name for field in schema if str(field.type) == 'dbdate']\n",
    "    for col in dbdate_cols:\n",
    "        table = table.set_column(table.schema.get_field_index(col), col, table.column(col).cast('string'))\n",
    "    df = table.to_pandas(ignore_metadata=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc3f493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Función para transformar datos a series temporales semanales completas\n",
    "def transformar_a_series_temporales(df_raw, fecha_inicio='2023-01-02', fecha_fin='2025-06-29', familia='BOLLERIA', output_path=None):\n",
    "    \"\"\"\n",
    "    Limpia, homogeneiza y agrega los datos diarios a series semanales completas para la familia indicada.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df_raw: DataFrame con datos crudos\n",
    "    - fecha_inicio: Fecha de inicio (str o datetime)\n",
    "    - fecha_fin: Fecha fin (str o datetime)\n",
    "    - familia: Familia de productos a filtrar (str)\n",
    "    - output_path: Path opcional para guardar el resultado (str o Path)\n",
    "    \n",
    "    Retorna:\n",
    "    - DataFrame con series temporales semanales\n",
    "    \"\"\"\n",
    "    df = df_raw.copy()\n",
    "    df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "    # Filtrar rango de fechas\n",
    "    df = df[(df['fecha'] >= fecha_inicio) & (df['fecha'] <= fecha_fin)]\n",
    "    # Homogeneizar familia si es necesario (ejemplo: 'BEBIDA' a 'BEBIDAS')\n",
    "    if 'familia' in df.columns:\n",
    "        df.loc[df['familia'] == 'BEBIDA', 'familia'] = 'BEBIDAS'\n",
    "    # Imputar valores nulos básicos\n",
    "    for col in ['base_imponible', 'total']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "    # Asegurar columnas exógenas\n",
    "    if 'is_summer_peak' not in df.columns:\n",
    "        df['is_summer_peak'] = 0\n",
    "    if 'is_easter' not in df.columns:\n",
    "        df['is_easter'] = 0\n",
    "    # Calcular semana ISO\n",
    "    iso = df['fecha'].dt.isocalendar()\n",
    "    df['year_iso'] = iso['year']\n",
    "    df['week_iso'] = iso['week']\n",
    "    # Contar días por semana/familia\n",
    "    conteo_dias = df.groupby(['year_iso','week_iso','familia'])['fecha'].count().reset_index(name='dias_semana')\n",
    "    # Agregación semanal\n",
    "    df_semanal = (\n",
    "        df.groupby(['year_iso','week_iso','familia'], as_index=False)\n",
    "          .agg({\n",
    "             'base_imponible': 'sum',\n",
    "             'is_summer_peak': 'max',\n",
    "             'is_easter':      'max'\n",
    "          })\n",
    "        .merge(conteo_dias, on=['year_iso','week_iso','familia'])\n",
    "    )\n",
    "    # Filtrar solo semanas completas\n",
    "    df_semanal = df_semanal[df_semanal['dias_semana'] == 7]\n",
    "    # Filtrar familia\n",
    "    df_bolleria_semanal = df_semanal.query(f\"familia=='{familia}'\").copy()\n",
    "    df_bolleria_semanal.rename(columns={'year_iso':'year','week_iso':'week'}, inplace=True)\n",
    "    df_bolleria_semanal = df_bolleria_semanal.sort_values(['year','week']).reset_index(drop=True)\n",
    "    \n",
    "    # Guardar el resultado si se proporciona un path de salida\n",
    "    if output_path:\n",
    "        df_bolleria_semanal.to_parquet(str(output_path), index=False)\n",
    "        print(f\"Series temporales guardadas en: {output_path}\")\n",
    "        \n",
    "    return df_bolleria_semanal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b430dc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Función para transformar datos en características y target\n",
    "def generar_features_y_target(df_semanal, lags=[1,2,3,4,12,24,52], output_path=None):\n",
    "    \"\"\"\n",
    "    Genera variables lag y variables contextuales, y separa features y target para modelado.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df_semanal: DataFrame con series temporales semanales\n",
    "    - lags: Lista de lags a generar\n",
    "    - output_path: Path opcional para guardar el DataFrame con features (str o Path)\n",
    "    \n",
    "    Retorna:\n",
    "    - X: DataFrame con features\n",
    "    - y: Series con target\n",
    "    - df_features: DataFrame completo con features y target\n",
    "    \"\"\"\n",
    "    df = df_semanal.copy()\n",
    "    df = df.sort_values(['year','week']).reset_index(drop=True)\n",
    "    # Generar lags\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df['base_imponible'].shift(lag)\n",
    "    # Eliminar filas con NaN (por los lags)\n",
    "    df_features = df.dropna().reset_index(drop=True)\n",
    "    # Selección de features y target\n",
    "    feature_cols = [f'lag_{lag}' for lag in lags]\n",
    "    if 'is_summer_peak' in df.columns:\n",
    "        feature_cols.append('is_summer_peak')\n",
    "    if 'is_easter' in df.columns:\n",
    "        feature_cols.append('is_easter')\n",
    "    X = df_features[feature_cols]\n",
    "    y = df_features['base_imponible']\n",
    "    \n",
    "    # Guardar el DataFrame con features si se proporciona un path\n",
    "    if output_path:\n",
    "        df_features.to_parquet(str(output_path), index=False)\n",
    "        print(f\"DataFrame con features guardado en: {output_path}\")\n",
    "        \n",
    "    return X, y, df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "466aa3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos desde: C:\\Workspace\\mlops_fleca_project\\data\\raw\\raw_data_bq_forecasting_20250630.parquet\n",
      "Datos raw cargados: (337353, 9)\n",
      "Datos semanales completos: (0, 7)\n",
      "Features shape: (0, 9), Target shape: (0,)\n",
      "Empty DataFrame\n",
      "Columns: [lag_1, lag_2, lag_3, lag_4, lag_12, lag_24, lag_52, is_summer_peak, is_easter]\n",
      "Index: []\n",
      "Datos semanales guardados en: C:\\Workspace\\mlops_fleca_project\\data\\processed\\ts_df_bolleria_semanal.parquet\n",
      "Dataset con features guardado en: C:\\Workspace\\mlops_fleca_project\\data\\processed\\ts_df_bolleria_baseline.parquet\n",
      "Datos semanales completos: (0, 7)\n",
      "Features shape: (0, 9), Target shape: (0,)\n",
      "Empty DataFrame\n",
      "Columns: [lag_1, lag_2, lag_3, lag_4, lag_12, lag_24, lag_52, is_summer_peak, is_easter]\n",
      "Index: []\n",
      "Datos semanales guardados en: C:\\Workspace\\mlops_fleca_project\\data\\processed\\ts_df_bolleria_semanal.parquet\n",
      "Dataset con features guardado en: C:\\Workspace\\mlops_fleca_project\\data\\processed\\ts_df_bolleria_baseline.parquet\n"
     ]
    }
   ],
   "source": [
    "# 5. Ejemplo de uso de las funciones utilizando paths centralizados\n",
    "\n",
    "# 1. Cargar datos raw usando path centralizado\n",
    "df_raw = cargar_datos_raw(str(PATHS['RAW_BQ_PARQUET']))\n",
    "print(f\"Datos raw cargados: {df_raw.shape}\")\n",
    "\n",
    "# 2. Transformar a series temporales semanales completas\n",
    "df_semanal = transformar_a_series_temporales(df_raw)\n",
    "print(f\"Datos semanales completos: {df_semanal.shape}\")\n",
    "\n",
    "# 3. Generar features y target para modelado\n",
    "X, y, df_final = generar_features_y_target(df_semanal)\n",
    "print(f\"Features shape: {X.shape}, Target shape: {y.shape}\")\n",
    "print(X.head())\n",
    "\n",
    "# 4. Guardar los resultados usando paths centralizados\n",
    "df_semanal.to_parquet(str(PATHS['TS_DF_BOLLERIA_SEMANAL']), index=False)\n",
    "print(f\"Datos semanales guardados en: {PATHS['TS_DF_BOLLERIA_SEMANAL']}\")\n",
    "\n",
    "# Guardar el conjunto de datos con features para el modelo baseline\n",
    "df_final.to_parquet(str(PATHS['TS_DF_BOLLERIA_BASELINE']), index=False)\n",
    "print(f\"Dataset con features guardado en: {PATHS['TS_DF_BOLLERIA_BASELINE']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-project-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
